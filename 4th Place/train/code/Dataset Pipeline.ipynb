{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3c980a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "\n",
    "import time\n",
    "import os\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "from datetime import datetime, date, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import xarray as xr\n",
    "import rasterio as rio\n",
    "\n",
    "import tqdm.auto as tq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7785cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = date(2014,9,1)\n",
    "end_date = date(2021,8,1) # Right border not included\n",
    "\n",
    "# Total months\n",
    "months = (\n",
    "    relativedelta(end_date, start_date).months + relativedelta(end_date, start_date).years * 12\n",
    ")\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "\n",
    "HRRR_SAMPLE = \"hrrr_sample.grib2\"\n",
    "HRRR_DIR = DATA_DIR / \"hrrr\"\n",
    "\n",
    "PRODUCT = 'MYD10A1'\n",
    "MODIS_DIR = DATA_DIR / \"modis\"\n",
    "\n",
    "SOIL_FILE = DATA_DIR / \"global_soil_regions\" / \"so2015v2.tif\"\n",
    "DEM_FILE = DATA_DIR / \"copernicus_dem\" / \"COP90.tif\"\n",
    "\n",
    "GRID_FILE = \"development/grid_cells.geojson\"\n",
    "\n",
    "OUTPUT_DIR = Path(\"development/\")\n",
    "\n",
    "# Modis projection\n",
    "PROJ4MODIS = \"+proj=sinu +lon_0=0 +x_0=0 +y_0=0 +a=6371007.181 +b=6371007.181 +units=m +no_defs\"\n",
    "# HRRR projection\n",
    "PROJ4HRRR = '+proj=lcc +lat_0=38.5 +lon_0=-97.5 +lat_1=38.5 +lat_2=38.5 +x_0=0 +y_0=0 +R=6371229 +units=m +no_defs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3d72ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cells = gpd.read_file(GRID_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15041294",
   "metadata": {},
   "source": [
    "## Gathering HRRR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565021c2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Obtaine x/y projection grid from sample with rasterio:\n",
    "ds = xr.open_dataset(HRRR_SAMPLE, engine='rasterio')\n",
    "\n",
    "proj_y = np.flip(ds.y)\n",
    "proj_x = ds.x\n",
    "\n",
    "del ds\n",
    "\n",
    "# Search points: HRRR projection\n",
    "mid_x = grid_cells.to_crs(PROJ4HRRR).geometry.centroid.x.values\n",
    "mid_y = grid_cells.to_crs(PROJ4HRRR).geometry.centroid.y.values\n",
    "\n",
    "mid_x = xr.DataArray(mid_x, dims=\"cell_id\")\n",
    "mid_y = xr.DataArray(mid_y, dims=\"cell_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb8724d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_points(date_range, \n",
    "               proj_x = proj_x, proj_y = proj_y,\n",
    "               mid_x = mid_x, mid_y = mid_y):\n",
    "    fnamest12 = []\n",
    "    for day in date_range:\n",
    "        for cycle in [12,11,10]:\n",
    "            if f\"{day:%Y%m%d}\" == \"20160805\": cycle = 10\n",
    "            filename = HRRR_DIR / f\"hrrr.{day:%Y%m%d}/conus/hrrr.t{cycle:02}z.wrfsfcf00.grib2\"\n",
    "            if filename.is_file(): \n",
    "                fnamest12.append(filename.as_posix())\n",
    "                break\n",
    "\n",
    "    fnamest00 = []\n",
    "    for day in date_range:\n",
    "        for cycle in [0,1,2]:\n",
    "            filename = HRRR_DIR / f\"hrrr.{day:%Y%m%d}/conus/hrrr.t{cycle:02}z.wrfsfcf00.grib2\"\n",
    "            if filename.is_file(): \n",
    "                fnamest00.append(filename.as_posix())\n",
    "                break\n",
    "    \n",
    "    def round_time(ds):\n",
    "        ds.coords['time'] = ds.coords['time'].dt.floor('D')\n",
    "        return ds\n",
    "    \n",
    "    ds = xr.merge([\n",
    "        # Temperature T12\n",
    "        xr.open_mfdataset(fnamest12, engine='cfgrib',\n",
    "                             backend_kwargs={'indexpath':''},\n",
    "                             drop_variables = ['latitude', 'longitude', 'valid_time', 'step'],\n",
    "                             filter_by_keys={'stepType': 'instant',\n",
    "                             'typeOfLevel': 'surface', 'shortName': 't'},\n",
    "                             preprocess = round_time,\n",
    "            concat_dim='time', combine='nested', parallel=True).rename({'t': 't12'}),\n",
    "        # U component of wind\n",
    "        xr.open_mfdataset(fnamest12, engine='cfgrib',\n",
    "                             backend_kwargs={'indexpath':''},\n",
    "                             drop_variables = ['latitude', 'longitude', 'valid_time', 'step'],\n",
    "                             filter_by_keys={\n",
    "                             'stepType': 'instant',\n",
    "                             'typeOfLevel': 'heightAboveGround',\n",
    "                             'shortName': 'u'},\n",
    "                             preprocess = round_time,\n",
    "                             concat_dim='time', combine='nested', parallel=True),\n",
    "        # V component of wind   \n",
    "        xr.open_mfdataset(fnamest12, engine='cfgrib',\n",
    "                             backend_kwargs={'indexpath':''},\n",
    "                             drop_variables = ['latitude', 'longitude', 'valid_time', 'step'],\n",
    "                             filter_by_keys={\n",
    "                             'stepType': 'instant',\n",
    "                             'typeOfLevel': 'heightAboveGround',\n",
    "                             'shortName': 'v'},\n",
    "                             preprocess = round_time,\n",
    "                             concat_dim='time', combine='nested', parallel=True),\n",
    "        # Water equivalent of accumulated snow depth\n",
    "        xr.open_mfdataset(fnamest12, engine='cfgrib',\n",
    "                             backend_kwargs={'indexpath':''},\n",
    "                             drop_variables = ['latitude', 'longitude', 'valid_time', 'step'],\n",
    "                             filter_by_keys={'stepType': 'instant',\n",
    "                             'typeOfLevel': 'surface', 'shortName': 'sdwe'},\n",
    "                             preprocess = round_time,\n",
    "                             concat_dim='time', combine='nested', parallel=True),\n",
    "        # Precipitable water\n",
    "        xr.open_mfdataset(fnamest12, engine='cfgrib',\n",
    "                             backend_kwargs={'indexpath':''},\n",
    "                             drop_variables = ['latitude', 'longitude', 'valid_time', 'step'],\n",
    "                             filter_by_keys={'stepType': 'instant',\n",
    "                                'typeOfLevel': 'atmosphereSingleLayer',\n",
    "                                'shortName': 'pwat'},\n",
    "                             preprocess = round_time,\n",
    "                             concat_dim='time', combine='nested', parallel=True),\n",
    "        # Maximum/Composite radar reflectivity\n",
    "        xr.open_mfdataset(fnamest12, engine='cfgrib',\n",
    "                             backend_kwargs={'indexpath':''},\n",
    "                             drop_variables = ['latitude', 'longitude', 'valid_time', 'step'],\n",
    "                             filter_by_keys={'stepType': 'instant',\n",
    "                                'typeOfLevel': 'atmosphere',\n",
    "                                'shortName': 'refc'},\n",
    "                             preprocess = round_time,\n",
    "                             concat_dim='time', combine='nested', parallel=True),\n",
    "        # Temperature T00\n",
    "        xr.open_mfdataset(fnamest00, engine='cfgrib',\n",
    "                             backend_kwargs={'indexpath':''},\n",
    "                             drop_variables = ['latitude', 'longitude', 'valid_time', 'step'],\n",
    "                             filter_by_keys={'stepType': 'instant',\n",
    "                             'typeOfLevel': 'surface', 'shortName': 't'},\n",
    "                             preprocess = round_time,\n",
    "            concat_dim='time', combine='nested', parallel=True).rename({'t': 't00'}),\n",
    "        \n",
    "        # Water equivalent of accumulated snow depth - Day accumulated\n",
    "        xr.open_mfdataset(fnamest00, engine='cfgrib',\n",
    "                             backend_kwargs={'indexpath':''},\n",
    "                             drop_variables = ['latitude', 'longitude', 'valid_time', 'step'],\n",
    "                             filter_by_keys={\n",
    "                             'stepType': 'accum',\n",
    "                             'typeOfLevel': 'surface',\n",
    "                             'shortName': 'sdwe'},\n",
    "                             preprocess = round_time,\n",
    "            concat_dim='time', combine='nested', parallel=True).rename({'sdwe': 'sdwea'})\n",
    "    ]).reindex({'time': date_range})\n",
    "    \n",
    "    ds['x'] = proj_x\n",
    "    ds['y'] = proj_y\n",
    "\n",
    "    points = ds.sel(x=mid_x, y=mid_y, method=\"nearest\")\n",
    "    del ds\n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7da7255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat folder for HRRR chunks:\n",
    "os.makedirs(f\"{OUTPUT_DIR}/hrrr\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7259dd8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in tq.trange(months):\n",
    "    ds = get_points(pd.date_range(\n",
    "            start_date + relativedelta(months=i),\n",
    "                  start_date + relativedelta(months=i+1), closed='left', freq='1D'))\n",
    "    # Save to file:\n",
    "    ds.to_netcdf(\n",
    "        f\"{OUTPUT_DIR}/hrrr/hrrr_{start_date + relativedelta(months=i):%Y%m}.nc\",\n",
    "                format=\"NETCDF4\", engine='netcdf4')\n",
    "    ds.close();\n",
    "    del ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa64bce",
   "metadata": {},
   "source": [
    "## Gathering MODIS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2436aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = grid_cells.to_crs(PROJ4MODIS).geometry.bounds\n",
    "\n",
    "# Data slice size\n",
    "rx, ry = 5, 3\n",
    "\n",
    "# Transform values\n",
    "a, _, b, _, c, d = 463.31271652791725, 0.0, -11119505.196667, 0.0, -463.31271652750013, 5559752.598333\n",
    "\n",
    "rowsn = (bounds.maxy.values - d ) / c\n",
    "colsn = (bounds.minx.values - b ) / a\n",
    "\n",
    "xs = xr.DataArray(\n",
    "    np.tile( np.stack(\n",
    "        [np.arange(x, x + rx) for x in np.floor(colsn).astype(int)]), (1,1,ry)).flatten())\n",
    "\n",
    "ys = xr.DataArray(\n",
    "    np.repeat( np.stack(\n",
    "        [np.arange(x, x + ry) for x in np.floor(rowsn).astype(int)]), rx, axis=-1).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0758ff4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(outter):\n",
    "    return [item for sublist in outter for item in sublist]\n",
    "\n",
    "def get_data(day,\n",
    "             x=xs, y=ys,\n",
    "             rx=5, ry=3,\n",
    "             cell_id = grid_cells.cell_id.values,\n",
    "             product = PRODUCT,\n",
    "             variable = 'NDSI',\n",
    "             data_dir = MODIS_DIR):\n",
    "    \n",
    "    # filenames for reading\n",
    "    filenames = flatten([\n",
    "                glob(f\"{data_dir}/{product}/{h:0>2d}/{v:0>2d}/{day:%Y%j}/{product}.A{day:%Y%j}.*.hdf\")\n",
    "                     for h, v in [(8,4),(8,5),(9,4),(9,5),(10,4),(10,5)]])\n",
    "    \n",
    "    if len(filenames) > 4:\n",
    "        xds = xr.open_mfdataset(filenames, engine='rasterio', variable=variable)\n",
    "        ds = xr.Dataset(\n",
    "            data_vars = {\n",
    "                variable : (\n",
    "                    [\"cell_id\", \"time\", \"x\", \"y\"],\n",
    "                            xds[variable].isel(x=xs, y=ys).data.reshape(-1, 1, ry, rx))\n",
    "            },\n",
    "            coords = dict(\n",
    "                    cell_id = cell_id,\n",
    "                    time = pd.date_range(day, day)\n",
    "                ),\n",
    "        )\n",
    "    else:\n",
    "        # No files for reading\n",
    "        ar = np.empty((cell_id.shape[0], 1, ry, rx), dtype=np.float32)\n",
    "        ar.fill(np.nan)\n",
    "        ds = xr.Dataset(\n",
    "            data_vars = {\n",
    "                variable : ([\"cell_id\", \"time\", \"x\", \"y\"], ar)\n",
    "            },\n",
    "            coords = dict(\n",
    "                    cell_id = cell_id,\n",
    "                    time = pd.date_range(day, day)\n",
    "                ),\n",
    "        )\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef82749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat folder for modis chunks:\n",
    "os.makedirs(f\"{OUTPUT_DIR}/modis\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869abf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tq.trange(months):\n",
    "    ds = xr.concat(\n",
    "        [get_data(day) for day in pd.date_range(\n",
    "            start_date + relativedelta(months=i),\n",
    "                  start_date + relativedelta(months=i+1), closed='left', freq='1D')],\n",
    "        dim='time'\n",
    "    )\n",
    "    # Save to file:\n",
    "    ds.to_netcdf(\n",
    "        f\"{OUTPUT_DIR}/modis/{PRODUCT}_{start_date + relativedelta(months=i):%Y%m}.nc\",\n",
    "                format=\"NETCDF4\", engine='netcdf4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f532e1f0",
   "metadata": {},
   "source": [
    "# Combine all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb375d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "ds = xr.open_mfdataset(f\"{OUTPUT_DIR}/hrrr/hrrr_*.nc\", engine='netcdf4')\n",
    "# Add cell id information\n",
    "ds = ds.assign_coords(cell_id=grid_cells.cell_id.values)\n",
    "# Remove unused coords\n",
    "ds = ds.drop([i for i in ds.coords if i not in ds.dims])\n",
    "\n",
    "# Loand NDSI\n",
    "ndsi = xr.open_mfdataset(f\"{OUTPUT_DIR}/modis/{PRODUCT}_*.nc\", engine='netcdf4')\n",
    "ndsi = ndsi.transpose(\"time\", \"cell_id\", \"x\", \"y\")\n",
    "# Merge datasets\n",
    "ds = xr.merge([ds, ndsi.ffill('time').fillna(0).reduce(np.nanmean, (\"x\", \"y\"))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc04b78c",
   "metadata": {},
   "source": [
    "## Add Sunlight Duration (minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035b891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate additional values for sunlingt duration\n",
    "grid_cells['lat'] = (grid_cells.geometry.bounds['maxy'] + grid_cells.geometry.bounds['miny']) / 2\n",
    "grid_cells['lon'] = (grid_cells.geometry.bounds['maxx'] + grid_cells.geometry.bounds['minx']) / 2\n",
    "grid_cells['lat_rad'] = np.pi * grid_cells['lat'] / 180\n",
    "grid_cells['tan_lat'] = np.tan(grid_cells['lat_rad'])\n",
    "grid_cells['k_cos'] = np.cos(np.pi * 90.833 / 180) / np.cos(grid_cells['lat_rad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75aba080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cvs file with sun decline information\n",
    "sun_decline = pd.read_csv(f\"../development/sun_decline.csv\", index_col=[0], parse_dates=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6f1eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caclculate values\n",
    "time_idx = ds.time.values\n",
    "\n",
    "sun_duration = grid_cells.loc[:, \"k_cos\"].values[None] * sun_decline.loc[time_idx, \"cos-1_decl\"].values[:, None]\n",
    "sun_duration -= grid_cells.loc[:, \"tan_lat\"].values[None] * sun_decline.loc[time_idx, \"tan_decl\"].values[:, None]\n",
    "sun_duration = 8 * 180 * np.arccos(sun_duration) / (np.pi * 1) # - 720 / 200 # k - 400\n",
    "sun_duration = sun_duration.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e49ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs = {\n",
    "    'long_name': \"Sunlight Duration\",\n",
    "    'shortName': \"sd\",\n",
    "    'units': \"minutes per day\",\n",
    "    'reference': \"https://gml.noaa.gov/grad/solcalc/calcdetails.html\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6492a6eb",
   "metadata": {},
   "source": [
    "## Add static data: Copernicus DEM and FAO-UNESCO Global Soil Regions Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b490cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "demtiff = rio.open(DEM_FILE)\n",
    "soiltif = rio.open(SOIL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d5665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dem = []\n",
    "images_soil = []\n",
    "bins = np.array([0, 1, 2, 3, 4, 10, 14, 20, 29, 39, 49, 59, 69, 79, 84, 94])\n",
    "\n",
    "for idx, row in grid_cells.iterrows():\n",
    "    \n",
    "    image_dem = demtiff.read(1,\n",
    "            window=demtiff.window(*row.geometry.bounds), out_shape=(10,10)) \n",
    "    images_dem.append(image_dem)\n",
    "    \n",
    "    image_soil = soiltif.read(1,\n",
    "            window=soiltif.window(*row.geometry.bounds), out_shape=(10,10))\n",
    "    image_soil = np.digitize(image_soil, bins, right=True)\n",
    "    images_soil.append(image_soil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4415ffa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dem = np.stack(images_dem).astype(np.float32)\n",
    "images_soil = np.stack(images_soil).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee6eee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.assign(dict(\n",
    "    sd = (['time', 'cell_id'], sun_duration, attrs),\n",
    "    dem = ([\"cell_id\", \"x\", \"y\"], images_dem),\n",
    "    soil = ([\"cell_id\", \"x\", \"y\"], images_soil),\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eb98ea",
   "metadata": {},
   "source": [
    "## Save file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534d6399",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.drop(\n",
    "    ['atmosphereSingleLayer','heightAboveGround',\n",
    "     'atmosphere', 'surface', 'valid_time'], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70086981",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.to_netcdf(f\"{OUTPUT_DIR}/train_dataset.nc\",format=\"NETCDF4\",\n",
    "             engine='netcdf4', encoding={\"sd\": {\"dtype\": \"float32\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908cbd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
