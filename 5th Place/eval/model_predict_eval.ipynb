{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_predict_eval.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "metadata": {
        "id": "uuXZzb-dHuqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "iW_wEmntsCaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Base Data Files"
      ],
      "metadata": {
        "id": "juRVV6pxbDMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import catboost as cb\n",
        "\n",
        "from joblib import load\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KDTree"
      ],
      "metadata": {
        "id": "bWmhTeXucQq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_date = '2022-06-30'\n",
        "first_run = False # '2022-01-13' <- first week model run\n",
        "data_dir = '/content/drive/MyDrive/snocast/eval/data'"
      ],
      "metadata": {
        "id": "a0eYeDAlbfoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_format = pd.read_csv(os.path.join(data_dir, 'submission_format.csv')).rename(columns = {'Unnamed: 0':'cell_id'})\n",
        "test_base = submission_format.fillna(0.)\n",
        "\n",
        "if not first_run:\n",
        "  prev_date = submission_format.columns[submission_format.columns.get_loc(run_date) - 1]\n",
        "  prev_submission = pd.read_csv(f'/content/drive/MyDrive/snocast/eval/submissions/submission_{prev_date}.csv')\n",
        "  print(prev_date)"
      ],
      "metadata": {
        "id": "MX6Oy94WY7t_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_gm(gm_df, dropna=True):\n",
        "  gm_df.columns = ['location_id'] + list(gm_df.columns[1:])\n",
        "  gm_melt = gm_df.melt(id_vars=[\"location_id\"],\n",
        "                      var_name=\"date\",\n",
        "                      value_name=\"swe\")\n",
        "  if dropna:\n",
        "    gm_melt = gm_melt.dropna()\n",
        "  return gm_melt"
      ],
      "metadata": {
        "id": "nJO6VSbOA0lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform ground measures\n",
        "ground_measures_metadata = pd.read_csv(os.path.join(data_dir, 'ground_measures_metadata.csv'))\n",
        "\n",
        "gm = pd.read_csv(os.path.join(data_dir, 'ground_measures/ground_measures_features.csv'))\n",
        "gm_recent = transform_gm(gm, False)\n",
        "gm_test = pd.read_csv(os.path.join(data_dir, 'ground_measures/ground_measures_test_features.csv'))\n",
        "gm_train = pd.read_csv(os.path.join(data_dir, 'ground_measures/ground_measures_train_features.csv'))\n",
        "gm_hist = pd.concat([transform_gm(gm_test), transform_gm(gm_train)])"
      ],
      "metadata": {
        "id": "VmN87NaZAlW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds_melt_test = test_base.melt(id_vars=[\"cell_id\"],\n",
        "                  var_name=\"date\",\n",
        "                  value_name=\"swe\").dropna()[['cell_id','date']]\n",
        "preds_melt_test = preds_melt_test[preds_melt_test['date'] == run_date]\n",
        "\n",
        "grid_elev = pd.read_parquet(os.path.join(data_dir, 'static/grid_cells_elev.parquet'))\n",
        "grid_elev_grad = pd.read_parquet(os.path.join(data_dir, 'static/test_elevation_grads.parquet'))\n",
        "grid_water = pd.read_parquet(os.path.join(data_dir, 'static/grid_water.parquet'))\n",
        "grid_water['water'] = grid_water['water'] - 1\n",
        "grid_lccs = pd.read_parquet(os.path.join(data_dir, 'static/grid_lccs.parquet'))\n",
        "grid_climate = pd.read_parquet(os.path.join(data_dir, f'hrrr/climate_{run_date}.parquet'))\n",
        "modis_terra = pd.read_parquet(os.path.join(data_dir, f'modis/modis_terra_pc_{run_date}.parquet'))\n",
        "modis_aqua = pd.read_parquet(os.path.join(data_dir, f'modis/modis_aqua_pc_{run_date}.parquet'))"
      ],
      "metadata": {
        "id": "mv3tLaDnaxDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Perform Data Transformations"
      ],
      "metadata": {
        "id": "2yOqvzgO6gSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create sequential dataframes for train and test\n",
        "test_pred_seq = pd.merge(preds_melt_test, grid_elev, how='inner', on='cell_id')\n",
        "test_pred_seq.columns = ['location_id', 'date', 'latitude', 'longitude', 'region', 'elevation_m', 'elevation_var_m']"
      ],
      "metadata": {
        "id": "79tM1JY7awy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge Aqua and Terra Modis datasets\n",
        "def transform_modis(df_modis_terra, df_modis_aqua):\n",
        "  df_terra = df_modis_terra.groupby(['location_id','date']).mean().reset_index()\n",
        "  df_aqua = df_modis_aqua.groupby(['location_id','date']).mean().reset_index()\n",
        "  df_modis = pd.merge(df_aqua, df_terra, how='outer', on=['date','location_id'], suffixes=('_aqua','_terra'))\n",
        "  df_modis['date'] = df_modis['date'].str.replace('_','-')\n",
        "  df_modis = df_modis.sort_values(['location_id','date']).reset_index(drop=True)\n",
        "  df_modis['flag'] = 1\n",
        "\n",
        "  return df_modis"
      ],
      "metadata": {
        "id": "jrdOBtgn-sWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_modis = transform_modis(modis_terra, modis_aqua)"
      ],
      "metadata": {
        "id": "qk4aLEXPawg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Need to run this in case we aren't able to pull Modis data all the way up to the run_date\n",
        "def prepare_modis_for_roll(modis_df, seq_df):\n",
        "    dates_df = seq_df[['location_id', 'date']]\n",
        "    missing_dates = pd.merge(dates_df,\n",
        "                             modis_df[['location_id', 'date', 'flag']],\n",
        "                             how='left',\n",
        "                             on=['location_id', 'date'])\n",
        "    dates_df = missing_dates[missing_dates['flag'].isna()][['location_id', 'date']].reset_index(drop=True)\n",
        "\n",
        "    for col in modis_df.columns:\n",
        "        if col not in ('location_id', 'date'):\n",
        "            dates_df[col] = np.nan\n",
        "\n",
        "    modis_df = pd.concat([modis_df, dates_df])\n",
        "    modis_df = modis_df.sort_values(['location_id', 'date']).reset_index(drop=True)\n",
        "\n",
        "    return modis_df"
      ],
      "metadata": {
        "id": "52TzTPHqY-n7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_modis = prepare_modis_for_roll(test_modis, test_pred_seq)"
      ],
      "metadata": {
        "id": "rGcnLYdXZdDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://stackoverflow.com/questions/13996302/python-rolling-functions-for-groupby-object\n",
        "def get_rolling_avgs(df, roll_cols, rolling_days_list):\n",
        "  all_roll_cols = []\n",
        "\n",
        "  df = df.sort_values(['location_id','date'])\n",
        "\n",
        "  for roll_days in rolling_days_list:\n",
        "    rolling_days_cols = [col + f'_{roll_days}_day' for col in roll_cols]\n",
        "    all_roll_cols.extend(rolling_days_cols)\n",
        "    df_roll = (df\n",
        "                      .groupby('location_id', sort=False)[['date'] + roll_cols]\n",
        "                      .rolling(roll_days, min_periods=1, on='date')\n",
        "                      .mean()\n",
        "                      .reset_index()\n",
        "                      .drop('level_1', axis=1))\n",
        "    \n",
        "    df = pd.merge(df, df_roll, how='left', on=['location_id','date'], suffixes=['',f'_{roll_days}_day'])\n",
        "\n",
        "  return df, all_roll_cols"
      ],
      "metadata": {
        "id": "sBmJbZEv-6c_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the 5-day and 15-day rolling average of the Modis data\n",
        "roll_cols = [\n",
        "             'NDSI_Snow_Cover_aqua',\n",
        "             'NDSI_Snow_Cover_terra',\n",
        "             ]\n",
        "\n",
        "rolling_days_list = [5, 15]\n",
        "\n",
        "test_modis, modis_roll_cols = get_rolling_avgs(test_modis, roll_cols, rolling_days_list)"
      ],
      "metadata": {
        "id": "DvfjMAuX2fu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = pd.merge(test_pred_seq, test_modis, how='left', on=['date','location_id'])"
      ],
      "metadata": {
        "id": "MQ4Y-Na62fr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the 3-day rolling average of the climate data\n",
        "climate_cols_2_roll = [\n",
        "             'TMP', \n",
        "             'SNOD', \n",
        "             'WEASD', \n",
        "             'SPFH', \n",
        "             'SNOWC', \n",
        "             'REFC',\n",
        "             'PRES', \n",
        "             'PWAT'\n",
        "             ]\n",
        "\n",
        "rolling_days_list = [3]\n",
        "\n",
        "grid_climate, climate_roll_cols = get_rolling_avgs(grid_climate, climate_cols_2_roll, rolling_days_list)"
      ],
      "metadata": {
        "id": "AyuvCj4f9RYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = pd.merge(test_dataset, grid_climate, how='left', on=['date','location_id'])"
      ],
      "metadata": {
        "id": "FMYj7dtp9cmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add in the snow season day feature\n",
        "test_dataset['datetime'] = pd.to_datetime(test_dataset['date'])\n",
        "test_dataset['snow_season_day'] = test_dataset.datetime.dt.dayofyear.apply(lambda x: x - 335 if x >= 335 else x + 30)\n",
        "\n",
        "gm_hist['datetime'] = pd.to_datetime(gm_hist['date'])\n",
        "gm_hist['snow_season_day'] = gm_hist.datetime.dt.dayofyear.apply(lambda x: x - 335 if x >= 335 else x + 30)\n",
        "\n",
        "gm_recent['datetime'] = pd.to_datetime(gm_recent['date'])\n",
        "gm_recent['snow_season_day'] = gm_recent.datetime.dt.dayofyear.apply(lambda x: x - 335 if x >= 335 else x + 30)"
      ],
      "metadata": {
        "id": "0ULvU5B43fBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate the snow season into periods of 14 days\n",
        "snow_season_period_dict = {}\n",
        "days_in_period = 14\n",
        "total_days = 213\n",
        "period = 0\n",
        "period_count = 0\n",
        "total_periods = int(total_days/days_in_period) - 1\n",
        "\n",
        "for day in range(total_days):\n",
        "  snow_season_period_dict[day] = period\n",
        "  period_count += 1\n",
        "  if period_count == days_in_period:\n",
        "    if period != total_periods:\n",
        "      period += 1\n",
        "    period_count = 0\n",
        "\n",
        "test_dataset['snow_season_period'] = test_dataset.snow_season_day.apply(lambda x: snow_season_period_dict[x])\n",
        "gm_hist['snow_season_period'] = gm_hist.snow_season_day.apply(lambda x: snow_season_period_dict[x])\n",
        "gm_recent['snow_season_period'] = gm_recent.snow_season_day.apply(lambda x: snow_season_period_dict[x])"
      ],
      "metadata": {
        "id": "JYc2CqwY9zIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each location/snow_season_period pair calculate the Z-Score (relative SWE). \n",
        "\n",
        "$\\frac{\\bar{X}-\\mu}{\\hat{\\sigma}}$"
      ],
      "metadata": {
        "id": "u_d0g7TTP1YK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the snow season period historical mean and standard deviation to calculate the relative swe for\n",
        "# each recent ground station measurement\n",
        "gm_period = (gm_hist.groupby(['location_id','snow_season_period'])\n",
        "                            .agg(swe_period_mean=('swe','mean'), swe_period_std=('swe','std'))\n",
        "                            .reset_index())\n",
        "gm_recent = pd.merge(gm_recent, gm_period, how='left', on=['location_id', 'snow_season_period'], suffixes=('','_period_mean'))\n",
        "gm_recent['relative_swe'] = (gm_recent['swe'] - gm_recent['swe_period_mean'])/(gm_recent['swe_period_std'])\n",
        "# Clip outliears of relative_swe due to small sample sizes\n",
        "gm_recent['relative_swe'] = (gm_recent.apply(lambda x: 0.0 if x.swe_period_mean == 0. and x.swe_period_std == 0. else x.relative_swe, axis=1)).clip(-5,5)"
      ],
      "metadata": {
        "id": "ErS9crVqNn61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Backfill most recent date relative_swe if NaN\n",
        "roll_cols = [\n",
        "             'relative_swe'\n",
        "             ]\n",
        "\n",
        "roll_window = [2]\n",
        "\n",
        "gm_recent, relative_swe_roll_cols = get_rolling_avgs(gm_recent, roll_cols, roll_window)\n",
        "gm_recent['relative_swe'] = gm_recent['relative_swe'].fillna(gm_recent['relative_swe_2_day'])\n",
        "gm_recent = gm_recent[gm_recent['swe'].notna()]"
      ],
      "metadata": {
        "id": "3OnpdP0HHGyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gm_recent.relative_swe.hist()"
      ],
      "metadata": {
        "id": "QuvTWM8mjAvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_k_neighbor_swe_data(location_df, neighbor_df, location_seq_df, neighbor_seq_df, k):\n",
        "  ''' function to map a location with a latitude, longitude, and elevation to\n",
        "  its k nearest ground measurement stations in 3-D space. The historical relative SWE\n",
        "  for the k nearest ground measurment stations are retrieved and averaged by\n",
        "  weighted distance. The averaged relative SWE of the k neighbors is returned.\n",
        "  '''\n",
        "  distance_cols = ['longitude','latitude']\n",
        "  scaler = StandardScaler()\n",
        "  scaler.fit(neighbor_df[distance_cols])\n",
        "  X_neighbor = scaler.transform(neighbor_df[distance_cols])\n",
        "  X_location = scaler.transform(location_df[distance_cols])\n",
        "\n",
        "  location_unique_dates = location_seq_df.date.unique()\n",
        "  neighbor_unique_dates = neighbor_seq_df.date.unique()\n",
        "  #date_dict = map_dates_to_most_recent_past_date(location_unique_dates, neighbor_unique_dates)\n",
        "\n",
        "  # Builds the tree on the neighbor data\n",
        "  tree = KDTree(X_neighbor, leaf_size=2)\n",
        "\n",
        "  # Get neighbors for location dataset\n",
        "  location_dist, location_idx = tree.query(X_location, k=k)\n",
        "\n",
        "  neighbor_data = []\n",
        "  # iterate through locations in train\n",
        "  for idx, row in location_df.iterrows():\n",
        "    if idx % 1000 == 0:\n",
        "      print(idx)\n",
        "    # for each location get neighbors and distances\n",
        "    location_id = row['location_id']\n",
        "    # get neighbors for the location\n",
        "    neighbors = neighbor_df.loc[location_idx[idx]]['location_id'].values\n",
        "    # build df for neighbors with distances to the location\n",
        "    distance_df = pd.DataFrame({'location_id': neighbors, 'distance': location_dist[idx]})\n",
        "    distance_df = distance_df[distance_df['distance'] != 0]\n",
        "    neighbors = distance_df['location_id'].unique()\n",
        "    # get historical relative swe data for neighbors\n",
        "    neighbor_swe_hist_df = neighbor_seq_df[neighbor_seq_df['location_id'].isin(neighbors)][['location_id','date','relative_swe']]\n",
        "    neighbor_swe_hist_df.columns = ['location_id','neighbor_date','neighbor_relative_swe']\n",
        "    # build sequential df for the location to capture predictions\n",
        "    location_swe_pred_df = pd.DataFrame({'date': location_unique_dates})\n",
        "    location_swe_pred_df['location_id'] = location_id\n",
        "    # map the dates location_swe_pred_df to applicable neighbor dates\n",
        "    location_swe_pred_df['neighbor_date'] = location_swe_pred_df['date'] #.apply(lambda x: date_dict[x])\n",
        "    # get the inverse distance weight to figure out the contribution for each neighbor\n",
        "    distance_df['inverse_distance_weight'] = distance_df['distance']**-1/(distance_df['distance']**-1).sum()\n",
        "    # build a lookup df for the neighbor sequential data\n",
        "    lookup_df = pd.merge(neighbor_swe_hist_df, distance_df, how='inner', on='location_id')\n",
        "    lookup_df['swe_contrib'] = lookup_df['neighbor_relative_swe']*lookup_df['inverse_distance_weight']\n",
        "    combined_df = pd.merge(location_swe_pred_df, lookup_df[['neighbor_date','swe_contrib']], how='inner', on='neighbor_date')\n",
        "    combined_df = combined_df[['location_id','date','swe_contrib']].groupby(['location_id','date']).sum().reset_index()\n",
        "    neighbor_data.extend(combined_df.values)\n",
        "\n",
        "  all_locations_df = pd.DataFrame(neighbor_data, columns=['location_id','date','neighbor_relative_swe'])\n",
        "  return all_locations_df"
      ],
      "metadata": {
        "id": "HvwP0c6PDcN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = 15\n",
        "## Get ground measure neighbor relative SWE for eval data\n",
        "# Build df for grid cells data\n",
        "location_df = grid_elev[['cell_id', 'latitude', 'longitude', 'elevation_m']]\n",
        "location_df.columns = ['location_id', 'latitude', 'longitude', 'elevation_m']\n",
        "location_seq_df = test_pred_seq[['date']]\n",
        "# Build df for ground measures\n",
        "neighbor_df = ground_measures_metadata[['station_id', 'elevation_m', 'latitude', 'longitude']]\n",
        "neighbor_df.columns = ['location_id', 'elevation_m', 'latitude', 'longitude']\n",
        "neighbor_seq_df = gm_recent[gm_recent['date'] == run_date][['location_id', 'date', 'relative_swe']]\n",
        "# Only include ground measure stations that have a relative_swe value for the run_date\n",
        "neighbor_df = pd.merge(neighbor_df, neighbor_seq_df, how='inner', on='location_id')\n",
        "print(neighbor_seq_df.date.max())"
      ],
      "metadata": {
        "id": "dySP-n4aKpME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neighbor_swe_df = get_k_neighbor_swe_data(location_df, neighbor_df, location_seq_df, neighbor_seq_df, k)"
      ],
      "metadata": {
        "id": "Q8ZQzeUaRCsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = pd.merge(test_dataset, neighbor_swe_df, how='left', on=['location_id','date'])"
      ],
      "metadata": {
        "id": "6HrdYIkuTmGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add in the water feature\n",
        "test_dataset = pd.merge(test_dataset, grid_water, how='left', on=['location_id'])"
      ],
      "metadata": {
        "id": "mWW_mYBk2fo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add in the land category feature\n",
        "test_dataset = pd.merge(test_dataset, grid_lccs, how='left', on=['location_id'])"
      ],
      "metadata": {
        "id": "RcQOVumU1MBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add in the elevation gradient features\n",
        "grid_elev_grad.columns = ['location_id','east_elev_grad','south_elev_grad','east_elev_pct','south_elev_pct']\n",
        "test_dataset = pd.merge(test_dataset, grid_elev_grad, how='left', on='location_id')"
      ],
      "metadata": {
        "id": "JJOkL7UUAKss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run XGBoost on transformed data"
      ],
      "metadata": {
        "id": "IyPzVOYA2gjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "climate_cols = ['SNOD', 'WEASD', 'SNOWC'] + ['TMP_3_day','SPFH_3_day','PRES_3_day','PWAT_3_day']\n",
        "xgb_cols = [\n",
        "            'latitude',\n",
        "            'longitude',\n",
        "            'elevation_m',\n",
        "            'elevation_var_m',\n",
        "            'snow_season_day',\n",
        "            'water',\n",
        "            'neighbor_relative_swe',\n",
        "            'east_elev_grad',\n",
        "            'south_elev_grad',\n",
        "            ] \\\n",
        "            + modis_roll_cols + climate_cols\n",
        "\n",
        "X = test_dataset[xgb_cols]"
      ],
      "metadata": {
        "id": "tu9KX5Xb5wnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_all = xgb.XGBRegressor()\n",
        "xgb_all.load_model('/content/drive/MyDrive/snocast/eval/models/xgb_all.txt')"
      ],
      "metadata": {
        "id": "YmbjHehCr933"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = load('/content/drive/MyDrive/snocast/eval/models/std_scaler.bin')"
      ],
      "metadata": {
        "id": "HihvQUlXsQqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = scaler.transform(X)"
      ],
      "metadata": {
        "id": "USg2fyXvsV_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make predictions on new data\n",
        "xgb_pred = xgb_all.predict(X)"
      ],
      "metadata": {
        "id": "7Lx_fF7F6WOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_pred[xgb_pred < 0] = 0.0"
      ],
      "metadata": {
        "id": "OGPUDzlE8iBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(xgb_pred.min(), xgb_pred.mean(), xgb_pred.std(), xgb_pred.max())"
      ],
      "metadata": {
        "id": "FjCYHXSd8Y_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset['xgb_swe_pred'] = xgb_pred"
      ],
      "metadata": {
        "id": "Xd54iPsn70KP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Light GBM on Transformed Data"
      ],
      "metadata": {
        "id": "4_SzgQhA2iZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset['neighbor_relative_swe'] = test_dataset['neighbor_relative_swe'].astype(float)"
      ],
      "metadata": {
        "id": "lPH57pvAIR7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_cols = ['lccs_0', 'lccs_1', 'lccs_2']\n",
        "lgb_cols = [\n",
        "            'latitude',\n",
        "            'longitude',\n",
        "            'elevation_m',\n",
        "            'elevation_var_m',\n",
        "            'snow_season_day',\n",
        "            'water',\n",
        "            'neighbor_relative_swe',\n",
        "            'east_elev_grad',\n",
        "            'south_elev_grad',\n",
        "            ] \\\n",
        "            + modis_roll_cols + climate_cols + cat_cols\n",
        "\n",
        "X_lgb = test_dataset[lgb_cols]"
      ],
      "metadata": {
        "id": "6brPWwdY2pFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lgb_reg = lgb.Booster(model_file='/content/drive/MyDrive/snocast/eval/models/lgb_all.txt')  # init model"
      ],
      "metadata": {
        "id": "mr3vOy-d2o9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lgb_pred = lgb_reg.predict(X_lgb)"
      ],
      "metadata": {
        "id": "z3hWJ9Cy2o03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lgb_pred[lgb_pred < 0] = 0.0"
      ],
      "metadata": {
        "id": "ENcoTkxI2orn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(lgb_pred.min(), lgb_pred.mean(), lgb_pred.std(), lgb_pred.max())"
      ],
      "metadata": {
        "id": "9HZT6omESed4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset['lgb_swe_pred'] = lgb_pred"
      ],
      "metadata": {
        "id": "y0cv80CfPB7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Catboost on Transformed Data"
      ],
      "metadata": {
        "id": "dz5nT1P7IGjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset['lccs_1'] = test_dataset['lccs_1'].fillna(0).astype(int)\n",
        "test_dataset['lccs_2'] = test_dataset['lccs_2'].fillna(0).astype(int)"
      ],
      "metadata": {
        "id": "V-ds136BID7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_cols = ['lccs_0', 'lccs_1', 'lccs_2','region']\n",
        "cb_cols = [\n",
        "            'latitude',\n",
        "            'longitude',\n",
        "            'elevation_m',\n",
        "            'elevation_var_m',\n",
        "            'snow_season_day',\n",
        "            'water',\n",
        "            'neighbor_relative_swe',\n",
        "            'east_elev_grad',\n",
        "            'south_elev_grad',\n",
        "            ] \\\n",
        "            + modis_roll_cols + climate_cols + cat_cols\n",
        "\n",
        "X_cb = test_dataset[cb_cols]"
      ],
      "metadata": {
        "id": "HX9VBjPHJLYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cb_dataset = cb.Pool(data=X_cb,\n",
        "                        cat_features=[20, 21, 22, 23]) "
      ],
      "metadata": {
        "id": "c1Rg8b_8Ja2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cb_model = cb.CatBoostRegressor()\n",
        "cb_model.load_model('/content/drive/MyDrive/snocast/eval/models/cb_all.txt')"
      ],
      "metadata": {
        "id": "4e7XMT0IJahN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cb_pred = cb_model.predict(X_cb)"
      ],
      "metadata": {
        "id": "FWHzUcxTJ1kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cb_pred[cb_pred < 0] = 0.0"
      ],
      "metadata": {
        "id": "G-ne4heWJ1ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cb_pred.min(), cb_pred.mean(), cb_pred.std(), cb_pred.max())"
      ],
      "metadata": {
        "id": "nj83llydSm4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset['cb_swe_pred'] = cb_pred"
      ],
      "metadata": {
        "id": "Nsyrg1fVJ9po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ensemble model predictions"
      ],
      "metadata": {
        "id": "y2XCmEt4H_vw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gb_ensemble(row):\n",
        "  if row['region'] == 'sierras':\n",
        "    swe_pred = (0.40*row['lgb_swe_pred']\n",
        "                + 0.25*row['xgb_swe_pred']\n",
        "                + 0.35*row['cb_swe_pred'])\n",
        "  elif row['region'] == 'central rockies':\n",
        "    swe_pred = (0.80*row['lgb_swe_pred']\n",
        "                + 0.20*row['xgb_swe_pred'])\n",
        "  else:\n",
        "    swe_pred = (0.70*row['lgb_swe_pred']\n",
        "                + 0.20*row['xgb_swe_pred']\n",
        "                + 0.10*row['cb_swe_pred'])\n",
        "  return swe_pred"
      ],
      "metadata": {
        "id": "Mx2Xr1OMPBxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset['best_swe_pred'] = test_dataset.apply(lambda x: gb_ensemble(x), axis=1)"
      ],
      "metadata": {
        "id": "m_1LcPM8Zvti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_dataset['best_swe_pred'].min(), \n",
        "      test_dataset['best_swe_pred'].mean(), \n",
        "      test_dataset['best_swe_pred'].std(), \n",
        "      test_dataset['best_swe_pred'].max(),\n",
        "      test_dataset['best_swe_pred'].median())"
      ],
      "metadata": {
        "id": "d1K_Ax57bmlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset.groupby('region')['best_swe_pred'].mean()"
      ],
      "metadata": {
        "id": "JXqGxojhqzhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the data to submission format\n",
        "test_preds = test_dataset[['location_id','date','best_swe_pred']]\n",
        "test_preds.columns = ['cell_id','date','swe_pred']\n",
        "test_preds = test_preds.pivot_table(index='cell_id', columns='date')\n",
        "test_preds.columns = test_preds.columns.droplevel().rename(None)\n",
        "test_preds = test_preds.reset_index(drop=False)"
      ],
      "metadata": {
        "id": "O0W-Fjfw8Syp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset.to_parquet(f'/content/drive/MyDrive/snocast/eval/data/test_preds_{run_date}.parquet')"
      ],
      "metadata": {
        "id": "mqufrizJqiQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if first_run == True:\n",
        "  submission = submission_format\n",
        "else:\n",
        "  submission = prev_submission\n",
        "submission[run_date] = test_preds[[run_date]].values"
      ],
      "metadata": {
        "id": "Q0HAhMONU8QK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission.sample(10)"
      ],
      "metadata": {
        "id": "akkVas8JcLGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission.to_csv(f'/content/drive/MyDrive/snocast/eval/submissions/submission_{run_date}.csv', index=False)"
      ],
      "metadata": {
        "id": "7GuFtcjdVZPn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}