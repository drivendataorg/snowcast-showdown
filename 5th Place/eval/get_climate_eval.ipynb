{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIjfmpWR4LnG"
      },
      "outputs": [],
      "source": [
        "# These libraries are needed for the pygrib library in Colab. \n",
        "# Note that is needed if you install pygrib using pip.\n",
        "# If you use conda, the libraries will be installed automatically.\n",
        "! apt-get install libeccodes-dev libproj-dev\n",
        "\n",
        "# Install the python packages\n",
        "! pip install pyproj\n",
        "! pip install pygrib\n",
        "\n",
        "# Uninstall existing shapely\n",
        "# We will re-install shapely in the next step by ignoring the binary\n",
        "# wheels to make it compatible with other modules that depend on \n",
        "# GEOS, such as Cartopy (used here).\n",
        "!pip uninstall --yes shapely\n",
        "\n",
        "# To install cartopy in Colab using pip, we need to install the library \n",
        "# dependencies first.\n",
        "\n",
        "!apt-get install -qq libgdal-dev libgeos-dev\n",
        "!pip install shapely --no-binary shapely\n",
        "!pip install cfgrib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqOkTpVfCvi7"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muqUwpX4_9o7"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io\n",
        "from datetime import date, datetime, timedelta\n",
        "import tempfile\n",
        "\n",
        "import xarray as xr\n",
        "import requests\n",
        "\n",
        "# Not used directly, but used via xarray\n",
        "import cfgrib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuc9zk-LBQDh"
      },
      "source": [
        "## Import Base Data Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tTvLvoF3LbR"
      },
      "outputs": [],
      "source": [
        "run_date = '2022-06-30'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mshiRZnECY4C"
      },
      "outputs": [],
      "source": [
        "ground_measures_metadata = pd.read_csv('/content/drive/MyDrive/snocast/eval/data/ground_measures_metadata.csv')\n",
        "submission_format = pd.read_csv('/content/drive/MyDrive/snocast/eval/data/submission_format.csv')\n",
        "lookback = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eaWiVLymfZd"
      },
      "outputs": [],
      "source": [
        "# get latitude longitude for grids\n",
        "f = open('/content/drive/MyDrive/snocast/eval/data/grid_cells.geojson')\n",
        "grid_cells = json.load(f)\n",
        "print('length grid_cells features: ', len(grid_cells['features']))\n",
        "\n",
        "ids = []\n",
        "lats = []\n",
        "lons = []\n",
        "bboxes = []\n",
        "\n",
        "for grid_cell in grid_cells['features']:\n",
        "    cell_id = grid_cell['properties']['cell_id']\n",
        "    coordinates = grid_cell['geometry']['coordinates'][0]\n",
        "    lon, lat = np.mean(coordinates, axis=0)\n",
        "    northeast_corner = np.max(coordinates, axis=0)\n",
        "    southwest_corner = np.min(coordinates, axis=0)\n",
        "    # bbox = [min_lon, min_lat, max_lon, max_lat]\n",
        "    bbox = np.concatenate([southwest_corner,northeast_corner])\n",
        "    ids.append(cell_id)\n",
        "    lats.append(lat)\n",
        "    lons.append(lon)\n",
        "    bboxes.append(bbox)\n",
        "\n",
        "grid_cells_pd = pd.DataFrame({'location_id': ids, \n",
        "                             'latitude': lats, \n",
        "                             'longitude': lons, \n",
        "                             'bbox': bboxes})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cI2IEu497Ya"
      },
      "source": [
        "## Get NOAA HRRR Data\n",
        "The NOAA HRRR is a real-time 3km resolution, hourly updated, cloud-resolving, convection-allowing atmospheric model, initialized by 3km grids with 3km radar assimilation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unqIY9LPCxCv"
      },
      "outputs": [],
      "source": [
        "max_date = datetime.strptime(run_date,'%Y-%m-%d')\n",
        "date_list = [(max_date - timedelta(days=x)).strftime('%Y-%m-%d') for x in range(lookback)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXX3FsftESXp"
      },
      "outputs": [],
      "source": [
        "date_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEpn6lCK97FB"
      },
      "outputs": [],
      "source": [
        "# Constants for creating the full URL\n",
        "blob_container = \"https://noaa-hrrr-bdp-pds.s3.amazonaws.com\"\n",
        "blob_container = \"https://noaahrrr.blob.core.windows.net/hrrr\"\n",
        "sector = \"conus\"\n",
        "cycle = 12        # 4 PM\n",
        "forecast_hour = 0   # offset from cycle time\n",
        "product = \"wrfsfcf\" # 2D surface levels\n",
        "\n",
        "# Put it all together\n",
        "file_path = f\"hrrr.t{cycle:02}z.{product}{forecast_hour:02}.grib2\"\n",
        "\n",
        "hrrr_day = date_list[0].replace(\"-\",\"\")\n",
        "url = f\"{blob_container}/hrrr.{hrrr_day}/{sector}/{file_path}\"\n",
        "#url = f'{blob_container}/hrrr.20220210/{sector}/{file_path}'\n",
        "\n",
        "print(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vf2tZOQ5G1Pc"
      },
      "outputs": [],
      "source": [
        "# Fetch the idx file by appending the .idx file extension to our already formatted URL\n",
        "r = requests.get(f\"{url}.idx\")\n",
        "url_idx = r.text.splitlines()\n",
        "\n",
        "# Take a peek at the content of the index\n",
        "print(*url_idx[0:10], sep=\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TB28XexZ_Mc_"
      },
      "outputs": [],
      "source": [
        "# https://www.nco.ncep.noaa.gov/pmb/products/hrrr/hrrr.t00z.wrfsfcf02.grib2.shtml\n",
        "metrics = [\n",
        "           (\":TMP:surface:anl\", \"t\"), # temperature [K]\n",
        "           (\":SNOD:surface:anl\", \"sde\"), # snow depth [m]\n",
        "           (\":WEASD:surface:anl\", \"sdwe\"), # water equivalent of accumulated snow depth [kg/m^2]\n",
        "           (\":SPFH:2 m above ground:anl:\", \"sh2\"), # specific humidity [kg/kg]\n",
        "           (\":SNOWC:surface:anl:\", \"snowc\"), # snow cover [%]\n",
        "           (\":REFC:entire atmosphere:anl:\", \"refc\"), # composite reflectivity [dB]\n",
        "           (\":PRES:surface:anl:\", \"sp\"), # pressure [Pa]\n",
        "           (\":PWAT:entire atmosphere (considered as a single layer):anl:\", \"pwat\"), # precipitable water [kg/m^2]\n",
        "           ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFSSn7fxBD1r"
      },
      "outputs": [],
      "source": [
        "def get_metric_ds(metric, url_idx):\n",
        "  metric_idx = [l for l in url_idx if metric in l][0].split(\":\")\n",
        "  # Pluck the byte offset from this line, plus the beginning offset of the next line\n",
        "  line_num = int(metric_idx[0])\n",
        "  range_start = metric_idx[1]\n",
        "  # The line number values are 1-indexed, so we don't need to increment it to get the next list index,\n",
        "  # but check we're not already reading the last line\n",
        "  next_line = url_idx[line_num].split(':') if line_num < len(url_idx) else None\n",
        "  # Pluck the start of the next byte offset, or nothing if we were on the last line\n",
        "  range_end = next_line[1] if next_line else None\n",
        "  file = tempfile.NamedTemporaryFile(prefix=\"tmp_\", delete=False)\n",
        "\n",
        "  headers = {\"Range\": f\"bytes={range_start}-{range_end}\"}\n",
        "  resp = requests.get(url, headers=headers, stream=True)\n",
        "\n",
        "  with file as f:\n",
        "      f.write(resp.content)\n",
        "\n",
        "  ds = xr.open_dataset(file.name, engine='cfgrib', \n",
        "                      backend_kwargs={'indexpath':''})\n",
        "\n",
        "  return ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_kZfWMP-NMa"
      },
      "outputs": [],
      "source": [
        "unique_ids = grid_cells_pd[['location_id','bbox']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0l8vwLDAmuPs"
      },
      "outputs": [],
      "source": [
        "all_data = [['location_id','date','TMP','SNOD','WEASD','SPFH','SNOWC','REFC','PRES','PWAT']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bspbjQPriSbF"
      },
      "outputs": [],
      "source": [
        "# 45 min. per day\n",
        "for pull_date in date_list:\n",
        "  print(pull_date)\n",
        "  hrrr_date = pull_date.replace('-','')\n",
        "\n",
        "  find_url = True\n",
        "  still_cycles_to_search = True\n",
        "  url_found = False\n",
        "  cycle = 16\n",
        "  while find_url and still_cycles_to_search:\n",
        "    file_path = f\"hrrr.t{cycle:02}z.{product}{forecast_hour:02}.grib2\"\n",
        "    url = f\"{blob_container}/hrrr.{hrrr_date}/{sector}/{file_path}\"\n",
        "    # Fetch the idx file by appending the .idx file extension to our already formatted URL\n",
        "    r = requests.get(f\"{url}.idx\")\n",
        "    url_idx = r.text.splitlines()\n",
        "    if url_idx[0] == '<?xml version=\"1.0\" encoding=\"UTF-8\"?>':\n",
        "      if cycle == 0:\n",
        "        still_cycles_to_search = False\n",
        "      cycle -= 1\n",
        "      print(f'bad url: {url}')\n",
        "    else:\n",
        "      find_url = False\n",
        "      url_found = True\n",
        "\n",
        "  if url_found:\n",
        "    ds_list = []\n",
        "    for m in metrics:\n",
        "      ds_list.append((get_metric_ds(m[0], url_idx),m[1]))\n",
        "    \n",
        "    for idx, row in unique_ids.iterrows():\n",
        "      if idx % 5000 == 0:\n",
        "        print(idx)\n",
        "      cell_id = row['location_id']\n",
        "      row_list = [cell_id, pull_date]\n",
        "      min_lon, min_lat, max_lon, max_lat = row['bbox']\n",
        "      for ds, m in ds_list:\n",
        "        expand_search = 0.025 # Expand the lat lon bounds of the search to ensure we get data\n",
        "        lat_values = (ds[m].latitude.values < max_lat + expand_search) & (ds[m].latitude.values > min_lat - expand_search)\n",
        "        # noaa hrrr longitude values are stored as degrees east so we need to subtract 360\n",
        "        lon_values = (ds[m].longitude.values - 360 < max_lon + expand_search) & (ds[m].longitude.values - 360 > min_lon - expand_search)\n",
        "        mask = np.multiply(lat_values,lon_values)\n",
        "        m_value = ds[m].values[mask].mean()\n",
        "        row_list.append( m_value )\n",
        "      all_data.append(row_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ybn_H48Bzk9S"
      },
      "outputs": [],
      "source": [
        "climate_df = pd.DataFrame(all_data[1:], columns=all_data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxT2D0Vl0R7G"
      },
      "outputs": [],
      "source": [
        "climate_df.to_parquet(f'/content/drive/MyDrive/snocast/eval/data/hrrr/climate_{run_date}.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilwZjqCXy5yg"
      },
      "outputs": [],
      "source": [
        "climate_df.sort_values(['location_id','date']).head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2LXKQGxy7Cq"
      },
      "outputs": [],
      "source": [
        "climate_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPA01IHIg8Ql"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "get_climate_eval.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('test-env')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "2de28f3ad7dd53d2a011fa8319efeaa9697028b633ab3b8e0b37bcc0e58faf2b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
