{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "get_water_bodies_train_test.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "background_execution": "on",
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Get Water Bodies Data\n",
        "This notebook retrieves the water body data from the Driven Data Public Assets S3 bucket and then determines the amount of water bodies for all test and train grid cells and saves it into the data/static directory."
      ],
      "metadata": {
        "id": "byh1gjbMYEDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install boto3"
      ],
      "metadata": {
        "id": "fb1k63ipVc_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "nqOkTpVfCvi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muqUwpX4_9o7"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import json\n",
        "import boto3\n",
        "import netCDF4\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BUCKET_NAME = 'drivendata-public-assets' # replace with your bucket name\n",
        "\n",
        "# enter authentication credentials\n",
        "s3 = boto3.resource('s3', aws_access_key_id = 'aws_access_key_id', \n",
        "                          aws_secret_access_key = 'aws_secret_access_key')"
      ],
      "metadata": {
        "id": "uC-cnuT5_c07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KEY = 'water_bodies_map.tar.gz' # replace with your object key\n",
        "\n",
        "try:\n",
        "  # we are trying to download training dataset from s3 with name `my-training-data.csv` \n",
        "  # to colab dir with name `training.csv`\n",
        "  s3.Bucket(BUCKET_NAME).download_file(KEY, 'water_bodies_map.tar.gz')\n",
        "  \n",
        "except botocore.exceptions.ClientError as e:\n",
        "  if e.response['Error']['Code'] == \"404\":\n",
        "    print(\"The object does not exist.\")\n",
        "  else:\n",
        "    raise"
      ],
      "metadata": {
        "id": "6P6fzkzF_cwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xf /content/water_bodies_map.tar.gz"
      ],
      "metadata": {
        "id": "UviWh1g2_ctX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fp='/content/ESACCI-LC-L4-WB-Map-150m-P13Y-2000-v4.0.nc' # your file name with the eventual path\n",
        "nc = netCDF4.Dataset(fp) # reading the nc file and creating Dataset"
      ],
      "metadata": {
        "id": "99XBnoFlFMBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wb_class = nc.variables['wb_class']\n",
        "wb_lat = np.array(nc.variables['lat'])\n",
        "wb_lon = np.array(nc.variables['lon'])"
      ],
      "metadata": {
        "id": "Of9VYyarFRlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Base Data Files"
      ],
      "metadata": {
        "id": "bUIQBQ-UI_Z0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/drive/MyDrive/snocast/train/data'\n",
        "\n",
        "ground_measures_train = pd.read_csv(os.path.join(data_dir, 'static/ground_measures_train_features.csv'))\n",
        "ground_measures_train.columns = ['station_id'] + list(ground_measures_train.columns[1:])\n",
        "gm_melt_train = ground_measures_train.melt(id_vars=[\"station_id\"],\n",
        "                                            var_name=\"date\",\n",
        "                                            value_name=\"swe\").dropna()\n",
        "            \n",
        "\n",
        "ground_measures_test = pd.read_csv(os.path.join(data_dir, 'static/ground_measures_test_features.csv'))\n",
        "ground_measures_test.columns = ['station_id'] + list(ground_measures_test.columns[1:])\n",
        "gm_melt_test = ground_measures_test.melt(id_vars=[\"station_id\"],\n",
        "                           var_name=\"date\",\n",
        "                           value_name=\"swe\").dropna()\n",
        "                           \n",
        "ground_measures_metadata = pd.read_csv(os.path.join(data_dir, 'static/ground_measures_metadata.csv'))\n",
        "ground_measures_all = pd.merge(ground_measures_train, ground_measures_test, how='outer', on='station_id')\n",
        "gm_melt_all = ground_measures_all.melt(id_vars=[\"station_id\"],\n",
        "                           var_name=\"date\",\n",
        "                           value_name=\"swe\").dropna()\n",
        "gm_seq = pd.merge(gm_melt_all, ground_measures_metadata, how='inner', on='station_id')\n",
        "\n",
        "train_labels = pd.read_csv(os.path.join(data_dir, 'static/train_labels.csv'))\n",
        "labels_melt_train = train_labels.melt(id_vars=[\"cell_id\"],\n",
        "                  var_name=\"date\",\n",
        "                  value_name=\"swe\").dropna()\n",
        "\n",
        "test_labels = pd.read_csv(os.path.join(data_dir, 'static/labels_2020_2021.csv'))\n",
        "labels_melt_test = test_labels.melt(id_vars=[\"cell_id\"],\n",
        "                  var_name=\"date\",\n",
        "                  value_name=\"swe\").dropna()"
      ],
      "metadata": {
        "id": "mshiRZnECY4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get latitude longitude for train and test grids\n",
        "f = open(os.path.join(data_dir, 'static/grid_cells.geojson'))\n",
        "grid_cells = json.load(f)\n",
        "print('length grid_cells features: ', len(grid_cells['features']))\n",
        "\n",
        "grid_features = defaultdict(dict)\n",
        "for grid_cell in grid_cells['features']:\n",
        "  cell_id = grid_cell['properties']['cell_id']\n",
        "  coordinates = grid_cell['geometry']['coordinates'][0]\n",
        "  region = grid_cell['properties']['region']\n",
        "  grid_features[cell_id] = {'coordinates': coordinates[1:],\n",
        "                            'region': region}\n",
        "\n",
        "grid_features_train = defaultdict(dict)\n",
        "train_ids = []\n",
        "train_lats = []\n",
        "train_lons = []\n",
        "train_regions = []\n",
        "train_bboxes = []\n",
        "grid_features_test = defaultdict(dict)\n",
        "test_ids = []\n",
        "test_lats = []\n",
        "test_lons = []\n",
        "test_regions = []\n",
        "test_bboxes = []\n",
        "\n",
        "\n",
        "for cell_id in train_labels['cell_id'].values:\n",
        "  train_ids.append(cell_id)\n",
        "  lon, lat = np.mean(grid_features[cell_id]['coordinates'], axis=0)\n",
        "  max_lon, max_lat = np.max(grid_features[cell_id]['coordinates'], axis=0)\n",
        "  min_lon, min_lat = np.min(grid_features[cell_id]['coordinates'], axis=0)\n",
        "  # bbox = [min_lon, min_lat, max_lon, max_lat]\n",
        "  bbox = np.array([min_lon, min_lat,max_lon, max_lat])\n",
        "  train_regions = grid_features[cell_id]['region']\n",
        "  train_lats.append(lat)\n",
        "  train_lons.append(lon)\n",
        "  train_bboxes.append(bbox)\n",
        "\n",
        "  grid_features[cell_id]['dataset'] = 'train'\n",
        "\n",
        "for cell_id in test_labels['cell_id'].values:\n",
        "  test_ids.append(cell_id)\n",
        "  lon, lat = np.mean(grid_features[cell_id]['coordinates'], axis=0)\n",
        "  max_lon, max_lat = np.max(grid_features[cell_id]['coordinates'], axis=0)\n",
        "  min_lon, min_lat = np.min(grid_features[cell_id]['coordinates'], axis=0)\n",
        "  # bbox = [min_lon, min_lat, max_lon, max_lat]\n",
        "  bbox = np.array([min_lon, min_lat,max_lon, max_lat])\n",
        "  test_regions = grid_features[cell_id]['region']\n",
        "  test_lats.append(lat)\n",
        "  test_lons.append(lon)\n",
        "  test_bboxes.append(bbox)\n",
        "\n",
        "  if 'dataset' in grid_features[cell_id].keys():\n",
        "    grid_features[cell_id]['dataset'] = 'both'\n",
        "  else:\n",
        "    grid_features[cell_id]['dataset'] = 'test'\n",
        "\n",
        "for cell_id in grid_features:\n",
        "  if grid_features[cell_id]['dataset'] in ('test','both'):\n",
        "    grid_features_test[cell_id] = grid_features[cell_id]\n",
        "  if grid_features[cell_id]['dataset'] in ('train','both'):\n",
        "    grid_features_train[cell_id] = grid_features[cell_id]\n",
        "print(\"test count: \", len(grid_features_test))\n",
        "print(\"train count: \", len(grid_features_train))\n",
        "\n",
        "\n",
        "train_lat_lon = pd.DataFrame({'cell_id': train_ids, \n",
        "                              'latitude': train_lats, \n",
        "                              'longitude': train_lons, \n",
        "                              'region': train_regions,\n",
        "                              'bbox': train_bboxes})\n",
        "test_lat_lon = pd.DataFrame({'cell_id': test_ids, \n",
        "                             'latitude': test_lats, \n",
        "                             'longitude': test_lons, \n",
        "                             'region': test_regions,\n",
        "                             'bbox': test_bboxes})"
      ],
      "metadata": {
        "id": "lBpi19GiERNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_water_bodies(df):\n",
        "  all_max_lat = df.latitude.max()\n",
        "  all_min_lat = df.latitude.min()\n",
        "  all_max_lon = df.longitude.max()\n",
        "  all_min_lon = df.longitude.min()\n",
        "\n",
        "  # Trim water body file to only relevant lat/lon\n",
        "  wb_lat_values = (wb_lat < all_max_lat) & (wb_lat > all_min_lat)\n",
        "  wb_lon_values = (wb_lon < all_max_lon) & (wb_lon > all_min_lon)\n",
        "\n",
        "  reduced_wb = wb_class[wb_lat_values, wb_lon_values]\n",
        "  reduced_lat = wb_lat[wb_lat_values]\n",
        "  reduced_lon = wb_lon[wb_lon_values]\n",
        "\n",
        "  mean_wb_arr = []\n",
        "\n",
        "  for idx, row in df.iterrows():\n",
        "    if idx % 500 == 0:\n",
        "      print(idx)\n",
        "    min_lon, min_lat, max_lon, max_lat = row['bbox']\n",
        "\n",
        "    lat_values = (reduced_lat < max_lat) & (reduced_lat > min_lat)\n",
        "    lon_values = (reduced_lon < max_lon) & (reduced_lon > min_lon)\n",
        "    mask = lon_values[np.newaxis, :] & lat_values[:, np.newaxis]\n",
        "\n",
        "    mean_wb = reduced_wb[mask].mean()\n",
        "    mean_wb_arr.append(mean_wb)\n",
        "\n",
        "  return mean_wb_arr"
      ],
      "metadata": {
        "id": "DqDAPqBFWy0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_wb_mean = get_water_bodies(test_lat_lon)\n",
        "test_lat_lon['water'] = test_wb_mean"
      ],
      "metadata": {
        "id": "0Vxp3OasXOo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_lat_lon = test_lat_lon[['cell_id', 'water']]"
      ],
      "metadata": {
        "id": "97GgBrrsVsKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_lat_lon.to_parquet('/content/drive/MyDrive/snocast/train/data/static/test_water.parquet')"
      ],
      "metadata": {
        "id": "CxqHmVHmVz0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_wb_mean = get_water_bodies(train_lat_lon)\n",
        "train_lat_lon['water'] = train_wb_mean"
      ],
      "metadata": {
        "id": "CY-PYlUsa2xY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_lat_lon = train_lat_lon[['cell_id', 'water']]"
      ],
      "metadata": {
        "id": "CEYgK2pDXt7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_lat_lon.to_parquet('/content/drive/MyDrive/snocast/train/data/static/train_water.parquet')"
      ],
      "metadata": {
        "id": "yOwG_5pqXt0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VBb9HXcgX8L3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}