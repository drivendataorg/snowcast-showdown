{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "get_elevation_gradient_all.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "background_execution": "on",
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Get Elevation Gradient data from Copernicus Digital Elevation Model (DEM)\n",
        "Elevation data was provided for the ground measures but not for the test and train datasets. This notebook pulls the southern and eastern elevation gradient for the ground measurements and the test and train grid cells and saves it into the data/static directory."
      ],
      "metadata": {
        "id": "zgLU5OtF-uUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pystac_client\n",
        "!pip install planetary_computer\n",
        "!pip install rasterio\n",
        "!pip install xarray-spatial"
      ],
      "metadata": {
        "id": "ujSkaroF-hXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "nqOkTpVfCvi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muqUwpX4_9o7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import planetary_computer\n",
        "import xarray\n",
        "import time\n",
        "from collections import defaultdict\n",
        "from pystac_client import Client\n",
        "import rasterio\n",
        "from rasterio import windows\n",
        "from rasterio import features\n",
        "from rasterio import warp\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze"
      ],
      "metadata": {
        "id": "FjTel2f5_XmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Base Data Files"
      ],
      "metadata": {
        "id": "bUIQBQ-UI_Z0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/drive/MyDrive/snocast/train/data'\n",
        "\n",
        "ground_measures_train = pd.read_csv(os.path.join(data_dir, 'static/ground_measures_train_features.csv'))\n",
        "ground_measures_train.columns = ['station_id'] + list(ground_measures_train.columns[1:])\n",
        "gm_melt_train = ground_measures_train.melt(id_vars=[\"station_id\"],\n",
        "                                            var_name=\"date\",\n",
        "                                            value_name=\"swe\").dropna()\n",
        "            \n",
        "\n",
        "ground_measures_test = pd.read_csv('/content/drive/MyDrive/snocast/dev/ground_measures_test_features.csv')\n",
        "ground_measures_test.columns = ['station_id'] + list(ground_measures_test.columns[1:])\n",
        "gm_melt_test = ground_measures_test.melt(id_vars=[\"station_id\"],\n",
        "                           var_name=\"date\",\n",
        "                           value_name=\"swe\").dropna()\n",
        "\n",
        "ground_measures_metadata = pd.read_csv('/content/drive/MyDrive/snocast/dev/ground_measures_metadata.csv')\n",
        "ground_measures_all = pd.concat([ground_measures_train, ground_measures_test], axis=1)\n",
        "\n",
        "\n",
        "train_labels = pd.read_csv('/content/drive/MyDrive/snocast/dev/train_labels.csv')\n",
        "labels_melt_train = train_labels.melt(id_vars=[\"cell_id\"],\n",
        "                  var_name=\"date\",\n",
        "                  value_name=\"swe\").dropna()\n",
        "\n",
        "submission_format = pd.read_csv('/content/drive/MyDrive/snocast/dev/submission_format.csv')\n",
        "preds_melt_test = submission_format.melt(id_vars=[\"cell_id\"],\n",
        "                  var_name=\"date\",\n",
        "                  value_name=\"swe\").dropna()\n",
        "\n",
        "train_label_unique = pd.read_csv('/content/drive/MyDrive/snocast/dev/train_label_unique.csv')\n",
        "test_pred_unique = pd.read_csv('/content/drive/MyDrive/snocast/dev/test_pred_unique.csv')"
      ],
      "metadata": {
        "id": "mshiRZnECY4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get latitude longitude for train and test grids\n",
        "f = open('/content/drive/MyDrive/snocast/dev/grid_cells.geojson')\n",
        "grid_cells = json.load(f)\n",
        "print('length grid_cells features: ', len(grid_cells['features']))\n",
        "\n",
        "grid_features = defaultdict(dict)\n",
        "for grid_cell in grid_cells['features']:\n",
        "  cell_id = grid_cell['properties']['cell_id']\n",
        "  coordinates = grid_cell['geometry']['coordinates'][0]\n",
        "  region = grid_cell['properties']['region']\n",
        "  grid_features[cell_id] = {'coordinates': coordinates[1:],\n",
        "                            'region': region,\n",
        "                            'geometry': grid_cell['geometry']}\n",
        "\n",
        "grid_features_train = defaultdict(dict)\n",
        "train_ids = []\n",
        "train_lats = []\n",
        "train_lons = []\n",
        "train_regions = []\n",
        "train_bboxes = []\n",
        "grid_features_test = defaultdict(dict)\n",
        "test_ids = []\n",
        "test_lats = []\n",
        "test_lons = []\n",
        "test_regions = []\n",
        "test_bboxes = []\n",
        "\n",
        "\n",
        "for cell_id in train_labels['cell_id'].values:\n",
        "  train_ids.append(cell_id)\n",
        "  lon, lat = np.mean(grid_features[cell_id]['coordinates'], axis=0)\n",
        "  northeast_corner = np.max(grid_features[cell_id]['coordinates'], axis=0)\n",
        "  southwest_corner = np.min(grid_features[cell_id]['coordinates'], axis=0)\n",
        "  # bbox = [min_lon, min_lat, max_lon, max_lat]\n",
        "  bbox = np.concatenate([southwest_corner,northeast_corner])\n",
        "  train_regions.append(grid_features[cell_id]['region'])\n",
        "  train_lats.append(lat)\n",
        "  train_lons.append(lon)\n",
        "  train_bboxes.append(bbox)\n",
        "\n",
        "  grid_features[cell_id]['dataset'] = 'train'\n",
        "\n",
        "for cell_id in submission_format['cell_id'].values:\n",
        "  test_ids.append(cell_id)\n",
        "  lon, lat = np.mean(grid_features[cell_id]['coordinates'], axis=0)\n",
        "  northeast_corner = np.max(grid_features[cell_id]['coordinates'], axis=0)\n",
        "  southwest_corner = np.min(grid_features[cell_id]['coordinates'], axis=0)\n",
        "  bbox = np.concatenate([southwest_corner,northeast_corner])\n",
        "  test_regions.append(grid_features[cell_id]['region'])\n",
        "  test_lats.append(lat)\n",
        "  test_lons.append(lon)\n",
        "  test_bboxes.append(bbox)\n",
        "\n",
        "  if 'dataset' in grid_features[cell_id].keys():\n",
        "    grid_features[cell_id]['dataset'] = 'both'\n",
        "  else:\n",
        "    grid_features[cell_id]['dataset'] = 'test'\n",
        "\n",
        "for cell_id in grid_features:\n",
        "  if grid_features[cell_id]['dataset'] in ('test','both'):\n",
        "    grid_features_test[cell_id] = grid_features[cell_id]\n",
        "  if grid_features[cell_id]['dataset'] in ('train','both'):\n",
        "    grid_features_train[cell_id] = grid_features[cell_id]\n",
        "print(\"test count: \", len(grid_features_test))\n",
        "print(\"train count: \", len(grid_features_train))\n",
        "\n",
        "\n",
        "train_lat_lon = pd.DataFrame({'cell_id': train_ids, \n",
        "                              'latitude': train_lats, \n",
        "                              'longitude': train_lons, \n",
        "                              'region': train_regions,\n",
        "                              'bbox': train_bboxes})\n",
        "test_lat_lon = pd.DataFrame({'cell_id': test_ids, \n",
        "                             'latitude': test_lats, \n",
        "                             'longitude': test_lons, \n",
        "                             'region': test_regions,\n",
        "                             'bbox': test_bboxes})"
      ],
      "metadata": {
        "id": "lBpi19GiERNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Data for Copernicus Digital Elevation Model (DEM)"
      ],
      "metadata": {
        "id": "8cI2IEu497Ya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_band(href, aoi):\n",
        "    with rasterio.open(href) as ds:\n",
        "        aoi_bounds = features.bounds(aoi)\n",
        "        warped_aoi_bounds = warp.transform_bounds(\"epsg:4326\", ds.crs, *aoi_bounds)\n",
        "        aoi_window = windows.from_bounds(transform=ds.transform, *warped_aoi_bounds)\n",
        "        try:\n",
        "          data = ds.read(1, window=aoi_window)\n",
        "        except:\n",
        "          data = x = np.array([[0, 0],[0, 0]])\n",
        "        return data"
      ],
      "metadata": {
        "id": "7EuGyCxUiSD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = Client.open(\n",
        "    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
        "    ignore_conformance=True,\n",
        ")"
      ],
      "metadata": {
        "id": "GEpn6lCK97FB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = test_lat_lon"
      ],
      "metadata": {
        "id": "EGdXRSrWSlyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "MwnAYARmTaE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aoi = grid_features['000ba8d9-d6d5-48da-84a2-1fa54951fae1']['geometry']\n",
        "aoi"
      ],
      "metadata": {
        "id": "7hL1TVxwWL5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all relevant items within the lat/lon bounds of the df\n",
        "search = client.search(\n",
        "    collections=[\"cop-dem-glo-30\"],\n",
        "    intersects=aoi,\n",
        ")\n",
        "\n",
        "items = list(search.get_items())\n",
        "print(f\"Returned {len(items)} items\")"
      ],
      "metadata": {
        "id": "TmAG_V28Sfk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "signed_asset = planetary_computer.sign(items[0].assets[\"data\"])\n",
        "elev_matrix = read_band(signed_asset.href, aoi)"
      ],
      "metadata": {
        "id": "IIvMYakRh2l8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(elev_matrix)\n",
        "plt.colorbar()"
      ],
      "metadata": {
        "id": "rUnH1cTAfTfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "southern_gradient = -1*np.diff(elev_matrix, axis=0)\n",
        "plt.imshow(southern_gradient)\n",
        "plt.colorbar()\n",
        "print(southern_gradient.mean())"
      ],
      "metadata": {
        "id": "pomU9Gnkj63U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "southern_gradient.shape"
      ],
      "metadata": {
        "id": "HFytwMfrk_eU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h, w = southern_gradient.shape\n",
        "(southern_gradient < 0).sum()/(h*w)"
      ],
      "metadata": {
        "id": "eOaF3yb4kqC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eastern_gradient = np.diff(elev_matrix, axis=1)\n",
        "plt.imshow(eastern_gradient)\n",
        "plt.colorbar()\n",
        "print(eastern_gradient.mean())"
      ],
      "metadata": {
        "id": "RerGjz4fV7HA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_elevations(df, gm=False):\n",
        "  east_grads = []\n",
        "  south_grads = []\n",
        "  east_pcts = []\n",
        "  south_pcts = []\n",
        "  ids = []\n",
        "\n",
        "  for idx, row in df.iterrows():\n",
        "    if idx % 250 == 0:\n",
        "      print(idx)\n",
        "    if gm:\n",
        "      cell_id = row['station_id']\n",
        "      lat = row['latitude']\n",
        "      lon = row['longitude']\n",
        "      eps = 0.001\n",
        "      aoi = {'coordinates': [[[lon-eps, lat-eps],\n",
        "                              [lon-eps, lat+eps],\n",
        "                              [lon+eps, lat+eps],\n",
        "                              [lon+eps, lat-eps],\n",
        "                              [lon-eps, lat-eps]]],\n",
        "                            'type': 'Polygon'}\n",
        "    else:\n",
        "      cell_id = row['cell_id']\n",
        "      aoi = grid_features[cell_id]['geometry']\n",
        "\n",
        "    # Get all relevant items within the lat/lon bounds of the df\n",
        "    search = client.search(\n",
        "        collections=[\"cop-dem-glo-30\"],\n",
        "        intersects=aoi,\n",
        "    )\n",
        "\n",
        "    need_item = True\n",
        "    num_tries = 0\n",
        "    while need_item:\n",
        "      try:\n",
        "        items = list(search.get_items())\n",
        "        need_item = False\n",
        "      except:\n",
        "        num_tries += 1\n",
        "        print('exception')\n",
        "        time.sleep(1)\n",
        "        if num_tries > 3:\n",
        "          need_item = False\n",
        "          print('give up')\n",
        "    \n",
        "    if num_tries <= 3:\n",
        "      loc_east_grads = []\n",
        "      loc_south_grads = []\n",
        "      loc_east_low = []\n",
        "      loc_south_low = []\n",
        "      loc_east_size = []\n",
        "      loc_south_size = []\n",
        "      for item in items:\n",
        "        signed_asset = planetary_computer.sign(item.assets[\"data\"])\n",
        "        elev_matrix = read_band(signed_asset.href, aoi)\n",
        "        eastern_grad = (np.diff(elev_matrix, axis=1))\n",
        "        e_h, e_w = eastern_grad.shape\n",
        "        east_low = (eastern_grad > 0).sum()\n",
        "        east_size = e_h*e_w\n",
        "        southern_grad = (-1*np.diff(elev_matrix, axis=0))\n",
        "        s_h, s_w = southern_grad.shape\n",
        "        south_low = (southern_grad > 0).sum()\n",
        "        south_size = s_h*s_w\n",
        "        loc_east_grads.append(eastern_grad.mean())\n",
        "        loc_south_grads.append(southern_grad.mean())\n",
        "        loc_east_low.append(east_low)\n",
        "        loc_east_size.append(east_size)\n",
        "        loc_south_low.append(south_low)\n",
        "        loc_south_size.append(south_size)\n",
        "\n",
        "      east_pct = np.sum(loc_east_low)/np.sum(loc_east_size)\n",
        "      east_pcts.append(east_pct)\n",
        "      east_grads.append(np.mean(loc_east_grads))\n",
        "      south_pct = np.sum(loc_south_low)/np.sum(loc_south_size)\n",
        "      south_pcts.append(south_pct)\n",
        "      south_grads.append(np.mean(loc_south_grads))\n",
        "      ids.append(cell_id)\n",
        "\n",
        "  return east_grads, south_grads, ids, east_pcts, south_pcts\n"
      ],
      "metadata": {
        "id": "0-NV-oTkaInq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_east_grads, test_south_grads, test_ids, test_east_pcts, test_south_pcts = get_elevations(test_lat_lon)"
      ],
      "metadata": {
        "id": "6w0zOTGbeNf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_elev_grads = pd.DataFrame({'cell_id': test_ids, \n",
        "                                'east_elev_grad': test_east_grads, \n",
        "                                'south_elev_grad': test_south_grads,\n",
        "                                'east_elev_pct': test_east_pcts,\n",
        "                                'south_elev_pct': test_south_pcts})"
      ],
      "metadata": {
        "id": "J2CBwbqqsBH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_elev_grads.to_parquet('/content/drive/MyDrive/snocast/train/data/static/test_elevation_grads.parquet')"
      ],
      "metadata": {
        "id": "d2btEA5Leg_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_east_grads, train_south_grads, train_ids, train_east_pcts, train_south_pcts = get_elevations(train_lat_lon)\n",
        "train_elev_grads = pd.DataFrame({'cell_id': train_ids, \n",
        "                                 'east_elev_grad': train_east_grads, \n",
        "                                 'south_elev_grad': train_south_grads,\n",
        "                                 'east_elev_pct': train_east_pcts,\n",
        "                                 'south_elev_pct': train_south_pcts})"
      ],
      "metadata": {
        "id": "P2qT8jLte0Z6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_elev_grads.head()"
      ],
      "metadata": {
        "id": "HA9n4EbgbXMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_elev_grads.to_parquet('/content/drive/MyDrive/snocast/train/data/static/train_elevation_grads.parquet')"
      ],
      "metadata": {
        "id": "L5JVEpare0Q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gm_east_grads, gm_south_grads, gm_ids, gm_east_pcts, gm_south_pcts = get_elevations(ground_measures_metadata, gm=True)\n",
        "gm_elev_grads = pd.DataFrame({'station_id': gm_ids, \n",
        "                              'east_elev_grad': gm_east_grads, \n",
        "                              'south_elev_grad': gm_south_grads,\n",
        "                              'east_elev_pct': gm_east_pcts,\n",
        "                              'south_elev_pct': gm_south_pcts})"
      ],
      "metadata": {
        "id": "JiO75r7hAleJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gm_elev_grads.to_parquet('/content/drive/MyDrive/snocast/train/data/static/gm_elevation_grads.parquet')"
      ],
      "metadata": {
        "id": "sTl4dkba8c0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kIavnHPhG_lS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}