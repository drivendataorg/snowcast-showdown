{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# These libraries are needed for the pygrib library in Colab. \n",
        "# Note that is needed if you install pygrib using pip.\n",
        "# If you use conda, the libraries will be installed automatically.\n",
        "! apt-get install libeccodes-dev libproj-dev\n",
        "\n",
        "# Install the python packages\n",
        "! pip install pyproj\n",
        "! pip install pygrib\n",
        "\n",
        "# Uninstall existing shapely\n",
        "# We will re-install shapely in the next step by ignoring the binary\n",
        "# wheels to make it compatible with other modules that depend on \n",
        "# GEOS, such as Cartopy (used here).\n",
        "!pip uninstall --yes shapely\n",
        "\n",
        "# To install cartopy in Colab using pip, we need to install the library \n",
        "# dependencies first.\n",
        "\n",
        "!apt-get install -qq libgdal-dev libgeos-dev\n",
        "!pip install shapely --no-binary shapely\n",
        "!pip install cfgrib"
      ],
      "metadata": {
        "id": "KQtZyiZCzqpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqOkTpVfCvi7"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muqUwpX4_9o7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import io\n",
        "from datetime import date, datetime, timedelta\n",
        "import tempfile\n",
        "\n",
        "import xarray as xr\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Not used directly, but used via xarray\n",
        "import cfgrib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mshiRZnECY4C"
      },
      "outputs": [],
      "source": [
        "data_dir = '/content/drive/MyDrive/snocast/train/data'\n",
        "\n",
        "ground_measures_train = pd.read_csv(os.path.join(data_dir, 'static/ground_measures_train_features.csv'))\n",
        "ground_measures_train.columns = ['station_id'] + list(ground_measures_train.columns[1:])\n",
        "gm_melt_train = ground_measures_train.melt(id_vars=[\"station_id\"],\n",
        "                                            var_name=\"date\",\n",
        "                                            value_name=\"swe\").dropna()\n",
        "            \n",
        "\n",
        "ground_measures_test = pd.read_csv(os.path.join(data_dir, 'static/ground_measures_test_features.csv'))\n",
        "ground_measures_test.columns = ['station_id'] + list(ground_measures_test.columns[1:])\n",
        "gm_melt_test = ground_measures_test.melt(id_vars=[\"station_id\"],\n",
        "                           var_name=\"date\",\n",
        "                           value_name=\"swe\").dropna()\n",
        "                           \n",
        "ground_measures_metadata = pd.read_csv(os.path.join(data_dir, 'static/ground_measures_metadata.csv'))\n",
        "ground_measures_all = pd.merge(ground_measures_train, ground_measures_test, how='outer', on='station_id')\n",
        "gm_melt_all = ground_measures_all.melt(id_vars=[\"station_id\"],\n",
        "                           var_name=\"date\",\n",
        "                           value_name=\"swe\").dropna()\n",
        "gm_seq = pd.merge(gm_melt_all, ground_measures_metadata, how='inner', on='station_id')\n",
        "\n",
        "train_labels = pd.read_csv(os.path.join(data_dir, 'static/train_labels.csv'))\n",
        "labels_melt_train = train_labels.melt(id_vars=[\"cell_id\"],\n",
        "                  var_name=\"date\",\n",
        "                  value_name=\"swe\").dropna()\n",
        "\n",
        "test_labels = pd.read_csv(os.path.join(data_dir, 'static/labels_2020_2021.csv'))\n",
        "labels_melt_test = test_labels.melt(id_vars=[\"cell_id\"],\n",
        "                  var_name=\"date\",\n",
        "                  value_name=\"swe\").dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnMIE6etTwl4"
      },
      "source": [
        "## Data Transform"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get latitude longitude for train and test grids\n",
        "f = open(os.path.join(data_dir, 'static/grid_cells.geojson'))\n",
        "grid_cells = json.load(f)\n",
        "print('length grid_cells features: ', len(grid_cells['features']))\n",
        "\n",
        "grid_features = defaultdict(dict)\n",
        "for grid_cell in grid_cells['features']:\n",
        "  cell_id = grid_cell['properties']['cell_id']\n",
        "  coordinates = grid_cell['geometry']['coordinates'][0]\n",
        "  region = grid_cell['properties']['region']\n",
        "  grid_features[cell_id] = {'coordinates': coordinates[1:],\n",
        "                            'region': region}\n",
        "\n",
        "grid_features_train = defaultdict(dict)\n",
        "train_ids = []\n",
        "train_lats = []\n",
        "train_lons = []\n",
        "train_regions = []\n",
        "train_bboxes = []\n",
        "grid_features_test = defaultdict(dict)\n",
        "test_ids = []\n",
        "test_lats = []\n",
        "test_lons = []\n",
        "test_regions = []\n",
        "test_bboxes = []\n",
        "\n",
        "\n",
        "for cell_id in train_labels['cell_id'].values:\n",
        "  train_ids.append(cell_id)\n",
        "  lon, lat = np.mean(grid_features[cell_id]['coordinates'], axis=0)\n",
        "  max_lon, max_lat = np.max(grid_features[cell_id]['coordinates'], axis=0)\n",
        "  min_lon, min_lat = np.min(grid_features[cell_id]['coordinates'], axis=0)\n",
        "  # bbox = [min_lon, min_lat, max_lon, max_lat]\n",
        "  bbox = np.array([min_lon, min_lat,max_lon, max_lat])\n",
        "  train_regions = grid_features[cell_id]['region']\n",
        "  train_lats.append(lat)\n",
        "  train_lons.append(lon)\n",
        "  train_bboxes.append(bbox)\n",
        "\n",
        "  grid_features[cell_id]['dataset'] = 'train'\n",
        "\n",
        "for cell_id in test_labels['cell_id'].values:\n",
        "  test_ids.append(cell_id)\n",
        "  lon, lat = np.mean(grid_features[cell_id]['coordinates'], axis=0)\n",
        "  max_lon, max_lat = np.max(grid_features[cell_id]['coordinates'], axis=0)\n",
        "  min_lon, min_lat = np.min(grid_features[cell_id]['coordinates'], axis=0)\n",
        "  # bbox = [min_lon, min_lat, max_lon, max_lat]\n",
        "  bbox = np.array([min_lon, min_lat,max_lon, max_lat])\n",
        "  test_regions = grid_features[cell_id]['region']\n",
        "  test_lats.append(lat)\n",
        "  test_lons.append(lon)\n",
        "  test_bboxes.append(bbox)\n",
        "\n",
        "  if 'dataset' in grid_features[cell_id].keys():\n",
        "    grid_features[cell_id]['dataset'] = 'both'\n",
        "  else:\n",
        "    grid_features[cell_id]['dataset'] = 'test'\n",
        "\n",
        "for cell_id in grid_features:\n",
        "  if grid_features[cell_id]['dataset'] in ('test','both'):\n",
        "    grid_features_test[cell_id] = grid_features[cell_id]\n",
        "  if grid_features[cell_id]['dataset'] in ('train','both'):\n",
        "    grid_features_train[cell_id] = grid_features[cell_id]\n",
        "print(\"test count: \", len(grid_features_test))\n",
        "print(\"train count: \", len(grid_features_train))\n",
        "\n",
        "\n",
        "train_lat_lon = pd.DataFrame({'cell_id': train_ids, \n",
        "                              'latitude': train_lats, \n",
        "                              'longitude': train_lons, \n",
        "                              'region': train_regions,\n",
        "                              'bbox': train_bboxes})\n",
        "test_lat_lon = pd.DataFrame({'cell_id': test_ids, \n",
        "                             'latitude': test_lats, \n",
        "                             'longitude': test_lons, \n",
        "                             'region': test_regions,\n",
        "                             'bbox': test_bboxes})"
      ],
      "metadata": {
        "id": "8eaWiVLymfZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create sequential dataframes for train and test\n",
        "train_label_seq = pd.merge(labels_melt_train, train_lat_lon, how='inner', on='cell_id')\n",
        "train_label_seq['datetime'] = pd.to_datetime(train_label_seq['date'])\n",
        "\n",
        "test_label_seq = pd.merge(labels_melt_test, test_lat_lon, how='inner', on='cell_id')\n",
        "test_label_seq['datetime'] = pd.to_datetime(test_label_seq['date'])"
      ],
      "metadata": {
        "id": "c4Hz-Kphmpzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bbox(row, expand):\n",
        "  # bbox = [min_lon, min_lat, max_lon, max_lat]\n",
        "  lat = row['latitude']\n",
        "  lon = row['longitude']\n",
        "  return [lon - expand, lat - expand, lon + expand, lat + expand]"
      ],
      "metadata": {
        "id": "MKkN3BhtPFdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create sequential swe by latitude and longitude for ground measure stations\n",
        "gm_seq['bbox'] = gm_seq[['latitude', 'longitude']].apply(lambda x: get_bbox(x, 0.005), axis=1)\n",
        "gm_seq['datetime'] = pd.to_datetime(gm_seq['date'])"
      ],
      "metadata": {
        "id": "YMCn4ZYuO7_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cI2IEu497Ya"
      },
      "source": [
        "## Get NOAA HRRR Data\n",
        "The NOAA HRRR is a real-time 3km resolution, hourly updated, cloud-resolving, convection-allowing atmospheric model, initialized by 3km grids with 3km radar assimilation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEpn6lCK97FB"
      },
      "outputs": [],
      "source": [
        "# Constants for creating the full URL\n",
        "blob_container = \"https://noaahrrr.blob.core.windows.net/hrrr\"\n",
        "blob_container = \"https://noaa-hrrr-bdp-pds.s3.amazonaws.com\"\n",
        "sector = \"conus\"\n",
        "yesterday = date.today() - timedelta(days=1)\n",
        "cycle = 16          # time to query\n",
        "forecast_hour = 0   # offset from cycle time\n",
        "product = \"wrfsfcf\" # 2D surface levels\n",
        "\n",
        "# Put it all together\n",
        "file_path = f\"hrrr.t{cycle:02}z.{product}{forecast_hour:02}.grib2\"\n",
        "\n",
        "url = f\"{blob_container}/hrrr.{yesterday:%Y%m%d}/{sector}/{file_path}\"\n",
        "\n",
        "print(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vf2tZOQ5G1Pc"
      },
      "outputs": [],
      "source": [
        "# Fetch the idx file by appending the .idx file extension to our already formatted URL\n",
        "r = requests.get(f\"{url}.idx\")\n",
        "url_idx = r.text.splitlines()\n",
        "\n",
        "# Take a peek at the content of the index\n",
        "print(*url_idx[0:10], sep=\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.nco.ncep.noaa.gov/pmb/products/hrrr/hrrr.t00z.wrfsfcf02.grib2.shtml\n",
        "metrics = [(\":TMP:surface:anl\", \"t\"), # temperature [K]\n",
        "           (\":SNOD:surface:anl\", \"sde\"), # snow depth [m]\n",
        "           (\":WEASD:surface:anl\", \"sdwe\"), # water equivalent of accumulated snow depth [kg/m^2]\n",
        "           (\":SPFH:2 m above ground:anl:\", \"q\"), # specific humidity [kg/kg]\n",
        "           (\":SNOWC:surface:anl:\", \"snowc\"), # snow cover [%]\n",
        "          # (\":ASNOW:surface:0-0 day acc fcst:\", \"asnow\"), # total snowfall [m]\n",
        "          # (\":CSNOW:surface:anl:\", \"csnow\"), # categorical snow [-]\n",
        "           (\":REFC:entire atmosphere:anl:\", \"refc\"), # composite reflectivity [dB]\n",
        "           (\":PRES:surface:anl:\", \"sp\"), # pressure [Pa]\n",
        "           (\":PWAT:entire atmosphere (considered as a single layer):anl:\", \"pwat\"), # precipitable water [kg/m^2]\n",
        "          # (\":ICEC:surface:anl:\", \"ci\"), # ice cover\n",
        "          # (\":TCDC:entire atmosphere:anl:\", # cloud cover\n",
        "          # (\":APCP:surface:\", \"tp\"), # total precipitation [kg/m^2]\n",
        "          # (\":PRATE:surface:anl:\", \"prate\"), # precipitation rate [kg/m^2/s]\n",
        "          # (\":SSRUN\")\n",
        "          # (\":BGRUN\")\n",
        "           ]"
      ],
      "metadata": {
        "id": "TB28XexZ_Mc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_metric_ds(metric, url_idx):\n",
        "  metric_idx = [l for l in url_idx if metric in l][0].split(\":\")\n",
        "  # Pluck the byte offset from this line, plus the beginning offset of the next line\n",
        "  line_num = int(metric_idx[0])\n",
        "  range_start = metric_idx[1]\n",
        "  # The line number values are 1-indexed, so we don't need to increment it to get the next list index,\n",
        "  # but check we're not already reading the last line\n",
        "  next_line = url_idx[line_num].split(':') if line_num < len(url_idx) else None\n",
        "  # Pluck the start of the next byte offset, or nothing if we were on the last line\n",
        "  range_end = next_line[1] if next_line else None\n",
        "  file = tempfile.NamedTemporaryFile(prefix=\"tmp_\", delete=False)\n",
        "\n",
        "  headers = {\"Range\": f\"bytes={range_start}-{range_end}\"}\n",
        "  resp = requests.get(url, headers=headers, stream=True)\n",
        "\n",
        "  with file as f:\n",
        "      f.write(resp.content)\n",
        "\n",
        "  ds = xr.open_dataset(file.name, engine='cfgrib', \n",
        "                      backend_kwargs={'indexpath':''})\n",
        "\n",
        "  return ds"
      ],
      "metadata": {
        "id": "YFSSn7fxBD1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_seq = gm_seq\n",
        "gm = True\n",
        "\n",
        "all_data = [['location_id','date','TMP','SNOD','WEASD','SPFH','SNOWC','REFC','PRES','PWAT']]\n",
        "\n",
        "unique_dates = df_seq[['date','datetime']].drop_duplicates().sort_values(['date'])\n",
        "\n",
        "if gm:\n",
        "  location_col = 'station_id'\n",
        "else:\n",
        "  location_col = 'cell_id'\n",
        "\n",
        "for days in range(3):\n",
        "  for _, row in unique_dates.iterrows():\n",
        "    date = row['date'].strftime('%Y-%m-%d')\n",
        "\n",
        "    # No good climate data earlier than '2015-01-01'\n",
        "    if date >= '2015-01-01':\n",
        "      print(date)\n",
        "      retrieveday = row['datetime'] - timedelta(days=days)\n",
        "      unique_ids = df_seq[df_seq['date'] == date][[location_col,'bbox']]\n",
        "\n",
        "      find_url = True\n",
        "      still_cycles_to_search = True\n",
        "      url_found = False\n",
        "      cycle = 16\n",
        "      while find_url and still_cycles_to_search:\n",
        "        file_path = f\"hrrr.t{cycle:02}z.{product}{forecast_hour:02}.grib2\"\n",
        "        url = f\"{blob_container}/hrrr.{retrieveday:%Y%m%d}/{sector}/{file_path}\"\n",
        "        # Fetch the idx file by appending the .idx file extension to our already formatted URL\n",
        "        r = requests.get(f\"{url}.idx\")\n",
        "        url_idx = r.text.splitlines()\n",
        "        if url_idx[0] == '<?xml version=\"1.0\" encoding=\"UTF-8\"?>':\n",
        "          if cycle == 0:\n",
        "            still_cycles_to_search = False\n",
        "          cycle -= 1\n",
        "          print(f'bad url: {url}')\n",
        "        else:\n",
        "          find_url = False\n",
        "          url_found = True\n",
        "\n",
        "      if url_found:\n",
        "        ds_list = []\n",
        "        for m in metrics:\n",
        "          ds_list.append((get_metric_ds(m[0], url_idx),m[1]))\n",
        "        \n",
        "        for idx, row in unique_ids.iterrows():\n",
        "          location_id = row[location_col]\n",
        "          row_list = [location_id, f'{retrieveday:%Y-%m-%d}']\n",
        "          min_lon, min_lat, max_lon, max_lat = row['bbox']\n",
        "          for ds, m in ds_list:\n",
        "            expand_search = 0.025\n",
        "            lat_values = (ds[m].latitude.values < max_lat + expand_search) & (ds[m].latitude.values > min_lat - expand_search)\n",
        "            # noaa hrrr longitude values are stored as degrees east so we need to subtract 360\n",
        "            lon_values = (ds[m].longitude.values - 360 < max_lon + expand_search) & (ds[m].longitude.values - 360 > min_lon - expand_search)\n",
        "            mask = np.multiply(lat_values,lon_values)\n",
        "            m_value = ds[m].values[mask].mean()\n",
        "            row_list.append( m_value )\n",
        "          all_data.append(row_list)"
      ],
      "metadata": {
        "id": "bspbjQPriSbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gm_climate_df = pd.DataFrame(all_data[1:], columns=all_data[0])"
      ],
      "metadata": {
        "id": "Ybn_H48Bzk9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gm_climate_df.to_parquet(os.path.join(data_dir, 'hrrr/gm_climate.parquet')"
      ],
      "metadata": {
        "id": "DxT2D0Vl0R7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_seq = train_label_seq\n",
        "gm = False\n",
        "\n",
        "all_data = [['location_id','date','TMP','SNOD','WEASD','SPFH','SNOWC','REFC','PRES','PWAT']]\n",
        "\n",
        "unique_dates = df_seq[['date','datetime']].drop_duplicates().sort_values(['date'])\n",
        "\n",
        "if gm:\n",
        "  location_col = 'station_id'\n",
        "else:\n",
        "  location_col = 'cell_id'\n",
        "\n",
        "for days in range(3):\n",
        "  for _, row in unique_dates.iterrows():\n",
        "    date = row['date'].strftime('%Y-%m-%d')\n",
        "\n",
        "    # No good climate data earlier than '2015-01-01'\n",
        "    if date >= '2015-01-01':\n",
        "      print(date)\n",
        "      retrieveday = row['datetime'] - timedelta(days=days)\n",
        "      unique_ids = df_seq[df_seq['date'] == date][[location_col,'bbox']]\n",
        "\n",
        "      find_url = True\n",
        "      still_cycles_to_search = True\n",
        "      url_found = False\n",
        "      cycle = 16\n",
        "      while find_url and still_cycles_to_search:\n",
        "        file_path = f\"hrrr.t{cycle:02}z.{product}{forecast_hour:02}.grib2\"\n",
        "        url = f\"{blob_container}/hrrr.{retrieveday:%Y%m%d}/{sector}/{file_path}\"\n",
        "        # Fetch the idx file by appending the .idx file extension to our already formatted URL\n",
        "        r = requests.get(f\"{url}.idx\")\n",
        "        url_idx = r.text.splitlines()\n",
        "        if url_idx[0] == '<?xml version=\"1.0\" encoding=\"UTF-8\"?>':\n",
        "          if cycle == 0:\n",
        "            still_cycles_to_search = False\n",
        "          cycle -= 1\n",
        "          print(f'bad url: {url}')\n",
        "        else:\n",
        "          find_url = False\n",
        "          url_found = True\n",
        "\n",
        "      if url_found:\n",
        "        ds_list = []\n",
        "        for m in metrics:\n",
        "          ds_list.append((get_metric_ds(m[0], url_idx),m[1]))\n",
        "        \n",
        "        for idx, row in unique_ids.iterrows():\n",
        "          location_id = row[location_col]\n",
        "          row_list = [location_id, f'{retrieveday:%Y-%m-%d}']\n",
        "          min_lon, min_lat, max_lon, max_lat = row['bbox']\n",
        "          for ds, m in ds_list:\n",
        "            expand_search = 0.025\n",
        "            lat_values = (ds[m].latitude.values < max_lat + expand_search) & (ds[m].latitude.values > min_lat - expand_search)\n",
        "            # noaa hrrr longitude values are stored as degrees east so we need to subtract 360\n",
        "            lon_values = (ds[m].longitude.values - 360 < max_lon + expand_search) & (ds[m].longitude.values - 360 > min_lon - expand_search)\n",
        "            mask = np.multiply(lat_values,lon_values)\n",
        "            m_value = ds[m].values[mask].mean()\n",
        "            row_list.append( m_value )\n",
        "          all_data.append(row_list)"
      ],
      "metadata": {
        "id": "me3DteOz1yV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_climate_df = pd.DataFrame(all_data[1:], columns=all_data[0])"
      ],
      "metadata": {
        "id": "SkxM3WYoT0b8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_climate_df.to_parquet(os.path.join(data_dir, 'hrrr/train_climate.parquet')"
      ],
      "metadata": {
        "id": "FjFzZlM8Tz1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_seq = test_label_seq\n",
        "gm = False\n",
        "\n",
        "all_data = [['location_id','date','TMP','SNOD','WEASD','SPFH','SNOWC','REFC','PRES','PWAT']]\n",
        "\n",
        "unique_dates = df_seq[['date','datetime']].drop_duplicates().sort_values(['date'])\n",
        "\n",
        "if gm:\n",
        "  location_col = 'station_id'\n",
        "else:\n",
        "  location_col = 'cell_id'\n",
        "\n",
        "for days in range(3):\n",
        "  for _, row in unique_dates.iterrows():\n",
        "    date = row['date'].strftime('%Y-%m-%d')\n",
        "\n",
        "    # No good climate data earlier than '2015-01-01'\n",
        "    if date >= '2015-01-01':\n",
        "      print(date)\n",
        "      retrieveday = row['datetime'] - timedelta(days=days)\n",
        "      unique_ids = df_seq[df_seq['date'] == date][[location_col,'bbox']]\n",
        "\n",
        "      find_url = True\n",
        "      still_cycles_to_search = True\n",
        "      url_found = False\n",
        "      cycle = 16\n",
        "      while find_url and still_cycles_to_search:\n",
        "        file_path = f\"hrrr.t{cycle:02}z.{product}{forecast_hour:02}.grib2\"\n",
        "        url = f\"{blob_container}/hrrr.{retrieveday:%Y%m%d}/{sector}/{file_path}\"\n",
        "        # Fetch the idx file by appending the .idx file extension to our already formatted URL\n",
        "        r = requests.get(f\"{url}.idx\")\n",
        "        url_idx = r.text.splitlines()\n",
        "        if url_idx[0] == '<?xml version=\"1.0\" encoding=\"UTF-8\"?>':\n",
        "          if cycle == 0:\n",
        "            still_cycles_to_search = False\n",
        "          cycle -= 1\n",
        "          print(f'bad url: {url}')\n",
        "        else:\n",
        "          find_url = False\n",
        "          url_found = True\n",
        "\n",
        "      if url_found:\n",
        "        ds_list = []\n",
        "        for m in metrics:\n",
        "          ds_list.append((get_metric_ds(m[0], url_idx),m[1]))\n",
        "        \n",
        "        for idx, row in unique_ids.iterrows():\n",
        "          location_id = row[location_col]\n",
        "          row_list = [location_id, f'{retrieveday:%Y-%m-%d}']\n",
        "          min_lon, min_lat, max_lon, max_lat = row['bbox']\n",
        "          for ds, m in ds_list:\n",
        "            expand_search = 0.025\n",
        "            lat_values = (ds[m].latitude.values < max_lat + expand_search) & (ds[m].latitude.values > min_lat - expand_search)\n",
        "            # noaa hrrr longitude values are stored as degrees east so we need to subtract 360\n",
        "            lon_values = (ds[m].longitude.values - 360 < max_lon + expand_search) & (ds[m].longitude.values - 360 > min_lon - expand_search)\n",
        "            mask = np.multiply(lat_values,lon_values)\n",
        "            m_value = ds[m].values[mask].mean()\n",
        "            row_list.append( m_value )\n",
        "          all_data.append(row_list)"
      ],
      "metadata": {
        "id": "qU9KTLtoTzqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_climate_df = pd.DataFrame(all_data[1:], columns=all_data[0])"
      ],
      "metadata": {
        "id": "lLeeZAnZUDXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_climate_df.to_parquet(os.path.join(data_dir, 'hrrr/test_climate.parquet')"
      ],
      "metadata": {
        "id": "otaqyI7HUDQQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "name": "get_climate_all.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "background_execution": "on"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}