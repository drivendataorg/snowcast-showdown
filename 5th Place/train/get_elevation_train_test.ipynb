{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "get_elevation_train_test.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "background_execution": "on",
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Get Elevation data from Copernicus Digital Elevation Model (DEM)\n",
        "Elevation data was provided for the ground measures but not for the test and train datasets. This notebook pulls the elevation mean and variance for the test and train grid cells and saves it into the data/static directory."
      ],
      "metadata": {
        "id": "zgLU5OtF-uUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pystac_client\n",
        "!pip install planetary_computer\n",
        "!pip install rasterio\n",
        "!pip install xarray-spatial"
      ],
      "metadata": {
        "id": "ujSkaroF-hXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "nqOkTpVfCvi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muqUwpX4_9o7"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import planetary_computer\n",
        "import xarray\n",
        "import time\n",
        "from collections import defaultdict\n",
        "from pystac_client import Client"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Base Data Files"
      ],
      "metadata": {
        "id": "bUIQBQ-UI_Z0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/drive/MyDrive/snocast/train/data'\n",
        "\n",
        "ground_measures_train = pd.read_csv(os.path.join(data_dir, 'static/ground_measures_train_features.csv'))\n",
        "ground_measures_train.columns = ['station_id'] + list(ground_measures_train.columns[1:])\n",
        "gm_melt_train = ground_measures_train.melt(id_vars=[\"station_id\"],\n",
        "                                            var_name=\"date\",\n",
        "                                            value_name=\"swe\").dropna()\n",
        "            \n",
        "\n",
        "ground_measures_test = pd.read_csv(os.path.join(data_dir, 'static/ground_measures_test_features.csv'))\n",
        "ground_measures_test.columns = ['station_id'] + list(ground_measures_test.columns[1:])\n",
        "gm_melt_test = ground_measures_test.melt(id_vars=[\"station_id\"],\n",
        "                           var_name=\"date\",\n",
        "                           value_name=\"swe\").dropna()\n",
        "                           \n",
        "ground_measures_metadata = pd.read_csv(os.path.join(data_dir, 'static/ground_measures_metadata.csv'))\n",
        "ground_measures_all = pd.merge(ground_measures_train, ground_measures_test, how='outer', on='station_id')\n",
        "gm_melt_all = ground_measures_all.melt(id_vars=[\"station_id\"],\n",
        "                           var_name=\"date\",\n",
        "                           value_name=\"swe\").dropna()\n",
        "gm_seq = pd.merge(gm_melt_all, ground_measures_metadata, how='inner', on='station_id')\n",
        "\n",
        "train_labels = pd.read_csv(os.path.join(data_dir, 'static/train_labels.csv'))\n",
        "labels_melt_train = train_labels.melt(id_vars=[\"cell_id\"],\n",
        "                  var_name=\"date\",\n",
        "                  value_name=\"swe\").dropna()\n",
        "\n",
        "test_labels = pd.read_csv(os.path.join(data_dir, 'static/labels_2020_2021.csv'))\n",
        "labels_melt_test = test_labels.melt(id_vars=[\"cell_id\"],\n",
        "                  var_name=\"date\",\n",
        "                  value_name=\"swe\").dropna()"
      ],
      "metadata": {
        "id": "mshiRZnECY4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get latitude longitude for train and test grids\n",
        "f = open(os.path.join(data_dir, 'static/grid_cells.geojson'))\n",
        "grid_cells = json.load(f)\n",
        "print('length grid_cells features: ', len(grid_cells['features']))\n",
        "\n",
        "grid_features = defaultdict(dict)\n",
        "for grid_cell in grid_cells['features']:\n",
        "  cell_id = grid_cell['properties']['cell_id']\n",
        "  coordinates = grid_cell['geometry']['coordinates'][0]\n",
        "  region = grid_cell['properties']['region']\n",
        "  grid_features[cell_id] = {'coordinates': coordinates[1:],\n",
        "                            'region': region}\n",
        "\n",
        "grid_features_train = defaultdict(dict)\n",
        "train_ids = []\n",
        "train_lats = []\n",
        "train_lons = []\n",
        "train_regions = []\n",
        "train_bboxes = []\n",
        "grid_features_test = defaultdict(dict)\n",
        "test_ids = []\n",
        "test_lats = []\n",
        "test_lons = []\n",
        "test_regions = []\n",
        "test_bboxes = []\n",
        "\n",
        "\n",
        "for cell_id in train_labels['cell_id'].values:\n",
        "  train_ids.append(cell_id)\n",
        "  lon, lat = np.mean(grid_features[cell_id]['coordinates'], axis=0)\n",
        "  max_lon, max_lat = np.max(grid_features[cell_id]['coordinates'], axis=0)\n",
        "  min_lon, min_lat = np.min(grid_features[cell_id]['coordinates'], axis=0)\n",
        "  # bbox = [min_lon, min_lat, max_lon, max_lat]\n",
        "  bbox = np.array([min_lon, min_lat,max_lon, max_lat])\n",
        "  train_regions = grid_features[cell_id]['region']\n",
        "  train_lats.append(lat)\n",
        "  train_lons.append(lon)\n",
        "  train_bboxes.append(bbox)\n",
        "\n",
        "  grid_features[cell_id]['dataset'] = 'train'\n",
        "\n",
        "for cell_id in test_labels['cell_id'].values:\n",
        "  test_ids.append(cell_id)\n",
        "  lon, lat = np.mean(grid_features[cell_id]['coordinates'], axis=0)\n",
        "  max_lon, max_lat = np.max(grid_features[cell_id]['coordinates'], axis=0)\n",
        "  min_lon, min_lat = np.min(grid_features[cell_id]['coordinates'], axis=0)\n",
        "  # bbox = [min_lon, min_lat, max_lon, max_lat]\n",
        "  bbox = np.array([min_lon, min_lat,max_lon, max_lat])\n",
        "  test_regions = grid_features[cell_id]['region']\n",
        "  test_lats.append(lat)\n",
        "  test_lons.append(lon)\n",
        "  test_bboxes.append(bbox)\n",
        "\n",
        "  if 'dataset' in grid_features[cell_id].keys():\n",
        "    grid_features[cell_id]['dataset'] = 'both'\n",
        "  else:\n",
        "    grid_features[cell_id]['dataset'] = 'test'\n",
        "\n",
        "for cell_id in grid_features:\n",
        "  if grid_features[cell_id]['dataset'] in ('test','both'):\n",
        "    grid_features_test[cell_id] = grid_features[cell_id]\n",
        "  if grid_features[cell_id]['dataset'] in ('train','both'):\n",
        "    grid_features_train[cell_id] = grid_features[cell_id]\n",
        "print(\"test count: \", len(grid_features_test))\n",
        "print(\"train count: \", len(grid_features_train))\n",
        "\n",
        "\n",
        "train_lat_lon = pd.DataFrame({'cell_id': train_ids, \n",
        "                              'latitude': train_lats, \n",
        "                              'longitude': train_lons, \n",
        "                              'region': train_regions,\n",
        "                              'bbox': train_bboxes})\n",
        "test_lat_lon = pd.DataFrame({'cell_id': test_ids, \n",
        "                             'latitude': test_lats, \n",
        "                             'longitude': test_lons, \n",
        "                             'region': test_regions,\n",
        "                             'bbox': test_bboxes})"
      ],
      "metadata": {
        "id": "lBpi19GiERNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Data for Copernicus Digital Elevation Model (DEM)"
      ],
      "metadata": {
        "id": "8cI2IEu497Ya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = Client.open(\n",
        "    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
        "    ignore_conformance=True,\n",
        ")"
      ],
      "metadata": {
        "id": "GEpn6lCK97FB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_elevations(df):\n",
        "  all_max_lat = df.latitude.max()\n",
        "  all_min_lat = df.latitude.min()\n",
        "  all_max_lon = df.longitude.max()\n",
        "  all_min_lon = df.longitude.min()\n",
        "  all_bbox = [all_min_lon, all_min_lat, all_max_lon, all_max_lat]\n",
        "\n",
        "  # Get all relevant items within the lat/lon bounds of the df\n",
        "  search = client.search(\n",
        "      collections=[\"cop-dem-glo-30\"],\n",
        "      bbox=all_bbox,\n",
        "  )\n",
        "\n",
        "  items = list(search.get_items())\n",
        "  if len(items) > 1:\n",
        "    print(f\"Returned {len(items)} items\")\n",
        "  \n",
        "  # Ran in 30 min. for 295 items\n",
        "  processed_items = []\n",
        "  for i in range(len(items)):\n",
        "    signed_asset = planetary_computer.sign(items[i].assets[\"data\"])\n",
        "    data = (\n",
        "        xarray.open_rasterio(signed_asset.href)\n",
        "        .squeeze()\n",
        "        .drop(\"band\")\n",
        "        .coarsen({\"y\": 5, \"x\": 5})\n",
        "        .mean()\n",
        "    )\n",
        "    processed_items.append(data)\n",
        "\n",
        "  mean_elevations = []\n",
        "  var_elevations = []\n",
        "\n",
        "  for idx, row in df.iterrows():\n",
        "    if idx % 100 == 0:\n",
        "      print(idx)\n",
        "    min_lon, min_lat, max_lon, max_lat = row['bbox']\n",
        "\n",
        "    sample_elevations = np.array([])\n",
        "    for data in processed_items:\n",
        "      lat_values = (data.y.values < max_lat) & (data.y.values > min_lat)\n",
        "      lon_values = (data.x.values < max_lon) & (data.x.values > min_lon)\n",
        "      mask = lon_values[np.newaxis, :] & lat_values[:, np.newaxis]\n",
        "      sample_elevations = np.concatenate([sample_elevations, data.values[mask]])\n",
        "    mean_elevation_m = sample_elevations.mean()\n",
        "    var_elevation_m = sample_elevations.var()\n",
        "    mean_elevations.append(mean_elevation_m)\n",
        "    var_elevations.append(var_elevation_m)\n",
        "\n",
        "  return mean_elevations, var_elevations\n"
      ],
      "metadata": {
        "id": "0-NV-oTkaInq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_mean_elevations, test_var_elevations = get_elevations(test_lat_lon)\n",
        "test_lat_lon['elevation_m'] = test_mean_elevations\n",
        "test_lat_lon['elevation_var_m'] = test_var_elevations"
      ],
      "metadata": {
        "id": "6w0zOTGbeNf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_lat_lon = test_lat_lon[['cell_id', 'latitude', 'longitude', 'region', 'elevation_m','elevation_var_m']]\n",
        "test_lat_lon.to_parquet('/content/drive/MyDrive/snocast/train/data/static/test_elevation.parquet')"
      ],
      "metadata": {
        "id": "d2btEA5Leg_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_mean_elevations, train_var_elevations = get_elevations(train_lat_lon)\n",
        "train_lat_lon['elevation_m'] = train_mean_elevations\n",
        "train_lat_lon['elevation_var_m'] = train_var_elevations"
      ],
      "metadata": {
        "id": "P2qT8jLte0Z6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_lat_lon = train_lat_lon[['cell_id', 'latitude', 'longitude', 'region', 'elevation_m','elevation_var_m']]\n",
        "train_lat_lon.to_parquet('/content/drive/MyDrive/snocast/train/data/static/train_elevation.parquet')"
      ],
      "metadata": {
        "id": "L5JVEpare0Q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_lat_lon.sample(5)"
      ],
      "metadata": {
        "id": "DL-aFg17mj-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JiO75r7hAleJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}