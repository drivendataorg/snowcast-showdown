{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "get_lccs_train_test.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "background_execution": "on",
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install boto3"
      ],
      "metadata": {
        "id": "WCfpyIKG_Ntc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "nqOkTpVfCvi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muqUwpX4_9o7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import boto3\n",
        "import netCDF4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BUCKET_NAME = 'drivendata-public-assets'\n",
        "\n",
        "# enter authentication credentials\n",
        "s3 = boto3.resource('s3', aws_access_key_id = 'aws_access_key_id', \n",
        "                          aws_secret_access_key = 'aws_secret_access_key')"
      ],
      "metadata": {
        "id": "uC-cnuT5_c07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KEY = 'land_cover_map.tar.gz'\n",
        "\n",
        "try:\n",
        "  s3.Bucket(BUCKET_NAME).download_file(KEY, 'land_cover_map.tar.gz')\n",
        "  \n",
        "except botocore.exceptions.ClientError as e:\n",
        "  if e.response['Error']['Code'] == \"404\":\n",
        "    print(\"The object does not exist.\")\n",
        "  else:\n",
        "    raise"
      ],
      "metadata": {
        "id": "6P6fzkzF_cwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xf /content/land_cover_map.tar.gz"
      ],
      "metadata": {
        "id": "UviWh1g2_ctX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/C3S-LC-L4-LCCS-Map-300m-P1Y-2020-v2.1.1.nc /content/drive/MyDrive/snocast/train/data/static/C3S-LC-L4-LCCS-Map-300m-P1Y-2020-v2.1.1.nc"
      ],
      "metadata": {
        "id": "Z_Ky6z0TjC4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fp='/content/drive/MyDrive/snocast/train/data/static/C3S-LC-L4-LCCS-Map-300m-P1Y-2020-v2.1.1.nc' # your file name with the eventual path\n",
        "nc = netCDF4.Dataset(fp) # reading the nc file and creating Dataset"
      ],
      "metadata": {
        "id": "99XBnoFlFMBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# http://maps.elie.ucl.ac.be/CCI/viewer/download/ESACCI-LC-QuickUserGuide-LC-Maps_v2-0-7.pdf\n",
        "lccs_class = nc.variables['lccs_class']\n",
        "lccs_lat = np.array(nc.variables['lat'])\n",
        "lccs_lon = np.array(nc.variables['lon'])"
      ],
      "metadata": {
        "id": "Of9VYyarFRlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Base Data Files"
      ],
      "metadata": {
        "id": "bUIQBQ-UI_Z0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/drive/MyDrive/snocast/train/data'\n",
        "\n",
        "ground_measures_train = pd.read_csv(os.path.join(data_dir, 'static/ground_measures_train_features.csv'))\n",
        "ground_measures_train.columns = ['station_id'] + list(ground_measures_train.columns[1:])\n",
        "gm_melt_train = ground_measures_train.melt(id_vars=[\"station_id\"],\n",
        "                                            var_name=\"date\",\n",
        "                                            value_name=\"swe\").dropna()\n",
        "            \n",
        "\n",
        "ground_measures_test = pd.read_csv(os.path.join(data_dir, 'static/ground_measures_test_features.csv'))\n",
        "ground_measures_test.columns = ['station_id'] + list(ground_measures_test.columns[1:])\n",
        "gm_melt_test = ground_measures_test.melt(id_vars=[\"station_id\"],\n",
        "                           var_name=\"date\",\n",
        "                           value_name=\"swe\").dropna()\n",
        "                           \n",
        "ground_measures_metadata = pd.read_csv(os.path.join(data_dir, 'static/ground_measures_metadata.csv'))\n",
        "ground_measures_all = pd.merge(ground_measures_train, ground_measures_test, how='outer', on='station_id')\n",
        "gm_melt_all = ground_measures_all.melt(id_vars=[\"station_id\"],\n",
        "                           var_name=\"date\",\n",
        "                           value_name=\"swe\").dropna()\n",
        "gm_seq = pd.merge(gm_melt_all, ground_measures_metadata, how='inner', on='station_id')\n",
        "\n",
        "train_labels = pd.read_csv(os.path.join(data_dir, 'static/train_labels.csv'))\n",
        "labels_melt_train = train_labels.melt(id_vars=[\"cell_id\"],\n",
        "                  var_name=\"date\",\n",
        "                  value_name=\"swe\").dropna()\n",
        "\n",
        "test_labels = pd.read_csv(os.path.join(data_dir, 'static/labels_2020_2021.csv'))\n",
        "labels_melt_test = test_labels.melt(id_vars=[\"cell_id\"],\n",
        "                  var_name=\"date\",\n",
        "                  value_name=\"swe\").dropna()"
      ],
      "metadata": {
        "id": "mshiRZnECY4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get latitude longitude for train and test grids\n",
        "f = open(os.path.join(data_dir, 'static/grid_cells.geojson'))\n",
        "grid_cells = json.load(f)\n",
        "print('length grid_cells features: ', len(grid_cells['features']))\n",
        "\n",
        "grid_features = defaultdict(dict)\n",
        "for grid_cell in grid_cells['features']:\n",
        "  cell_id = grid_cell['properties']['cell_id']\n",
        "  coordinates = grid_cell['geometry']['coordinates'][0]\n",
        "  region = grid_cell['properties']['region']\n",
        "  grid_features[cell_id] = {'coordinates': coordinates[1:],\n",
        "                            'region': region,\n",
        "                            'geometry': grid_cell['geometry']}\n",
        "\n",
        "grid_features_train = defaultdict(dict)\n",
        "train_ids = []\n",
        "train_lats = []\n",
        "train_lons = []\n",
        "train_regions = []\n",
        "train_bboxes = []\n",
        "grid_features_test = defaultdict(dict)\n",
        "test_ids = []\n",
        "test_lats = []\n",
        "test_lons = []\n",
        "test_regions = []\n",
        "test_bboxes = []\n",
        "\n",
        "\n",
        "for cell_id in train_labels['cell_id'].values:\n",
        "  train_ids.append(cell_id)\n",
        "  coordinates = grid_features[cell_id]['geometry']['coordinates'][0]\n",
        "  lon, lat = np.mean(coordinates, axis=0)\n",
        "  max_lon, max_lat = np.max(coordinates, axis=0)\n",
        "  min_lon, min_lat = np.min(coordinates, axis=0)\n",
        "  # bbox = [min_lon, min_lat, max_lon, max_lat]\n",
        "  bbox = np.array([min_lon, min_lat, max_lon, max_lat])\n",
        "  train_regions = grid_features[cell_id]['region']\n",
        "  train_lats.append(lat)\n",
        "  train_lons.append(lon)\n",
        "  train_bboxes.append(bbox)\n",
        "\n",
        "  grid_features[cell_id]['dataset'] = 'train'\n",
        "\n",
        "for cell_id in test_labels['cell_id'].values:\n",
        "  test_ids.append(cell_id)\n",
        "  coordinates = grid_features[cell_id]['geometry']['coordinates'][0]\n",
        "  lon, lat = np.mean(coordinates, axis=0)\n",
        "  max_lon, max_lat = np.max(coordinates, axis=0)\n",
        "  min_lon, min_lat = np.min(coordinates, axis=0)\n",
        "  # bbox = [min_lon, min_lat, max_lon, max_lat]\n",
        "  bbox = np.array([min_lon, min_lat, max_lon, max_lat])\n",
        "  test_regions = grid_features[cell_id]['region']\n",
        "  test_lats.append(lat)\n",
        "  test_lons.append(lon)\n",
        "  test_bboxes.append(bbox)\n",
        "\n",
        "  if 'dataset' in grid_features[cell_id].keys():\n",
        "    grid_features[cell_id]['dataset'] = 'both'\n",
        "  else:\n",
        "    grid_features[cell_id]['dataset'] = 'test'\n",
        "\n",
        "for cell_id in grid_features:\n",
        "  if grid_features[cell_id]['dataset'] in ('test','both'):\n",
        "    grid_features_test[cell_id] = grid_features[cell_id]\n",
        "  if grid_features[cell_id]['dataset'] in ('train','both'):\n",
        "    grid_features_train[cell_id] = grid_features[cell_id]\n",
        "print(\"test count: \", len(grid_features_test))\n",
        "print(\"train count: \", len(grid_features_train))\n",
        "\n",
        "\n",
        "train_lat_lon = pd.DataFrame({'cell_id': train_ids, \n",
        "                              'latitude': train_lats, \n",
        "                              'longitude': train_lons, \n",
        "                              'region': train_regions,\n",
        "                              'bbox': train_bboxes})\n",
        "test_lat_lon = pd.DataFrame({'cell_id': test_ids, \n",
        "                             'latitude': test_lats, \n",
        "                             'longitude': test_lons, \n",
        "                             'region': test_regions,\n",
        "                             'bbox': test_bboxes})"
      ],
      "metadata": {
        "id": "XTsNZvsjy5Fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lccs(df):\n",
        "  all_max_lat = df.latitude.max()\n",
        "  all_min_lat = df.latitude.min()\n",
        "  all_max_lon = df.longitude.max()\n",
        "  all_min_lon = df.longitude.min()\n",
        "\n",
        "  # Trim to only relevant lat lon\n",
        "  lccs_lat_values = (lccs_lat < all_max_lat) & (lccs_lat > all_min_lat)\n",
        "  lccs_lon_values = (lccs_lon < all_max_lon) & (lccs_lon > all_min_lon)\n",
        "\n",
        "  reduced_lccs = np.squeeze(lccs_class[:, lccs_lat_values, lccs_lon_values])\n",
        "  reduced_lat = lccs_lat[lccs_lat_values]\n",
        "  reduced_lon = lccs_lon[lccs_lon_values]\n",
        "\n",
        "  lccs_arr = []\n",
        "\n",
        "  for idx, row in df.iterrows():\n",
        "    if idx % 100 == 0:\n",
        "      print(idx)\n",
        "    min_lon, min_lat, max_lon, max_lat = row['bbox']\n",
        "\n",
        "    lat_values = (reduced_lat < max_lat) & (reduced_lat > min_lat)\n",
        "    lon_values = (reduced_lon < max_lon) & (reduced_lon > min_lon)\n",
        "    mask = lon_values[np.newaxis, :] & lat_values[:, np.newaxis]\n",
        "\n",
        "    arr = reduced_lccs[mask]\n",
        "    lccs_cat, lccs_count = np.unique(arr, return_counts=True)\n",
        "    lccs_len = len(arr)\n",
        "\n",
        "    land_cover = {}\n",
        "    land_cover['location_id'] = row['cell_id']\n",
        "    lccs_order = np.flip(np.argsort(lccs_count))\n",
        "    for i in range(3):\n",
        "      if i+1 <= len(lccs_order):\n",
        "        land_cover[f'lccs_{i}'] = lccs_cat[lccs_order[i]]\n",
        "        land_cover[f'lccs_pct_{i}'] = lccs_count[lccs_order[i]]/lccs_len\n",
        "      else:\n",
        "        land_cover[f'lccs_{i}'] = 0\n",
        "        land_cover[f'lccs_pct_{i}'] = np.nan\n",
        "    \n",
        "    lccs_arr.append(land_cover)\n",
        "\n",
        "  return lccs_arr"
      ],
      "metadata": {
        "id": "yn8Ovb4S5ibJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_lccs_arr = get_lccs(test_lat_lon)"
      ],
      "metadata": {
        "id": "QvNKHewu57e3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_lccs_df = pd.DataFrame(test_lccs_arr)"
      ],
      "metadata": {
        "id": "2jtln4oJFm_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_lccs_df.shape"
      ],
      "metadata": {
        "id": "WKQ1GRT8I7Pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_lccs_df.to_parquet('/content/drive/MyDrive/snocast/train/data/static/test_lccs.parquet')"
      ],
      "metadata": {
        "id": "4GdT3xQ-4qDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_lccs_arr = get_lccs(train_lat_lon)"
      ],
      "metadata": {
        "id": "cyThoF0RndNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_lccs_df = pd.DataFrame(train_lccs_arr)"
      ],
      "metadata": {
        "id": "ONcrhhe-n-ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_lccs_df.to_parquet('/content/drive/MyDrive/snocast/train/data/static/train_lccs.parquet')"
      ],
      "metadata": {
        "id": "IaMgVlU66-JA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "S2Nu8cEiRbtS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}