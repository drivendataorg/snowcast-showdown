{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IeY8XKxtH0jM"
      },
      "outputs": [],
      "source": [
        "!pip install catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sd7IkG8zzj1_"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSPLwkRmOeYg"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import catboost as cb\n",
        "from joblib import dump\n",
        "\n",
        "from sklearn import linear_model\n",
        "from sklearn.neighbors import KDTree\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# For catboost output\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YmAwPwt0HBk"
      },
      "source": [
        "## Load Ground Measures, Train, and Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8opmtbssOjoP"
      },
      "outputs": [],
      "source": [
        "data_dir = '/content/drive/MyDrive/snocast/train/data'\n",
        "\n",
        "# Get ground measures for train dataset\n",
        "ground_measures_train = pd.read_csv(os.path.join(data_dir, 'static/ground_measures_train_features.csv'))\n",
        "ground_measures_train.columns = ['station_id'] + list(ground_measures_train.columns[1:])\n",
        "gm_melt_train = ground_measures_train.melt(id_vars=[\"station_id\"],\n",
        "                                            var_name=\"date\",\n",
        "                                            value_name=\"swe\")\n",
        "            \n",
        "# Get ground measures for test dataset\n",
        "ground_measures_test = pd.read_csv(os.path.join(data_dir, 'static/ground_measures_test_features.csv'))\n",
        "ground_measures_test.columns = ['station_id'] + list(ground_measures_test.columns[1:])\n",
        "gm_melt_test = ground_measures_test.melt(id_vars=[\"station_id\"],\n",
        "                           var_name=\"date\",\n",
        "                           value_name=\"swe\")\n",
        "\n",
        "ground_measures_metadata = pd.read_csv(os.path.join(data_dir, 'static/ground_measures_metadata.csv'))\n",
        "ground_measures_all = pd.concat([ground_measures_train, ground_measures_test], axis=1)\n",
        "gm_melt_all = pd.concat([gm_melt_train, gm_melt_test])\n",
        "\n",
        "# Get the train labels data\n",
        "train_labels = pd.read_csv(os.path.join(data_dir, 'static/train_labels.csv'))\n",
        "labels_melt_train = train_labels.melt(id_vars=[\"cell_id\"],\n",
        "                  var_name=\"date\",\n",
        "                  value_name=\"swe\").dropna()\n",
        "\n",
        "# Get the test labels data\n",
        "test_labels = pd.read_csv(os.path.join(data_dir, 'static/labels_2020_2021.csv'))\n",
        "labels_melt_test = test_labels.melt(id_vars=[\"cell_id\"],\n",
        "                  var_name=\"date\",\n",
        "                  value_name=\"swe\").dropna()\n",
        "\n",
        "# Get elevation data for train and test\n",
        "train_label_elev = pd.read_parquet(os.path.join(data_dir, 'static/train_elevation.parquet'))\n",
        "test_pred_elev = pd.read_parquet(os.path.join(data_dir, 'static/test_elevation.parquet'))\n",
        "\n",
        "train_elev_grad = pd.read_parquet(os.path.join(data_dir, 'static/train_elevation_grads.parquet'))\n",
        "test_elev_grad = pd.read_parquet(os.path.join(data_dir, 'static/test_elevation_grads.parquet'))\n",
        "gm_elev_grad = pd.read_parquet(os.path.join(data_dir, 'static/gm_elevation_grads.parquet'))\n",
        "\n",
        "gm_modis_aqua = pd.read_parquet(os.path.join(data_dir, 'modis/modis_aqua_gm.parquet'))\n",
        "gm_modis_terra = pd.read_parquet(os.path.join(data_dir, 'modis/modis_terra_gm.parquet'))\n",
        "\n",
        "train_modis_aqua = pd.read_parquet(os.path.join(data_dir, 'modis/modis_aqua_train.parquet'))\n",
        "train_modis_terra = pd.read_parquet(os.path.join(data_dir, 'modis/modis_terra_train.parquet'))\n",
        "\n",
        "test_modis_aqua = pd.read_parquet(os.path.join(data_dir, 'modis/modis_aqua_test.parquet'))\n",
        "test_modis_terra = pd.read_parquet(os.path.join(data_dir, 'modis/modis_terra_test.parquet'))\n",
        "\n",
        "train_water = pd.read_parquet(os.path.join(data_dir, 'static/train_water.parquet'))\n",
        "train_water['water'] = train_water['water'] - 1\n",
        "test_water = pd.read_parquet(os.path.join(data_dir, 'static/test_water.parquet'))\n",
        "test_water['water'] = test_water['water'] - 1\n",
        "\n",
        "test_lccs = pd.read_parquet(os.path.join(data_dir, 'static/test_lccs.parquet'))\n",
        "train_lccs = pd.read_parquet(os.path.join(data_dir, 'static/train_lccs.parquet'))\n",
        "gm_lccs = pd.read_parquet(os.path.join(data_dir, 'static/gm_lccs.parquet'))\n",
        "\n",
        "# Pull in the NOAA HRRR Climate Data\n",
        "gm_climate = pd.read_parquet(os.path.join(data_dir, 'hrrr/gm_climate.parquet'))\n",
        "train_climate = pd.read_parquet(os.path.join(data_dir, 'hrrr/train_climate.parquet'))\n",
        "test_climate = pd.read_parquet(os.path.join(data_dir, 'hrrr/test_climate.parquet'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIhElYRP0OyA"
      },
      "outputs": [],
      "source": [
        "# Create sequential dataframes for train and test\n",
        "train_label_seq = pd.merge(labels_melt_train, train_label_elev, how='inner', on='cell_id')\n",
        "test_label_seq = pd.merge(labels_melt_test, test_pred_elev, how='inner', on='cell_id')\n",
        "\n",
        "# Create sequential dataframe for ground measure stations\n",
        "gm_seq = pd.merge(gm_melt_all, ground_measures_metadata, how='inner', on='station_id')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6M6gsI8SjRe5"
      },
      "outputs": [],
      "source": [
        "# Combine sequential datasets\n",
        "gm_seq.columns = ['location_id', 'date', 'swe', 'name', 'elevation_m', 'latitude', 'longitude', 'state']\n",
        "\n",
        "train_label_seq.columns = ['location_id', 'date', 'swe', 'latitude', 'longitude', 'region', 'elevation_m', 'elevation_var_m']\n",
        "\n",
        "test_label_seq.columns = ['location_id', 'date', 'swe', 'latitude', 'longitude', 'region', 'elevation_m', 'elevation_var_m']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDDG884xA3Z4"
      },
      "outputs": [],
      "source": [
        "# Merge Aqua and Terra Modis datasets\n",
        "def transform_modis(df_modis_terra, df_modis_aqua):\n",
        "  df_terra = df_modis_terra.groupby(['location_id','date']).mean().reset_index()\n",
        "  df_aqua = df_modis_aqua.groupby(['location_id','date']).mean().reset_index()\n",
        "  df_modis = pd.merge(df_aqua, df_terra, how='outer', on=['date','location_id'], suffixes=('_aqua','_terra'))\n",
        "  df_modis['date'] = df_modis['date'].str.replace('_','-')\n",
        "  df_modis = df_modis.sort_values(['location_id','date']).reset_index(drop=True)\n",
        "\n",
        "  return df_modis\n",
        "\n",
        "gm_modis = transform_modis(gm_modis_terra, gm_modis_aqua)\n",
        "train_modis = transform_modis(train_modis_terra, train_modis_aqua)\n",
        "test_modis = transform_modis(test_modis_terra, test_modis_aqua)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdLV6UKYC-Jg"
      },
      "outputs": [],
      "source": [
        "# https://stackoverflow.com/questions/13996302/python-rolling-functions-for-groupby-object\n",
        "def get_rolling_avgs(df, roll_cols, rolling_days_list):\n",
        "  all_roll_cols = []\n",
        "\n",
        "  df = df.sort_values(['location_id','date'])\n",
        "\n",
        "  for roll_days in rolling_days_list:\n",
        "    rolling_days_cols = [col + f'_{roll_days}_day' for col in roll_cols]\n",
        "    all_roll_cols.extend(rolling_days_cols)\n",
        "    df_roll = (df\n",
        "                      .groupby('location_id', sort=False)[['date'] + roll_cols]\n",
        "                      .rolling(roll_days, min_periods=1, on='date')\n",
        "                      .mean()\n",
        "                      .reset_index()\n",
        "                      .drop('level_1', axis=1))\n",
        "    \n",
        "    df = pd.merge(df, df_roll, how='left', on=['location_id','date'], suffixes=['',f'_{roll_days}_day'])\n",
        "\n",
        "  return df, all_roll_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSRPkz-du_RD"
      },
      "outputs": [],
      "source": [
        "# Get the 5-day and 15-day rolling average of the Modis data\n",
        "roll_cols = [\n",
        "             'NDSI_Snow_Cover_aqua',\n",
        "             'NDSI_Snow_Cover_terra',\n",
        "             ]\n",
        "\n",
        "rolling_days_list = [5, 15]\n",
        "\n",
        "gm_modis, modis_roll_cols = get_rolling_avgs(gm_modis, roll_cols, rolling_days_list)\n",
        "train_modis, modis_roll_cols = get_rolling_avgs(train_modis, roll_cols, rolling_days_list)\n",
        "test_modis, modis_roll_cols = get_rolling_avgs(test_modis, roll_cols, rolling_days_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "of98B2FwwsTv"
      },
      "outputs": [],
      "source": [
        "# Merge the sequence data with the Modis data\n",
        "gm_xgboost = pd.merge(gm_seq, gm_modis, how='left', on=['date','location_id'])\n",
        "train_xgboost = pd.merge(train_label_seq, train_modis, how='left', on=['date','location_id'])\n",
        "test_xgboost = pd.merge(test_label_seq, test_modis, how='left', on=['date','location_id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdR20Slb_QvM"
      },
      "outputs": [],
      "source": [
        "# Get the 3-day rolling average of the climate data\n",
        "climate_cols_2_roll = [\n",
        "             'TMP', \n",
        "             'SNOD', \n",
        "             'WEASD', \n",
        "             'SPFH', \n",
        "             'SNOWC', \n",
        "             'REFC',\n",
        "             'PRES', \n",
        "             'PWAT'\n",
        "             ]\n",
        "\n",
        "rolling_days_list = [3]\n",
        "\n",
        "gm_climate, climate_roll_cols = get_rolling_avgs(gm_climate, climate_cols_2_roll, rolling_days_list)\n",
        "train_climate, climate_roll_cols = get_rolling_avgs(train_climate, climate_cols_2_roll, rolling_days_list)\n",
        "test_climate, climate_roll_cols = get_rolling_avgs(test_climate, climate_cols_2_roll, rolling_days_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQ99xDSk_YHB"
      },
      "outputs": [],
      "source": [
        "# Merge the climate data with the Modis data\n",
        "gm_xgboost = pd.merge(gm_xgboost, gm_climate, how='left', on=['date','location_id'])\n",
        "train_xgboost = pd.merge(train_xgboost, train_climate, how='left', on=['date','location_id'])\n",
        "test_xgboost = pd.merge(test_xgboost, test_climate, how='left', on=['date','location_id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNA8tQqilrkA"
      },
      "outputs": [],
      "source": [
        "# Add in the snow season day feature\n",
        "gm_xgboost['datetime'] = pd.to_datetime(gm_xgboost['date'])\n",
        "gm_xgboost['snow_season_day'] = gm_xgboost.datetime.dt.dayofyear.apply(lambda x: x - 335 if x >= 335 else x + 30)\n",
        "\n",
        "train_xgboost['datetime'] = pd.to_datetime(train_xgboost['date'])\n",
        "train_xgboost['snow_season_day'] = train_xgboost.datetime.dt.dayofyear.apply(lambda x: x - 335 if x >= 335 else x + 30)\n",
        "\n",
        "test_xgboost['datetime'] = pd.to_datetime(test_xgboost['date'])\n",
        "test_xgboost['snow_season_day'] = test_xgboost.datetime.dt.dayofyear.apply(lambda x: x - 335 if x >= 335 else x + 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAaAAnVf_eWX"
      },
      "outputs": [],
      "source": [
        "# Separate the snow season into periods of 14 days\n",
        "snow_season_period_dict = {}\n",
        "days_in_period = 14\n",
        "total_days = 213\n",
        "period = 0\n",
        "period_count = 0\n",
        "total_periods = int(total_days/days_in_period) - 1\n",
        "\n",
        "for day in range(total_days):\n",
        "  snow_season_period_dict[day] = period\n",
        "  period_count += 1\n",
        "  if period_count == days_in_period:\n",
        "    if period != total_periods:\n",
        "      period += 1\n",
        "    period_count = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgfOPphl_hUY"
      },
      "outputs": [],
      "source": [
        "train_xgboost['snow_season_period'] = train_xgboost.snow_season_day.apply(lambda x: snow_season_period_dict[x])\n",
        "test_xgboost['snow_season_period'] = test_xgboost.snow_season_day.apply(lambda x: snow_season_period_dict[x])\n",
        "gm_xgboost['snow_season_period'] = gm_xgboost.snow_season_day.apply(lambda x: snow_season_period_dict[x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTRnoLzi_oeZ"
      },
      "outputs": [],
      "source": [
        "# Get the snow season period historical mean and standard deviation to calculate the relative swe for\n",
        "# each ground station measurement\n",
        "gm_period = (gm_xgboost.groupby(['location_id','snow_season_period'])\n",
        "                            .agg(swe_period_mean=('swe','mean'), swe_period_std=('swe','std'))\n",
        "                            .reset_index())\n",
        "gm_xgboost = pd.merge(gm_xgboost, gm_period, how='left', on=['location_id', 'snow_season_period'], suffixes=('','_period'))\n",
        "gm_xgboost['relative_swe'] = (gm_xgboost['swe'] - gm_xgboost['swe_period_mean'])/gm_xgboost['swe_period_std']\n",
        "gm_xgboost['relative_swe'] = gm_xgboost.apply(lambda x: 0.0 if x.swe_period_mean == 0. and x.swe_period_std == 0. else x.relative_swe, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1W4HpkJz4UA"
      },
      "outputs": [],
      "source": [
        "# Backfill most recent date relative_swe if NaN\n",
        "roll_cols = [\n",
        "             'relative_swe'\n",
        "             ]\n",
        "\n",
        "roll_window = [2]\n",
        "\n",
        "gm_xgboost, relative_swe_roll_cols = get_rolling_avgs(gm_xgboost, roll_cols, roll_window)\n",
        "\n",
        "gm_xgboost['relative_swe'] = gm_xgboost['relative_swe'].fillna(gm_xgboost['relative_swe_2_day'])\n",
        "gm_xgboost = gm_xgboost[gm_xgboost['swe'].notna()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MW5idfSt_oUI"
      },
      "outputs": [],
      "source": [
        "# Need to map train measurement dates to the most recent past ground measurement date\n",
        "def map_dates_to_most_recent_past_date(unique_dates_from, unique_dates_to):\n",
        "  all_sorted = sorted([(d, 1) for d in unique_dates_from] + [(d, 0) for d in unique_dates_to])\n",
        "  date_dict = {}\n",
        "  for i in range(len(all_sorted)):\n",
        "    if all_sorted[i][1] == 1:\n",
        "      still_looking = True\n",
        "      j = i\n",
        "      while still_looking:\n",
        "        j -= 1\n",
        "        if all_sorted[j][1] == 0:\n",
        "          date_dict[all_sorted[i][0]] = all_sorted[j][0]\n",
        "          still_looking = False\n",
        "  return date_dict\n",
        "\n",
        "unique_train_dates = train_label_seq.date.unique()\n",
        "unique_gm_dates = gm_seq.date.unique()\n",
        "date_dict = map_dates_to_most_recent_past_date(unique_train_dates, unique_gm_dates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBOFOvdn_oIy"
      },
      "outputs": [],
      "source": [
        "def get_k_neighbor_swe_data(location_df, neighbor_df, location_seq_df, neighbor_seq_df, k):\n",
        "  ''' function to map a location with a latitude, longitude, and elevation to\n",
        "  its k nearest ground measurement stations in 3-D space. The historical relative SWE\n",
        "  for the k nearest ground measurment stations are retrieved and averaged by\n",
        "  weighted distance. The averaged relative SWE of the k neighbors is returned.\n",
        "  '''\n",
        "  distance_cols = ['longitude','latitude']\n",
        "  scaler = StandardScaler()\n",
        "  scaler.fit(neighbor_df[distance_cols])\n",
        "  X_neighbor = scaler.transform(neighbor_df[distance_cols])\n",
        "  X_location = scaler.transform(location_df[distance_cols])\n",
        "\n",
        "  location_unique_dates = location_seq_df.date.unique()\n",
        "  neighbor_unique_dates = neighbor_seq_df.date.unique()\n",
        "  date_dict = map_dates_to_most_recent_past_date(location_unique_dates, neighbor_unique_dates)\n",
        "\n",
        "  # Builds the tree on the neighbor data\n",
        "  tree = KDTree(X_neighbor, leaf_size=2)\n",
        "\n",
        "  # Get neighbors for location dataset\n",
        "  location_dist, location_idx = tree.query(X_location, k=k)\n",
        "\n",
        "  neighbor_data = []\n",
        "  # iterate through locations in train\n",
        "  for idx, row in location_df.iterrows():\n",
        "    if idx % 1000 == 0:\n",
        "      print(idx)\n",
        "    # for each location get neighbors and distances\n",
        "    location_id = row['location_id']\n",
        "    # get neighbors for the location\n",
        "    neighbors = neighbor_df.loc[location_idx[idx]]['location_id'].values\n",
        "    # build df for neighbors with distances to the location\n",
        "    distance_df = pd.DataFrame({'location_id': neighbors, 'distance': location_dist[idx]})\n",
        "    distance_df = distance_df[distance_df['distance'] != 0]\n",
        "    neighbors = distance_df['location_id'].unique()\n",
        "    # get historical relative swe data for neighbors\n",
        "    neighbor_swe_hist_df = neighbor_seq_df[neighbor_seq_df['location_id'].isin(neighbors)][['location_id','date','relative_swe']]\n",
        "    neighbor_swe_hist_df.columns = ['location_id','neighbor_date','neighbor_relative_swe']\n",
        "    # build sequential df for the location to capture predictions\n",
        "    location_swe_pred_df = pd.DataFrame({'date': location_unique_dates})\n",
        "    location_swe_pred_df['location_id'] = location_id\n",
        "    # map the dates location_swe_pred_df to applicable neighbor dates\n",
        "    location_swe_pred_df['neighbor_date'] = location_swe_pred_df['date'].apply(lambda x: date_dict[x])\n",
        "    # get the inverse distance weight to figure out the contribution for each neighbor\n",
        "    distance_df['inverse_distance_weight'] = distance_df['distance']**-1/(distance_df['distance']**-1).sum()\n",
        "    # build a lookup df for the neighbor sequential data\n",
        "    lookup_df = pd.merge(neighbor_swe_hist_df, distance_df, how='inner', on='location_id')\n",
        "    lookup_df['swe_contrib'] = lookup_df['neighbor_relative_swe']*lookup_df['inverse_distance_weight']\n",
        "    combined_df = pd.merge(location_swe_pred_df, lookup_df[['neighbor_date','swe_contrib']], how='inner', on='neighbor_date')\n",
        "    combined_df = combined_df[['location_id','date','swe_contrib']].groupby(['location_id','date']).sum().reset_index()\n",
        "    neighbor_data.extend(combined_df.values)\n",
        "\n",
        "  all_locations_df = pd.DataFrame(neighbor_data, columns=['location_id','date','neighbor_relative_swe'])\n",
        "  return all_locations_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-fj6HbSBS7e"
      },
      "outputs": [],
      "source": [
        "k = 15\n",
        "# Get ground measure neighbor relative SWE for train data\n",
        "# location_df\n",
        "location_df = train_label_elev[['cell_id', 'latitude', 'longitude', 'elevation_m']]\n",
        "location_df.columns = ['location_id', 'latitude', 'longitude', 'elevation_m']\n",
        "# location_seq_df\n",
        "location_seq_df = train_label_seq[['date']]\n",
        "# neighbor_df\n",
        "neighbor_df = ground_measures_metadata[['station_id', 'elevation_m', 'latitude', 'longitude']]\n",
        "neighbor_df.columns = ['location_id', 'elevation_m', 'latitude', 'longitude']\n",
        "# neighbor_seq_df\n",
        "neighbor_seq_df = gm_xgboost[['location_id', 'date', 'relative_swe']]\n",
        "\n",
        "train_neighbor_swe_df = get_k_neighbor_swe_data(location_df, neighbor_df, location_seq_df, neighbor_seq_df, k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJ-B_6beBSoF"
      },
      "outputs": [],
      "source": [
        "# Get ground measure neighbor relative SWE for gm data\n",
        "gm_neighbor_swe_df = get_k_neighbor_swe_data(neighbor_df, neighbor_df, neighbor_seq_df, neighbor_seq_df, k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpMHMba7BSd3"
      },
      "outputs": [],
      "source": [
        "# Get ground measure neighbor relative SWE for test data\n",
        "# location_df\n",
        "location_df = test_pred_elev[['cell_id', 'latitude', 'longitude', 'elevation_m']]\n",
        "location_df.columns = ['location_id', 'latitude', 'longitude', 'elevation_m']\n",
        "# location_seq_df\n",
        "location_seq_df = test_label_seq[['date']]\n",
        "# neighbor_seq_df\n",
        "neighbor_seq_df = gm_xgboost[['location_id', 'date', 'relative_swe']]\n",
        "\n",
        "test_neighbor_swe_df = get_k_neighbor_swe_data(location_df, neighbor_df, location_seq_df, neighbor_seq_df, k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bm8JkT8ZBslG"
      },
      "outputs": [],
      "source": [
        "train_xgboost = pd.merge(train_xgboost, train_neighbor_swe_df, how='left', on=['location_id','date'])\n",
        "test_xgboost = pd.merge(test_xgboost, test_neighbor_swe_df, how='left', on=['location_id','date'])\n",
        "gm_xgboost = pd.merge(gm_xgboost, gm_neighbor_swe_df, how='left', on=['location_id','date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETdLDP0ZMnPx"
      },
      "outputs": [],
      "source": [
        "# Add in the water feature\n",
        "train_water.columns = ['location_id','water']\n",
        "test_water.columns = ['location_id','water']\n",
        "train_xgboost = pd.merge(train_xgboost, train_water, how='left', on=['location_id'])\n",
        "test_xgboost = pd.merge(test_xgboost, test_water, how='left', on=['location_id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwzfeQXAZJK5"
      },
      "outputs": [],
      "source": [
        "# Add in land category feature\n",
        "train_lccs.columns = ['location_id','lccs_0', 'lccs_pct_0', 'lccs_1', 'lccs_pct_1', 'lccs_2', 'lccs_pct_2']\n",
        "test_lccs.columns = ['location_id','lccs_0', 'lccs_pct_0', 'lccs_1', 'lccs_pct_1', 'lccs_2', 'lccs_pct_2']\n",
        "gm_lccs.columns = ['location_id','lccs_0']\n",
        "train_xgboost = pd.merge(train_xgboost, train_lccs, how='left', on=['location_id'])\n",
        "test_xgboost = pd.merge(test_xgboost, test_lccs, how='left', on=['location_id'])\n",
        "gm_xgboost = pd.merge(gm_xgboost, gm_lccs, how='left', on=['location_id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4-krpB4Ha6E"
      },
      "outputs": [],
      "source": [
        "# Add in the elevation gradient features\n",
        "train_elev_grad.columns = ['location_id','east_elev_grad','south_elev_grad','east_elev_pct','south_elev_pct']\n",
        "test_elev_grad.columns = ['location_id','east_elev_grad','south_elev_grad','east_elev_pct','south_elev_pct']\n",
        "gm_elev_grad.columns = ['location_id','east_elev_grad','south_elev_grad','east_elev_pct','south_elev_pct']\n",
        "train_xgboost = pd.merge(train_xgboost, train_elev_grad, how='left', on='location_id')\n",
        "test_xgboost = pd.merge(test_xgboost, test_elev_grad, how='left', on='location_id')\n",
        "gm_xgboost = pd.merge(gm_xgboost, gm_elev_grad, how='left', on='location_id')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wV6cl5hgG7ui"
      },
      "outputs": [],
      "source": [
        "all_xgboost = pd.concat([gm_xgboost, train_xgboost])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6k7MwSqA3SEZ"
      },
      "outputs": [],
      "source": [
        "all_xgboost = all_xgboost[all_xgboost['date'] <= '2019-12-31']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPwAdA6y3L69"
      },
      "outputs": [],
      "source": [
        "all_xgboost.date.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PaXbd09i3t2Y"
      },
      "outputs": [],
      "source": [
        "all_xgboost.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5d2fbQmR_iH"
      },
      "outputs": [],
      "source": [
        "all_xgboost.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kI4GFiorgpN9"
      },
      "outputs": [],
      "source": [
        "climate_cols = ['SNOD', 'WEASD', 'SNOWC'] + ['TMP_3_day','SPFH_3_day','PRES_3_day','PWAT_3_day']\n",
        "xgb_cols = [\n",
        "            'latitude',\n",
        "            'longitude',\n",
        "            'elevation_m',\n",
        "            'elevation_var_m',\n",
        "            'snow_season_day',\n",
        "            'water',\n",
        "            'neighbor_relative_swe',\n",
        "            'east_elev_grad',\n",
        "            'south_elev_grad',\n",
        "            ] \\\n",
        "            + modis_roll_cols + climate_cols\n",
        "\n",
        "label_col = ['swe']\n",
        "\n",
        "X = all_xgboost[xgb_cols]\n",
        "y = all_xgboost[label_col]\n",
        "\n",
        "X_test = test_xgboost[xgb_cols]\n",
        "y_test = test_xgboost[label_col]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4PgpZjNdvrZ"
      },
      "outputs": [],
      "source": [
        "# scale data\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X)\n",
        "X = scaler.transform(X)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PYK-PDIg-cM"
      },
      "outputs": [],
      "source": [
        "# define data_dmatrix\n",
        "xgb_dmatrix = xgb.DMatrix(data=X,label=y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cGLfuf68exh"
      },
      "outputs": [],
      "source": [
        "params = {\n",
        "          'n_estimators': 1000,\n",
        "          'min_split_loss': 0.08,\n",
        "          'gamma': 0.1,\n",
        "          'max_depth': 32,\n",
        "          'min_child_weight': 45,\n",
        "          'learning_rate': 0.04,\n",
        "          'objective':'reg:squarederror',\n",
        "          'n_jobs': 4,\n",
        "          'subsample': 0.95,\n",
        "          'colsample_bytree': 0.95,\n",
        "          }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSCjIuIwuxqK"
      },
      "source": [
        "## Make Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoTbRY-qcyt9"
      },
      "outputs": [],
      "source": [
        "# instantiate the XGB regressor\n",
        "xgb_reg = xgb.XGBRegressor(**params)\n",
        "\n",
        "# fit the regressor to the training data\n",
        "xgb_reg.fit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTCVpyz1u4bk"
      },
      "outputs": [],
      "source": [
        "# make predictions on test data\n",
        "xgb_pred = xgb_reg.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2B_VEMQRTel"
      },
      "outputs": [],
      "source": [
        "xgb_pred[xgb_pred < 0] = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2WrVxlFv4ZW"
      },
      "outputs": [],
      "source": [
        "test_xgboost['xgb_swe_pred'] = xgb_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpe8-5Vlnr6S"
      },
      "outputs": [],
      "source": [
        "print('XGBoost model accuracy score: {0:0.4f}'\n",
        ".format(mean_squared_error(test_xgboost['swe'], test_xgboost['xgb_swe_pred'], squared=False)))\n",
        "# best 3.5091"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKP5e6lc-MzE"
      },
      "source": [
        "## LightGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hp7mk-kLDo8S"
      },
      "outputs": [],
      "source": [
        "all_xgboost['neighbor_relative_swe'] = all_xgboost['neighbor_relative_swe'].astype(float)\n",
        "test_xgboost['neighbor_relative_swe'] = test_xgboost['neighbor_relative_swe'].astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tfw2SkEjb7g6"
      },
      "outputs": [],
      "source": [
        "cat_cols = ['lccs_0', 'lccs_1', 'lccs_2']\n",
        "lgb_cols = [\n",
        "            'latitude',\n",
        "            'longitude',\n",
        "            'elevation_m',\n",
        "            'elevation_var_m',\n",
        "            'snow_season_day',\n",
        "            'water',\n",
        "            'neighbor_relative_swe',\n",
        "            'east_elev_grad',\n",
        "            'south_elev_grad',\n",
        "            ] \\\n",
        "            + modis_roll_cols + climate_cols + cat_cols\n",
        "\n",
        "label_col = ['swe']\n",
        "\n",
        "X_lgb = all_xgboost[lgb_cols]\n",
        "y = all_xgboost[label_col]\n",
        "\n",
        "X_lgb_test = test_xgboost[lgb_cols]\n",
        "y_test = test_xgboost[label_col]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Hy_IHSz-Q4I"
      },
      "outputs": [],
      "source": [
        "train_data = lgb.Dataset(X_lgb, label=y)\n",
        "test_data = lgb.Dataset(X_lgb_test, label=y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHyiLqwD-j5J"
      },
      "outputs": [],
      "source": [
        "lgb_reg = lgb.LGBMRegressor(\n",
        "            nthread=4,\n",
        "            n_estimators=20000,\n",
        "            num_leaves=64,\n",
        "            max_depth=32,\n",
        "            learning_rate=0.04,\n",
        "            min_child_weight=30,\n",
        "            subsample=1,\n",
        "            colsample_bytree=0.95,\n",
        "            reg_alpha=0.0,\n",
        "            reg_lambda=0.075,\n",
        "            min_split_gain=0.0,\n",
        "            silent=-1,\n",
        "            verbose=-1,\n",
        "            )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyWbsHtO_R7w"
      },
      "outputs": [],
      "source": [
        "lgb_reg.fit(X_lgb,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpRKLft_D_3i"
      },
      "outputs": [],
      "source": [
        "lgb_pred = lgb_reg.predict(X_lgb_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6gWsgblqNsx"
      },
      "outputs": [],
      "source": [
        "lgb_pred[lgb_pred < 0] = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33RYbFgllPnb"
      },
      "outputs": [],
      "source": [
        "test_xgboost['lgb_swe_pred'] = lgb_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnDnb_I7FY_H"
      },
      "outputs": [],
      "source": [
        "print('LGB model accuracy score: {0:0.4f}'\n",
        ".format(mean_squared_error(test_xgboost['swe'], test_xgboost['lgb_swe_pred'], squared=False)))\n",
        "# best 3.3145"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K253PgBAgFI9"
      },
      "source": [
        "## Catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkETofWVjpvb"
      },
      "outputs": [],
      "source": [
        "all_xgboost['lccs_1'] = all_xgboost['lccs_1'].fillna(0).astype(int)\n",
        "all_xgboost['lccs_2'] = all_xgboost['lccs_2'].fillna(0).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWur_Qslvzns"
      },
      "outputs": [],
      "source": [
        "all_xgboost['region'] = all_xgboost['region'].fillna('gm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "np1wOmGSjIaJ"
      },
      "outputs": [],
      "source": [
        "cat_cols = ['lccs_0', 'lccs_1', 'lccs_2', 'region']\n",
        "cb_cols = [\n",
        "            'latitude',\n",
        "            'longitude',\n",
        "            'elevation_m',\n",
        "            'elevation_var_m',\n",
        "            'snow_season_day',\n",
        "            'water',\n",
        "            'neighbor_relative_swe',\n",
        "            'east_elev_grad',\n",
        "            'south_elev_grad',\n",
        "            ] \\\n",
        "            + modis_roll_cols + climate_cols + cat_cols\n",
        "\n",
        "label_col = ['swe']\n",
        "\n",
        "X_cb = all_xgboost[cb_cols]\n",
        "y = all_xgboost[label_col]\n",
        "\n",
        "X_cb_test = test_xgboost[cb_cols]\n",
        "y_test = test_xgboost[label_col]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIUntFjKgcPd"
      },
      "outputs": [],
      "source": [
        "train_dataset = cb.Pool(data=X_cb, \n",
        "                        label=y,\n",
        "                        cat_features=[20, 21, 22, 23]) \n",
        "test_dataset = cb.Pool(data=X_cb_test,\n",
        "                       label=y_test,\n",
        "                       cat_features=[20, 21, 22, 23])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ex-IY2UtgcMn"
      },
      "outputs": [],
      "source": [
        "cb_model = cb.CatBoostRegressor(iterations=550,\n",
        "                             learning_rate=0.04,\n",
        "                             depth=16,\n",
        "                             l2_leaf_reg=0.05,\n",
        "                             model_size_reg=None,\n",
        "                             loss_function=\"RMSE\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFtKC6hLgcLT"
      },
      "outputs": [],
      "source": [
        "cb_model.fit(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QQmSFbggcF_"
      },
      "outputs": [],
      "source": [
        "cb_pred = cb_model.predict(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MaE-614T2P58"
      },
      "outputs": [],
      "source": [
        "cb_pred[cb_pred < 0] = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boeWQdEq2Pya"
      },
      "outputs": [],
      "source": [
        "test_xgboost['cb_swe_pred'] = cb_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ST74baTf2Pr9"
      },
      "outputs": [],
      "source": [
        "print('CB model accuracy score: {0:0.4f}'\n",
        ".format(mean_squared_error(test_xgboost['swe'], test_xgboost['cb_swe_pred'], squared=False)))\n",
        "# best 3.4749"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnyxURFcEWtB"
      },
      "outputs": [],
      "source": [
        "cb_fi = cb_model.feature_importances_.argsort()\n",
        "np.array(cb_cols)[cb_fi]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4UUBwTSFZxF"
      },
      "source": [
        "## Ensemble XGB, LGB, and Catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZmLtiuDEQ_l"
      },
      "outputs": [],
      "source": [
        "# Use Linear Regression to calculate weight to assign to each model\n",
        "reg = linear_model.LinearRegression(fit_intercept=False)\n",
        "reg.fit(test_xgboost[['lgb_swe_pred','xgb_swe_pred','cb_swe_pred']], test_xgboost['swe'])\n",
        "test_xgboost['optimal_swe'] = reg.predict(test_xgboost[['lgb_swe_pred','xgb_swe_pred','cb_swe_pred']])\n",
        "print(reg.coef_/sum(reg.coef_))\n",
        "mean_squared_error(test_xgboost['swe'], test_xgboost['optimal_swe'], squared=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sierras = test_xgboost[test_xgboost['region'] == 'sierras']\n",
        "reg = linear_model.LinearRegression(fit_intercept=False)\n",
        "reg.fit(sierras[['lgb_swe_pred','xgb_swe_pred','cb_swe_pred']], sierras['swe'])\n",
        "print(reg.coef_/sum(reg.coef_))"
      ],
      "metadata": {
        "id": "CEaQ8maqylHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rockies = test_xgboost[test_xgboost['region'] == 'central rockies']\n",
        "reg = linear_model.LinearRegression(fit_intercept=False)\n",
        "reg.fit(rockies[['lgb_swe_pred','xgb_swe_pred','cb_swe_pred']], rockies['swe'])\n",
        "print(reg.coef_/sum(reg.coef_))"
      ],
      "metadata": {
        "id": "VL25CzwdyM9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "other = test_xgboost[test_xgboost['region'] == 'other']\n",
        "reg = linear_model.LinearRegression(fit_intercept=False)\n",
        "reg.fit(other[['lgb_swe_pred','xgb_swe_pred','cb_swe_pred']], other['swe'])\n",
        "print(reg.coef_/sum(reg.coef_))"
      ],
      "metadata": {
        "id": "9YlyNwkpzJ2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DUqonJpEn1g"
      },
      "outputs": [],
      "source": [
        "# See rmse by region\n",
        "test_xgboost['rse'] = (test_xgboost['swe'] - test_xgboost['optimal_swe'])**2\n",
        "np.sqrt(test_xgboost.rse.mean())\n",
        "#3.194904948806366"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLSWe56nEnqa"
      },
      "outputs": [],
      "source": [
        "np.sqrt(test_xgboost.groupby('region').mean()['rse'])\n",
        "# Best\n",
        "# central rockies    3.025345\n",
        "# other              2.597061\n",
        "# sierras            3.433604"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVaKoiHo5pWl"
      },
      "outputs": [],
      "source": [
        "np.sqrt(test_xgboost.groupby('snow_season_period').mean()['rse'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufVo6btZHMhc"
      },
      "outputs": [],
      "source": [
        "def gb_ensemble(row):\n",
        "  if row['region'] == 'sierras':\n",
        "    swe_pred = (0.40*row['lgb_swe_pred']\n",
        "                + 0.25*row['xgb_swe_pred']\n",
        "                + 0.35*row['cb_swe_pred'])\n",
        "  elif row['region'] == 'central rockies':\n",
        "    swe_pred = (0.80*row['lgb_swe_pred']\n",
        "                + 0.20*row['xgb_swe_pred'])\n",
        "  else:\n",
        "    swe_pred = (0.70*row['lgb_swe_pred']\n",
        "                + 0.20*row['xgb_swe_pred']\n",
        "                + 0.10*row['cb_swe_pred'])\n",
        "  return swe_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlLgANLcH1eX"
      },
      "outputs": [],
      "source": [
        "test_xgboost['best_swe_pred'] = test_xgboost.apply(lambda x: gb_ensemble(x), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IU3IPSkLELSM"
      },
      "outputs": [],
      "source": [
        "print('Ensemble model accuracy score: {0:0.4f}'\n",
        ".format(mean_squared_error(test_xgboost['swe'], test_xgboost['best_swe_pred'], squared=False)))\n",
        "# best 3.2217"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9oojP2qI22l"
      },
      "outputs": [],
      "source": [
        "test_xgboost['rse'] = (test_xgboost['swe'] - test_xgboost['best_swe_pred'])**2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2cCRfFfI9OD"
      },
      "outputs": [],
      "source": [
        "np.sqrt(test_xgboost.groupby('region').mean()['rse'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDBw9GiM6dmV"
      },
      "outputs": [],
      "source": [
        "test_xgboost[['swe','region','lgb_swe_pred','xgb_swe_pred','cb_swe_pred','best_swe_pred']].sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42m_TTdw5R0V"
      },
      "outputs": [],
      "source": [
        "lgb.plot_importance(lgb_reg, importance_type=\"gain\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRKF8Jgp0q-s"
      },
      "outputs": [],
      "source": [
        "lgb.plot_importance(lgb_reg) # importance_type = \"split\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6MI_Ro8Rsld"
      },
      "source": [
        "## Train Model on all data for Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "002bBEnsL3Z-"
      },
      "outputs": [],
      "source": [
        "final_dataset = pd.concat([gm_xgboost, train_xgboost, test_xgboost])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCl2l_OgIvk4"
      },
      "source": [
        "#### XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csq1izO9Q6p8"
      },
      "outputs": [],
      "source": [
        "X_all = final_dataset[xgb_cols]\n",
        "y_all = final_dataset[label_col]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzmepdcXC8HU"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "scaler.fit(X_all)\n",
        "X_all = scaler.transform(X_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGwDFrfBV0dE"
      },
      "outputs": [],
      "source": [
        "dump(scaler, '/content/drive/MyDrive/snocast/eval/models/std_scaler.bin', compress=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4te17HbDmD7I"
      },
      "outputs": [],
      "source": [
        "# define data_dmatrix\n",
        "xgb_dmatrix = xgb.DMatrix(data=X_all,label=y_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2nYgvBfp7Ga"
      },
      "outputs": [],
      "source": [
        "params = {\n",
        "          'n_estimators': 1100,\n",
        "          'min_split_loss': 0.08,\n",
        "          'gamma': 0.1,\n",
        "          'max_depth': 32,\n",
        "          'min_child_weight': 45,\n",
        "          'learning_rate': 0.04,\n",
        "          'objective':'reg:squarederror',\n",
        "          'n_jobs': 4,\n",
        "          'subsample': 0.95,\n",
        "          'colsample_bytree': 0.95,\n",
        "          }\n",
        "\n",
        "# instantiate the regressor\n",
        "xgb_reg_all = xgb.XGBRegressor(**params) # Params defined during cross-validation above\n",
        "\n",
        "# fit the regressor to the training data\n",
        "xgb_reg_all.fit(X_all, y_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYLA3k0eqMyz"
      },
      "outputs": [],
      "source": [
        "xgb_reg_all.save_model('/content/drive/MyDrive/snocast/eval/models/xgb_all.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtsOaKAOrHyM"
      },
      "outputs": [],
      "source": [
        "# dump model with feature map\n",
        "xgb_reg_all.get_booster().dump_model('/content/drive/MyDrive/snocast/train/models/xgb_all.json', dump_format='json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SP6USSgzIyP-"
      },
      "source": [
        "#### LightGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7TovWjHIf2s"
      },
      "outputs": [],
      "source": [
        "final_dataset['neighbor_relative_swe'] = final_dataset['neighbor_relative_swe'].astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkEJvYroVQxm"
      },
      "outputs": [],
      "source": [
        "X_all = final_dataset[lgb_cols]\n",
        "y_all = final_dataset[label_col]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcFSdKhQVQna"
      },
      "outputs": [],
      "source": [
        "train_data = lgb.Dataset(X_all, label=y_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUWJx6zhVQdh"
      },
      "outputs": [],
      "source": [
        "lgb_reg = lgb.LGBMRegressor(\n",
        "            nthread=4,\n",
        "            n_estimators=21000,\n",
        "            num_leaves=64,\n",
        "            max_depth=32,\n",
        "            learning_rate=0.04,\n",
        "            min_child_weight=30,\n",
        "            subsample=1,\n",
        "            colsample_bytree=0.95,\n",
        "            reg_alpha=0.0,\n",
        "            reg_lambda=0.075,\n",
        "            min_split_gain=0.0,\n",
        "            silent=-1,\n",
        "            verbose=-1,\n",
        "            )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sm1IcxctVpIw"
      },
      "outputs": [],
      "source": [
        "lgb_reg.fit(X_all,y_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3COEad0xLsPl"
      },
      "outputs": [],
      "source": [
        "lgb_reg.booster_.save_model('/content/drive/MyDrive/snocast/eval/models/lgb_all.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nb7xJUP-I1UZ"
      },
      "source": [
        "#### Catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyLvki05ItUv"
      },
      "outputs": [],
      "source": [
        "final_dataset['lccs_1'] = final_dataset['lccs_1'].fillna(0).astype(int)\n",
        "final_dataset['lccs_2'] = final_dataset['lccs_2'].fillna(0).astype(int)\n",
        "final_dataset['region'] = final_dataset['region'].fillna('gm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgvbhQHQGTOK"
      },
      "outputs": [],
      "source": [
        "X_all = final_dataset[cb_cols]\n",
        "y_all = final_dataset[label_col]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrPpupz1GTLC"
      },
      "outputs": [],
      "source": [
        "train_dataset = cb.Pool(data=X_all, \n",
        "                        label=y_all,\n",
        "                        cat_features=[20, 21, 22, 23]) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kRl9J5kGTHL"
      },
      "outputs": [],
      "source": [
        "cb_model = cb.CatBoostRegressor(iterations=600,\n",
        "                             learning_rate=0.04,\n",
        "                             depth=16,\n",
        "                             l2_leaf_reg=0.05,\n",
        "                             model_size_reg=None,\n",
        "                             loss_function=\"RMSE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RflM3MAzGS-I"
      },
      "outputs": [],
      "source": [
        "cb_model.fit(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJxNNLxyHHaw"
      },
      "outputs": [],
      "source": [
        "cb_model.save_model('/content/drive/MyDrive/snocast/eval/models/cb_all.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fi3VCgMpSS0"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "train_model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}