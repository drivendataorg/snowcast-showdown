{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hulA24FNQ1m5"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8ViTTAaQ6H6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "from collections import defaultdict\n",
        "import ee"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IezUf5ccSSn6"
      },
      "source": [
        "## Import Base Data Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Lg6EdLiRZ8_"
      },
      "outputs": [],
      "source": [
        "data_dir = '/content/drive/MyDrive/snocast/train/data'\n",
        "\n",
        "ground_measures_train = pd.read_csv(os.path.join(data_dir, 'static/ground_measures_train_features.csv'))\n",
        "ground_measures_train.columns = ['station_id'] + list(ground_measures_train.columns[1:])\n",
        "gm_melt_train = ground_measures_train.melt(id_vars=[\"station_id\"],\n",
        "                                            var_name=\"date\",\n",
        "                                            value_name=\"swe\").dropna()\n",
        "            \n",
        "\n",
        "ground_measures_test = pd.read_csv(os.path.join(data_dir, 'static/ground_measures_test_features.csv'))\n",
        "ground_measures_test.columns = ['station_id'] + list(ground_measures_test.columns[1:])\n",
        "gm_melt_test = ground_measures_test.melt(id_vars=[\"station_id\"],\n",
        "                           var_name=\"date\",\n",
        "                           value_name=\"swe\").dropna()\n",
        "                           \n",
        "ground_measures_metadata = pd.read_csv(os.path.join(data_dir, 'static/ground_measures_metadata.csv'))\n",
        "ground_measures_all = pd.merge(ground_measures_train, ground_measures_test, how='outer', on='station_id')\n",
        "gm_melt_all = ground_measures_all.melt(id_vars=[\"station_id\"],\n",
        "                           var_name=\"date\",\n",
        "                           value_name=\"swe\").dropna()\n",
        "gm_seq = pd.merge(gm_melt_all, ground_measures_metadata, how='inner', on='station_id')\n",
        "\n",
        "train_labels = pd.read_csv(os.path.join(data_dir, 'static/train_labels.csv'))\n",
        "labels_melt_train = train_labels.melt(id_vars=[\"cell_id\"],\n",
        "                  var_name=\"date\",\n",
        "                  value_name=\"swe\").dropna()\n",
        "\n",
        "test_labels = pd.read_csv(os.path.join(data_dir, 'static/labels_2020_2021.csv'))\n",
        "labels_melt_test = test_labels.melt(id_vars=[\"cell_id\"],\n",
        "                  var_name=\"date\",\n",
        "                  value_name=\"swe\").dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0GJtFEFR7mj"
      },
      "outputs": [],
      "source": [
        "# get latitude longitude for train and test grids\n",
        "f = open(os.path.join(data_dir, 'static/grid_cells.geojson'))\n",
        "grid_cells = json.load(f)\n",
        "print('length grid_cells features: ', len(grid_cells['features']))\n",
        "\n",
        "grid_features = defaultdict(dict)\n",
        "for grid_cell in grid_cells['features']:\n",
        "  cell_id = grid_cell['properties']['cell_id']\n",
        "  coordinates = grid_cell['geometry']['coordinates'][0]\n",
        "  region = grid_cell['properties']['region']\n",
        "  grid_features[cell_id] = {'coordinates': coordinates[1:],\n",
        "                            'region': region}\n",
        "\n",
        "grid_features_train = defaultdict(dict)\n",
        "train_ids = []\n",
        "train_lats = []\n",
        "train_lons = []\n",
        "train_regions = []\n",
        "train_bboxes = []\n",
        "grid_features_test = defaultdict(dict)\n",
        "test_ids = []\n",
        "test_lats = []\n",
        "test_lons = []\n",
        "test_regions = []\n",
        "test_bboxes = []\n",
        "\n",
        "\n",
        "for cell_id in train_labels['cell_id'].values:\n",
        "  train_ids.append(cell_id)\n",
        "  lon, lat = np.mean(grid_features[cell_id]['coordinates'], axis=0)\n",
        "  max_lon, max_lat = np.max(grid_features[cell_id]['coordinates'], axis=0)\n",
        "  min_lon, min_lat = np.min(grid_features[cell_id]['coordinates'], axis=0)\n",
        "  # bbox = [min_lon, min_lat, max_lon, max_lat]\n",
        "  bbox = np.array([min_lon, min_lat,max_lon, max_lat])\n",
        "  train_regions = grid_features[cell_id]['region']\n",
        "  train_lats.append(lat)\n",
        "  train_lons.append(lon)\n",
        "  train_bboxes.append(bbox)\n",
        "\n",
        "  grid_features[cell_id]['dataset'] = 'train'\n",
        "\n",
        "for cell_id in test_labels['cell_id'].values:\n",
        "  test_ids.append(cell_id)\n",
        "  lon, lat = np.mean(grid_features[cell_id]['coordinates'], axis=0)\n",
        "  max_lon, max_lat = np.max(grid_features[cell_id]['coordinates'], axis=0)\n",
        "  min_lon, min_lat = np.min(grid_features[cell_id]['coordinates'], axis=0)\n",
        "  # bbox = [min_lon, min_lat, max_lon, max_lat]\n",
        "  bbox = np.array([min_lon, min_lat,max_lon, max_lat])\n",
        "  test_regions = grid_features[cell_id]['region']\n",
        "  test_lats.append(lat)\n",
        "  test_lons.append(lon)\n",
        "  test_bboxes.append(bbox)\n",
        "\n",
        "  if 'dataset' in grid_features[cell_id].keys():\n",
        "    grid_features[cell_id]['dataset'] = 'both'\n",
        "  else:\n",
        "    grid_features[cell_id]['dataset'] = 'test'\n",
        "\n",
        "for cell_id in grid_features:\n",
        "  if grid_features[cell_id]['dataset'] in ('test','both'):\n",
        "    grid_features_test[cell_id] = grid_features[cell_id]\n",
        "  if grid_features[cell_id]['dataset'] in ('train','both'):\n",
        "    grid_features_train[cell_id] = grid_features[cell_id]\n",
        "print(\"test count: \", len(grid_features_test))\n",
        "print(\"train count: \", len(grid_features_train))\n",
        "\n",
        "\n",
        "train_lat_lon = pd.DataFrame({'cell_id': train_ids, \n",
        "                              'latitude': train_lats, \n",
        "                              'longitude': train_lons, \n",
        "                              'region': train_regions,\n",
        "                              'bbox': train_bboxes})\n",
        "test_lat_lon = pd.DataFrame({'cell_id': test_ids, \n",
        "                             'latitude': test_lats, \n",
        "                             'longitude': test_lons, \n",
        "                             'region': test_regions,\n",
        "                             'bbox': test_bboxes})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create sequential dataframes for train and test\n",
        "train_label_seq = pd.merge(labels_melt_train, train_lat_lon, how='inner', on='cell_id')\n",
        "test_pred_seq = pd.merge(labels_melt_test, test_lat_lon, how='inner', on='cell_id')\n",
        "gm_seq = pd.merge(gm_melt_all, ground_measures_metadata, how='inner', on='station_id')"
      ],
      "metadata": {
        "id": "P0W-4kdcWo77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-z2IQlUr-RM5"
      },
      "outputs": [],
      "source": [
        "# Trigger the authentication flow.\n",
        "ee.Authenticate()\n",
        "\n",
        "# Initialize the library.\n",
        "ee.Initialize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYs19q9C-JKk"
      },
      "outputs": [],
      "source": [
        "# Import the MODIS Terra Snow Cover Daily Global 500m collection.\n",
        "terra = ee.ImageCollection('MODIS/006/MOD10A1')\n",
        "\n",
        "# Import the MODIS Aqua Snow Cover Daily Global 500m collection.\n",
        "aqua = ee.ImageCollection('MODIS/006/MYD10A1')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_modis_data(df, df_seq, gm=False):\n",
        "  unique_dates = df_seq['date'].unique()\n",
        "  \n",
        "  min_date = (datetime.datetime.strptime(unique_dates.min(),'%Y-%m-%d') - datetime.timedelta(days=15)).strftime('%Y-%m-%d')\n",
        "  max_date = (datetime.datetime.strptime(unique_dates.max(),'%Y-%m-%d') + datetime.timedelta(days=1)).strftime('%Y-%m-%d')\n",
        "  print(min_date, max_date)\n",
        "\n",
        "  if gm:\n",
        "    location_col = 'station_id'\n",
        "  else:\n",
        "    location_col = 'cell_id'\n",
        "  modis_cols = [location_col,'latitude','longitude']\n",
        "  unique_ids = df[modis_cols]\n",
        "  print(unique_ids.shape)\n",
        "\n",
        "  terra_snow_cover = terra.select('NDSI_Snow_Cover').filterDate(min_date, max_date)\n",
        "  aqua_snow_cover = aqua.select('NDSI_Snow_Cover').filterDate(min_date, max_date)\n",
        "  terra_info = terra_snow_cover.getInfo()['features']\n",
        "  aqua_info = aqua_snow_cover.getInfo()['features']\n",
        "  print('Terra min date: {}'.format(terra_info[0]['properties']['system:index']))\n",
        "  print('Terra max date: {}'.format(terra_info[-1]['properties']['system:index']))\n",
        "  print('Aqua min date: {}'.format(aqua_info[0]['properties']['system:index']))\n",
        "  print('Aqua max date: {}'.format(aqua_info[-1]['properties']['system:index']))\n",
        "\n",
        "  output_cols = ['date',\n",
        "                  'longitude',\n",
        "                  'latitude',\n",
        "                  'time',\n",
        "                  'NDSI_Snow_Cover']\n",
        "\n",
        "  terra_list = []\n",
        "  aqua_list = []\n",
        "  terra_ids = []\n",
        "  aqua_ids = []\n",
        "\n",
        "  # Runs in 4 hours\n",
        "  for idx, row in df.iterrows():\n",
        "      if idx % 250 == 0:\n",
        "        print(idx)\n",
        "\n",
        "      # Define a region of interest with a buffer zone of 500 m\n",
        "      poi = ee.Geometry.Point(row['longitude'], row['latitude'])\n",
        "      roi = poi.buffer(500)\n",
        "\n",
        "      terra_data = terra_snow_cover.getRegion(roi, scale=500).getInfo()[1:]\n",
        "      terra_ids.extend([row[location_col]]*len(terra_data))\n",
        "      terra_list.extend(terra_data)\n",
        "\n",
        "      aqua_data = aqua_snow_cover.getRegion(roi, scale=500).getInfo()[1:]\n",
        "      aqua_ids.extend([row[location_col]]*len(aqua_data))\n",
        "      aqua_list.extend(aqua_data)\n",
        "\n",
        "  terra_df = pd.DataFrame(terra_list, columns=output_cols)\n",
        "  terra_df['location_id'] = terra_ids\n",
        "\n",
        "  aqua_df = pd.DataFrame(aqua_list, columns=output_cols)\n",
        "  aqua_df['location_id'] = aqua_ids\n",
        "\n",
        "  return terra_df, aqua_df"
      ],
      "metadata": {
        "id": "3v-T2fM_Znhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_terra_df, train_aqua_df = get_modis_data(train_lat_lon, train_label_seq)"
      ],
      "metadata": {
        "id": "ZMTEa-YScy9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_terra_df.to_parquet(f'/content/drive/MyDrive/snocast/train/data/modis/modis_terra_train.parquet')\n",
        "train_aqua_df.to_parquet(f'/content/drive/MyDrive/snocast/train/data/modis/modis_aqua_train.parquet')"
      ],
      "metadata": {
        "id": "QpRWRSw-Znb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_terra_df, test_aqua_df = get_modis_data(test_lat_lon, test_pred_seq, gm=True)"
      ],
      "metadata": {
        "id": "dT_qTlEUzKTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_terra_df.to_parquet(f'/content/drive/MyDrive/snocast/test/data/modis/modis_terra_test.parquet')\n",
        "test_aqua_df.to_parquet(f'/content/drive/MyDrive/snocast/test/data/modis/modis_aqua_test.parquet')"
      ],
      "metadata": {
        "id": "XV_mIpBPzmRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gm_terra_df, gm_aqua_df = get_modis_data(ground_measures_metadata, gm_seq, gm=True)"
      ],
      "metadata": {
        "id": "ZFj_IzG7f3jN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gm_terra_df.to_parquet(f'/content/drive/MyDrive/snocast/train/data/modis/modis_terra_gm.parquet')\n",
        "gm_aqua_df.to_parquet(f'/content/drive/MyDrive/snocast/train/data/modis/modis_aqua_gm.parquet')"
      ],
      "metadata": {
        "id": "UHo0r5QKzV2N"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "get_modis_all.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}