# Solution - Snowcast Showdown

Username: TeamUArizona

## Summary

This directory contains the data and python codes to train and run the Multilinear Regression (MLR) 
models to predict SWE for the Snowcast Showdown competition.  The codes make SWE predictions 
based entirely on provided SNOTEL and CDEC data (i.e. SNOTEL/CDEC ground measure data are the 
predictors in the MLR models).  Except for a couple hundred pixels which timeseries data from 
SNOTEL/CDEC stations, models are trained to predict University of Arizona (UA) SWE values based on 
the provided SNOTEL/CDEC data.  In addition to the MLR models, there is also code to fill missing 
SNOTEL/CDEC data and to perform additional bias correction for areas that have observed SWE data 
in the period of record (e.g. from past Lidar flyovers).  All of the model weights that are used for 
gap filling, MLR, and bias adjustment are saved as frozen model weights in saved files.  Ultimately, 
model predictions are made by averaging an ensemble of MLR models which are trained using different 
combinations of years.  Based on our tests, this ensemble of MLR models performed best of several 
different machine learning methods that we tested, and using the UA SWE data provided good model 
training under a variety of conditions. 

Contents of this directory:

```
TrainModels.py  	- Code to train the MLR models
RunModels.py   	 	- Code to run the trained MLR models in realtime
```

Note that all model assets are located in the original submitted code directory

```
Models         		- Directory containing frozen model weights 
Output CSV      	- Directory where submission files are saved
Data            	- Directory of data used in the development of the models

    grid_cells.geojson    		- provided spatial data layer containing information  
                                    	  about each predicted location
    ground_measures_metadata.csv 	- provided information about the ground measures data
                                    	  used as model input
    ground_measures_data.csv      	- SNOTEL/CDEC data used during model training
    ua_swe_data.csv              	- UA SWE data for each predicted location during the 
                                    	  modeled training period
    train_label_data.csv          	- other observed SWE data during the training period.  
```

SNOTEL/CDEC data come from the provided data files.  UA SWE data 
(https://nsidc.org/data/nsidc-0719) are downscaled to 800 m using 800 m PRISM climatology data (from 
https://prism.oregonstate.edu/). Code to downsample the data is [provided here](https://github.com/broxtopd/UASWE).

Train label data are from the provided data files, as well as some 
additional ASO data from https://nsidc.org/data/aso/data-summaries and 
https://www.airbornesnowobservatories.com/.

# Setup

This solution requires python and several python dependencies to run.  The following python dependencies 
are required: sys, os, datetime, geojson, numpy, scipy, scikit-learn, pickle, urllib, io, time.  Depending 
on the python installation, some of these dependencies may need to be installed e.g. using pip or conda.  

It is likely that different versions will work, but the system was developed with the following python 
and dependency versions (installed using conda 4.11.0 and pip 21.0.1 on Windows):

```
Name            Version
python          3.8.5
geojson         2.5.0
numpy           1.19.2
scipy           1.5.2
scikit-learn    1.0.2
urllib3         1.26.4
pickle-mixin    1.0.2
```

# Hardware

One can expect processing times of ~10 minutes per ensemble member for model training (TrainModels.py), 
while model inference should occur in ~30-90 seconds (RunModels.py),depending on how many grid cells are 
estimated.  Tested on Windows 10 using an Intel(R) Core(TM) i7-8550U CPU @ 1.80GHz. 8 Gb Ram

# Run training

To train the models, invoke `TrainModels.py`

usage: TrainModels.py <year(s) withheld> <name given to model weights file>

TrainModels.py will perform model training for all years in the period of record (2013-2021), except for 
years listed as withheld years (which can be a single year, list of years separated by commas, or "nan" 
for none withheld).  All model weights will be saved in the Models directory with a name that is 
specified.  If years are withheld from model training, the models will also be run and evaluated for
the withheld years to give a sense of independent model performance.  An ensemble of MLR models is 
generated by repeatedly invoking TrainModels.py and withholding different years to generate each model 
as follows:

```
python TrainModels.py nan prediction_2022_e0
python TrainModels.py 2013 prediction_2022_e1
python TrainModels.py 2014 prediction_2022_e2
python TrainModels.py 2015 prediction_2022_e3
python TrainModels.py 2016 prediction_2022_e4
python TrainModels.py 2017 prediction_2022_e5
python TrainModels.py 2018 prediction_2022_e6
python TrainModels.py 2019 prediction_2022_e7
python TrainModels.py 2020 prediction_2022_e8
python TrainModels.py 2021 prediction_2022_e9
```

For convenience, the above commands can be executed by double clicking `TrainModels.bat` (on Windows)

Details about the modelling steps will be described in the forthcoming model report, but briefly, the 
algorithm consists largely of two prediction models for filling missing data and performing MLR to 
predict SWE at the modelling locations.  Both algorithms are similar in that surrounding SNOTEL/CDEC 
data is used to predict the target station/grid.  The gap filling algorithm differs from traditional MLR 
though in that it involves making separate predictions for each ground station based on surrounding 
stations, and weighting the predictions according to their correlation strength (how well the prediction 
matches the actual data when it is not missing), which are combined using a weighted average. This 
allows for any number of the surrounding stations to also contain missing data, in which case, those 
predictions are not used.  On the other hand, the MLR algorithm to predict SWE at the target grid cells 
requires no missing data (hence the gap filling step).  A final bias correction is applied in case a given 
grid cell has observed ground data at any point in the past.

TrainModels.py has parameters for each of these modelling steps and are described in the code 
comments.  For filling in missing data, the parameters control whether to use or not use particular 
stations based on how much data they have, how stations are selected as predictors for a given station, 
and how each of the predictions from each station is weighted.  The next set of parameters controls the 
multilinear regression: the minimum amount of ground data required to use in the MLR models, how 
stations are selected as predictors for a given location, and a check to ensure that week to week 
variations aren't too large.  For bias correction, the parameters specify the minimum number of ground 
samples at a particular location required to attempt bias correction, and a range of coefficients allowed 
for the equations used for bias correction.

# Run inference

To run the models: invoke `RunModels.py`

usage: RunModels.py <name(s) given to model weights file(s)> <name given to output csv file>

RunModels.py will download current ground measures data from the competition website, fill missing 
ground measures data, perform the MLR, and if needed, apply a bias adjustment for each predicted 
location based on the saved model weights.  It can either run with a single model weights file or multiple
weights files (in which case, it will average the predictions based on each set of weights).  Finally, it 
produces a csv file suitable for submission to the competition, which is saved in the Output CSV 
directory.

The command to run RunModels.py with the above generated model ensemble is:

```
python RunModels.py prediction_2022_e0,prediction_2022_e1,prediction_2022_e2,prediction_2022_e3,prediction_2022_e4,prediction_2022_e5,prediction_2022_e6,prediction_2022_e7,prediction_2022_e8,prediction_2022_e9 prediction_2022_ens
```

For convenience, the above command can be executed by double clicking `RunModels.bat` (on Windows)
