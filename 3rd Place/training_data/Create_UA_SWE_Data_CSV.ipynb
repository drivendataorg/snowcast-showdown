{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68226ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Script to generate ua_swe_data.csv (from UA SWE tif files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a022c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup Python environment\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import geojson\n",
    "import numpy as np\n",
    "from osgeo import gdal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ebf8588",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get Metadata for the Evaluation Stage Grid Cells\n",
    "\n",
    "with open('Data/Snowcast Evaluation/grid_cells.geojson') as f:\n",
    "    gj = geojson.load(f)\n",
    "features = gj['features']\n",
    "evaluation_cell_ids = []\n",
    "evaluation_coordinates = []\n",
    "for feature in features:\n",
    "    evaluation_cell_ids.append(feature['properties']['cell_id'])\n",
    "    evaluation_coordinates.append(feature['geometry']['coordinates'])\n",
    "\n",
    "evaluation_cell_ids = np.array(evaluation_cell_ids)  # So we can do numpy stuff to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c9ea4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the time array by looking at the Training Label Data from the Development Stage\n",
    "# (Training dates will be defined for all of these days)\n",
    "\n",
    "# Training dataset\n",
    "with open('Data/Snowcast Development/train_labels.csv') as f:\n",
    "    tline = f.readline().replace('\\n','')\n",
    "    header = tline.split(',')\n",
    "    \n",
    "times = np.array(header[1:])\n",
    "\n",
    "# 2020-2021 Data\n",
    "with open('Data/Snowcast Evaluation/labels_2020_2021.csv') as f:\n",
    "    tline = f.readline().replace('\\n','')\n",
    "    header = tline.split(',')\n",
    "    \n",
    "times_2021 = np.array(header[1:])\n",
    "\n",
    "# Concatenate the time vectors\n",
    "times = np.concatenate((times, times_2021))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b02790c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get locations of UA grid cells to extract data for\n",
    "\n",
    "# Read one of the files to get spatial information about it\n",
    "yyyy, mm, dd = times[0].split('-')\n",
    "src = gdal.Open('Data/UASWE/' + yyyy + '/' + mm + '/' + dd + '/SWE.tif')\n",
    "ncols = src.RasterXSize\n",
    "nrows = src.RasterYSize\n",
    "ulx, xres, xskew, uly, yskew, yres  = src.GetGeoTransform()\n",
    "lrx = ulx + (ncols * xres)\n",
    "lry = uly + (nrows * yres)\n",
    "data = src.ReadAsArray()\n",
    "src = None\n",
    "\n",
    "UASWE_1km_XLocations = []\n",
    "UASWE_1km_YLocations = []\n",
    "\n",
    "# Matrix of latitudes and longitudes (used in the 'if' block below)\n",
    "x = np.tile(np.arange(ulx+xres/2, lrx, xres), [nrows, 1])\n",
    "y = np.tile(np.arange(uly+yres/2, lry, yres), [ncols, 1]).T\n",
    "\n",
    "# Loop through the locations and extract data for each grid cell\n",
    "for i in range(len(evaluation_cell_ids)):\n",
    "    \n",
    "    # From the centroids of each grid cell, figure out which UA SWE grid cell to extract data from\n",
    "    X_center = (evaluation_coordinates[i][0][0][0] + evaluation_coordinates[i][0][2][0]) / 2\n",
    "    Y_center = (evaluation_coordinates[i][0][0][1] + evaluation_coordinates[i][0][1][1]) / 2\n",
    "    xloc = -int((ulx - X_center) / xres)\n",
    "    yloc = -int((uly - Y_center) / yres)\n",
    "    \n",
    "    # If the pixel is nan (e.g. water body), then look for the nearest non-nan pixel\n",
    "    if np.isnan(data[yloc,xloc]):\n",
    "        locs = (x > X_center-0.05) * (x < X_center+0.05) * (y > Y_center-0.05) * (y < Y_center+0.05)\n",
    "        dist_sub = np.sqrt((x[locs]-X_center)**2 + (y[locs]-Y_center)**2)\n",
    "        dist = np.ones(x.shape) * np.nan\n",
    "        dist[locs] = dist_sub\n",
    "        dist[np.isnan(data)] = np.nan\n",
    "        loc = np.where(dist == np.nanmin(dist))\n",
    "        xloc = loc[0][0]\n",
    "        yloc = loc[1][0]\n",
    "        \n",
    "    UASWE_1km_XLocations.append(xloc)\n",
    "    UASWE_1km_YLocations.append(yloc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75139e08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df49f0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Data for 2013/01/01\n",
      "Reading Data for 2013/01/08\n",
      "Reading Data for 2013/01/15\n",
      "Reading Data for 2013/01/22\n",
      "Reading Data for 2013/01/29\n",
      "Reading Data for 2013/02/05\n",
      "Reading Data for 2013/02/12\n",
      "Reading Data for 2013/02/19\n",
      "Reading Data for 2013/02/26\n",
      "Reading Data for 2013/03/05\n",
      "Reading Data for 2013/03/12\n",
      "Reading Data for 2013/03/19\n",
      "Reading Data for 2013/03/26\n",
      "Reading Data for 2013/04/02\n",
      "Reading Data for 2013/04/03\n",
      "Reading Data for 2013/04/09\n",
      "Reading Data for 2013/04/16\n",
      "Reading Data for 2013/04/23\n",
      "Reading Data for 2013/04/29\n",
      "Reading Data for 2013/04/30\n",
      "Reading Data for 2013/05/03\n",
      "Reading Data for 2013/05/07\n",
      "Reading Data for 2013/05/14\n",
      "Reading Data for 2013/05/21\n",
      "Reading Data for 2013/05/25\n",
      "Reading Data for 2013/05/28\n",
      "Reading Data for 2013/06/01\n",
      "Reading Data for 2013/06/04\n",
      "Reading Data for 2013/06/08\n",
      "Reading Data for 2013/06/11\n",
      "Reading Data for 2013/06/18\n",
      "Reading Data for 2013/06/25\n",
      "Reading Data for 2013/12/03\n",
      "Reading Data for 2013/12/10\n",
      "Reading Data for 2013/12/17\n",
      "Reading Data for 2013/12/24\n",
      "Reading Data for 2013/12/31\n",
      "Reading Data for 2014/01/07\n",
      "Reading Data for 2014/01/14\n",
      "Reading Data for 2014/01/21\n",
      "Reading Data for 2014/01/28\n",
      "Reading Data for 2014/02/04\n",
      "Reading Data for 2014/02/11\n",
      "Reading Data for 2014/02/18\n",
      "Reading Data for 2014/02/25\n",
      "Reading Data for 2014/03/04\n",
      "Reading Data for 2014/03/11\n",
      "Reading Data for 2014/03/18\n",
      "Reading Data for 2014/03/25\n",
      "Reading Data for 2014/04/01\n",
      "Reading Data for 2014/04/08\n",
      "Reading Data for 2014/04/15\n",
      "Reading Data for 2014/04/22\n",
      "Reading Data for 2014/04/29\n",
      "Reading Data for 2014/05/06\n",
      "Reading Data for 2014/05/13\n",
      "Reading Data for 2014/05/20\n",
      "Reading Data for 2014/05/27\n",
      "Reading Data for 2014/06/03\n",
      "Reading Data for 2014/06/10\n",
      "Reading Data for 2014/06/17\n",
      "Reading Data for 2014/06/24\n",
      "Reading Data for 2014/12/02\n",
      "Reading Data for 2014/12/09\n",
      "Reading Data for 2014/12/16\n",
      "Reading Data for 2014/12/23\n",
      "Reading Data for 2014/12/30\n",
      "Reading Data for 2015/01/06\n",
      "Reading Data for 2015/01/13\n",
      "Reading Data for 2015/01/20\n",
      "Reading Data for 2015/01/27\n",
      "Reading Data for 2015/02/03\n",
      "Reading Data for 2015/02/10\n",
      "Reading Data for 2015/02/17\n",
      "Reading Data for 2015/02/24\n",
      "Reading Data for 2015/03/03\n",
      "Reading Data for 2015/03/10\n",
      "Reading Data for 2015/03/17\n",
      "Reading Data for 2015/03/24\n",
      "Reading Data for 2015/03/31\n",
      "Reading Data for 2015/04/07\n",
      "Reading Data for 2015/04/14\n",
      "Reading Data for 2015/04/21\n",
      "Reading Data for 2015/04/28\n",
      "Reading Data for 2015/05/05\n",
      "Reading Data for 2015/05/12\n",
      "Reading Data for 2015/05/19\n",
      "Reading Data for 2015/05/26\n",
      "Reading Data for 2015/06/02\n",
      "Reading Data for 2015/06/09\n",
      "Reading Data for 2015/06/16\n",
      "Reading Data for 2015/06/23\n",
      "Reading Data for 2015/06/30\n",
      "Reading Data for 2015/12/01\n",
      "Reading Data for 2015/12/08\n",
      "Reading Data for 2015/12/15\n",
      "Reading Data for 2015/12/22\n",
      "Reading Data for 2015/12/29\n",
      "Reading Data for 2016/01/05\n",
      "Reading Data for 2016/01/12\n",
      "Reading Data for 2016/01/19\n",
      "Reading Data for 2016/01/26\n",
      "Reading Data for 2016/02/02\n",
      "Reading Data for 2016/02/08\n",
      "Reading Data for 2016/02/09\n",
      "Reading Data for 2016/02/16\n",
      "Reading Data for 2016/02/23\n",
      "Reading Data for 2016/03/01\n",
      "Reading Data for 2016/03/08\n",
      "Reading Data for 2016/03/15\n",
      "Reading Data for 2016/03/22\n",
      "Reading Data for 2016/03/26\n",
      "Reading Data for 2016/03/29\n",
      "Reading Data for 2016/04/01\n",
      "Reading Data for 2016/04/03\n",
      "Reading Data for 2016/04/04\n",
      "Reading Data for 2016/04/05\n",
      "Reading Data for 2016/04/07\n",
      "Reading Data for 2016/04/12\n",
      "Reading Data for 2016/04/16\n",
      "Reading Data for 2016/04/19\n",
      "Reading Data for 2016/04/26\n",
      "Reading Data for 2016/05/03\n",
      "Reading Data for 2016/05/09\n",
      "Reading Data for 2016/05/10\n",
      "Reading Data for 2016/05/17\n",
      "Reading Data for 2016/05/24\n",
      "Reading Data for 2016/05/27\n",
      "Reading Data for 2016/05/31\n",
      "Reading Data for 2016/06/07\n",
      "Reading Data for 2016/06/14\n",
      "Reading Data for 2016/06/21\n",
      "Reading Data for 2016/06/26\n",
      "Reading Data for 2016/06/28\n",
      "Reading Data for 2016/12/06\n",
      "Reading Data for 2016/12/13\n",
      "Reading Data for 2016/12/20\n",
      "Reading Data for 2016/12/27\n",
      "Reading Data for 2017/01/03\n",
      "Reading Data for 2017/01/10\n",
      "Reading Data for 2017/01/17\n",
      "Reading Data for 2017/01/24\n",
      "Reading Data for 2017/01/28\n",
      "Reading Data for 2017/01/29\n",
      "Reading Data for 2017/01/31\n",
      "Reading Data for 2017/02/07\n",
      "Reading Data for 2017/02/14\n",
      "Reading Data for 2017/02/21\n",
      "Reading Data for 2017/02/28\n",
      "Reading Data for 2017/03/07\n",
      "Reading Data for 2017/03/14\n",
      "Reading Data for 2017/03/21\n",
      "Reading Data for 2017/03/28\n",
      "Reading Data for 2017/04/04\n",
      "Reading Data for 2017/04/11\n",
      "Reading Data for 2017/04/18\n",
      "Reading Data for 2017/04/25\n",
      "Reading Data for 2017/05/02\n",
      "Reading Data for 2017/05/09\n",
      "Reading Data for 2017/05/16\n",
      "Reading Data for 2017/05/23\n",
      "Reading Data for 2017/05/30\n",
      "Reading Data for 2017/06/06\n",
      "Reading Data for 2017/06/13\n",
      "Reading Data for 2017/06/20\n",
      "Reading Data for 2017/06/27\n",
      "Reading Data for 2017/12/05\n",
      "Reading Data for 2017/12/12\n",
      "Reading Data for 2017/12/19\n",
      "Reading Data for 2017/12/26\n",
      "Reading Data for 2018/01/02\n",
      "Reading Data for 2018/01/09\n",
      "Reading Data for 2018/01/16\n",
      "Reading Data for 2018/01/23\n",
      "Reading Data for 2018/01/30\n",
      "Reading Data for 2018/02/06\n",
      "Reading Data for 2018/02/13\n",
      "Reading Data for 2018/02/20\n",
      "Reading Data for 2018/02/27\n",
      "Reading Data for 2018/03/04\n",
      "Reading Data for 2018/03/06\n",
      "Reading Data for 2018/03/13\n",
      "Reading Data for 2018/03/20\n",
      "Reading Data for 2018/03/27\n",
      "Reading Data for 2018/03/30\n",
      "Reading Data for 2018/03/31\n",
      "Reading Data for 2018/04/03\n",
      "Reading Data for 2018/04/10\n",
      "Reading Data for 2018/04/17\n",
      "Reading Data for 2018/04/22\n",
      "Reading Data for 2018/04/23\n",
      "Reading Data for 2018/04/24\n",
      "Reading Data for 2018/04/25\n",
      "Reading Data for 2018/04/26\n",
      "Reading Data for 2018/05/01\n",
      "Reading Data for 2018/05/08\n",
      "Reading Data for 2018/05/15\n",
      "Reading Data for 2018/05/22\n",
      "Reading Data for 2018/05/24\n",
      "Reading Data for 2018/05/28\n",
      "Reading Data for 2018/05/29\n",
      "Reading Data for 2018/06/01\n",
      "Reading Data for 2018/06/02\n",
      "Reading Data for 2018/06/05\n",
      "Reading Data for 2018/06/12\n",
      "Reading Data for 2018/06/19\n",
      "Reading Data for 2018/06/26\n",
      "Reading Data for 2018/12/04\n",
      "Reading Data for 2018/12/11\n",
      "Reading Data for 2018/12/18\n",
      "Reading Data for 2018/12/25\n",
      "Reading Data for 2019/01/01\n",
      "Reading Data for 2019/01/08\n",
      "Reading Data for 2019/01/15\n",
      "Reading Data for 2019/01/22\n",
      "Reading Data for 2019/01/29\n",
      "Reading Data for 2019/02/05\n",
      "Reading Data for 2019/02/12\n",
      "Reading Data for 2019/02/19\n",
      "Reading Data for 2019/02/26\n",
      "Reading Data for 2019/03/05\n",
      "Reading Data for 2019/03/09\n",
      "Reading Data for 2019/03/12\n",
      "Reading Data for 2019/03/15\n",
      "Reading Data for 2019/03/16\n",
      "Reading Data for 2019/03/17\n",
      "Reading Data for 2019/03/19\n",
      "Reading Data for 2019/03/24\n",
      "Reading Data for 2019/03/25\n",
      "Reading Data for 2019/03/26\n",
      "Reading Data for 2019/03/29\n",
      "Reading Data for 2019/04/02\n",
      "Reading Data for 2019/04/07\n",
      "Reading Data for 2019/04/08\n",
      "Reading Data for 2019/04/09\n",
      "Reading Data for 2019/04/16\n",
      "Reading Data for 2019/04/17\n",
      "Reading Data for 2019/04/18\n",
      "Reading Data for 2019/04/19\n",
      "Reading Data for 2019/04/21\n",
      "Reading Data for 2019/04/23\n",
      "Reading Data for 2019/04/27\n",
      "Reading Data for 2019/04/28\n",
      "Reading Data for 2019/04/30\n",
      "Reading Data for 2019/05/01\n",
      "Reading Data for 2019/05/02\n",
      "Reading Data for 2019/05/03\n",
      "Reading Data for 2019/05/07\n",
      "Reading Data for 2019/05/14\n",
      "Reading Data for 2019/05/21\n",
      "Reading Data for 2019/05/28\n",
      "Reading Data for 2019/06/04\n",
      "Reading Data for 2019/06/05\n",
      "Reading Data for 2019/06/08\n",
      "Reading Data for 2019/06/09\n",
      "Reading Data for 2019/06/10\n",
      "Reading Data for 2019/06/11\n",
      "Reading Data for 2019/06/13\n",
      "Reading Data for 2019/06/14\n",
      "Reading Data for 2019/06/18\n",
      "Reading Data for 2019/06/24\n",
      "Reading Data for 2019/06/25\n",
      "Reading Data for 2019/12/03\n",
      "Reading Data for 2019/12/10\n",
      "Reading Data for 2019/12/17\n",
      "Reading Data for 2019/12/24\n",
      "Reading Data for 2019/12/31\n",
      "Reading Data for 2020/01/07\n",
      "Reading Data for 2020/01/14\n",
      "Reading Data for 2020/01/21\n",
      "Reading Data for 2020/01/28\n",
      "Reading Data for 2020/02/04\n",
      "Reading Data for 2020/02/11\n",
      "Reading Data for 2020/02/18\n",
      "Reading Data for 2020/02/25\n",
      "Reading Data for 2020/03/03\n",
      "Reading Data for 2020/03/10\n",
      "Reading Data for 2020/03/17\n",
      "Reading Data for 2020/03/24\n",
      "Reading Data for 2020/03/31\n",
      "Reading Data for 2020/04/07\n",
      "Reading Data for 2020/04/14\n",
      "Reading Data for 2020/04/21\n",
      "Reading Data for 2020/04/28\n",
      "Reading Data for 2020/05/05\n",
      "Reading Data for 2020/05/12\n",
      "Reading Data for 2020/05/19\n",
      "Reading Data for 2020/05/26\n",
      "Reading Data for 2020/06/02\n",
      "Reading Data for 2020/06/09\n",
      "Reading Data for 2020/06/16\n",
      "Reading Data for 2020/06/23\n",
      "Reading Data for 2020/06/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Data for 2020/12/01\n",
      "Reading Data for 2020/12/08\n",
      "Reading Data for 2020/12/15\n",
      "Reading Data for 2020/12/22\n",
      "Reading Data for 2020/12/29\n",
      "Reading Data for 2021/01/05\n",
      "Reading Data for 2021/01/12\n",
      "Reading Data for 2021/01/19\n",
      "Reading Data for 2021/01/26\n",
      "Reading Data for 2021/02/02\n",
      "Reading Data for 2021/02/09\n",
      "Reading Data for 2021/02/16\n",
      "Reading Data for 2021/02/23\n",
      "Reading Data for 2021/03/02\n",
      "Reading Data for 2021/03/09\n",
      "Reading Data for 2021/03/16\n",
      "Reading Data for 2021/03/23\n",
      "Reading Data for 2021/03/30\n",
      "Reading Data for 2021/04/06\n",
      "Reading Data for 2021/04/13\n",
      "Reading Data for 2021/04/20\n",
      "Reading Data for 2021/04/27\n",
      "Reading Data for 2021/05/04\n",
      "Reading Data for 2021/05/11\n",
      "Reading Data for 2021/05/18\n",
      "Reading Data for 2021/05/25\n",
      "Reading Data for 2021/06/01\n",
      "Reading Data for 2021/06/08\n",
      "Reading Data for 2021/06/15\n",
      "Reading Data for 2021/06/22\n",
      "Reading Data for 2021/06/29\n"
     ]
    }
   ],
   "source": [
    "# Extract the UA Data (using the above lookup structures)\n",
    "\n",
    "AllLocations_data = np.ones([len(evaluation_cell_ids),len(times)])\n",
    "\n",
    "# Loop through the times\n",
    "for t in range(len(times)):\n",
    "    yyyy, mm, dd = times[t].split('-')\n",
    "    print('Reading Data for ' + yyyy + '/' + mm + '/' + dd);\n",
    "    \n",
    "    # Get data from the relevent file\n",
    "    src = gdal.Open('Data/UASWE/' + yyyy + '/' + mm + '/' + dd + '/SWE.tif')\n",
    "    data = src.ReadAsArray()\n",
    "    src = None\n",
    "    \n",
    "    # And extract the correct data for the grid cell\n",
    "    for i in range(len(evaluation_cell_ids)):\n",
    "        AllLocations_data[i,t] = max(0,data[UASWE_1km_YLocations[i],UASWE_1km_XLocations[i]])/25.4 # Convert to inches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55135c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write Output File\n",
    "\n",
    "# Write the first line\n",
    "f = open('Training Tables/ua_swe_data.csv', 'w')\n",
    "f.write('cell_id')\n",
    "for time in times:\n",
    "    f.write(',' + time)\n",
    "f.write('\\n')\n",
    "i = 0\n",
    "\n",
    "# For subsequent lines, write the cell id and then the data for each date\n",
    "for evaluation_cell_id in evaluation_cell_ids:\n",
    "    f.write(evaluation_cell_id)\n",
    "    for d in range(len(times)):\n",
    "        f.write(',{:.2f}'.format(max(0,AllLocations_data[i, d])))\n",
    "    f.write('\\n')\n",
    "    i = i+1\n",
    "    \n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369aae1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
