{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee55ae7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Script to generate train_label_data.csv (from Snowcast and other ASO, and SNOTEL/CDEC data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d7d8209",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup Python Environment\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import geojson\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from osgeo import gdal\n",
    "import subprocess\n",
    "from datetime import datetime, timedelta\n",
    "from osgeo import ogr\n",
    "\n",
    "# To make gdal ulility programs (which are used below) work properly, you need to have them \n",
    "# on the system path and set the PROJ_LIB and GDAL_DATA environment variables. If they are not, \n",
    "# then you can do this here.  This code works for Anaconda (Windows), but is not needed if GDAL \n",
    "# is already set up.\n",
    "\n",
    "# pypath = os.path.dirname(sys.executable)\n",
    "# sys.path.append(pypath + '/Library/bin')\n",
    "# os.environ['PROJ_LIB'] = pypath + '/Library/share/proj'\n",
    "# os.environ['GDAL_DATA'] = pypath + '/Library/share'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f9d029b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get Metadata for the Evaluation Stage Grid Cells\n",
    "\n",
    "with open('Data/Snowcast Evaluation/grid_cells.geojson') as f:\n",
    "    gj = geojson.load(f)\n",
    "features = gj['features']\n",
    "evaluation_cell_ids = []\n",
    "evaluation_coordinates = []\n",
    "for feature in features:\n",
    "    evaluation_cell_ids.append(feature['properties']['cell_id'])\n",
    "    evaluation_coordinates.append(feature['geometry']['coordinates'])\n",
    "\n",
    "evaluation_cell_ids = np.array(evaluation_cell_ids)  # So we can do numpy stuff to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4227f880",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get Metadata for the Development Stage Grid Cells\n",
    "\n",
    "with open('Data/Snowcast Development/grid_cells.geojson') as f:\n",
    "    gj = geojson.load(f)\n",
    "features = gj['features']\n",
    "development_cell_ids = []\n",
    "development_coordinates = []\n",
    "for feature in features:\n",
    "    development_cell_ids.append(feature['properties']['cell_id'])\n",
    "    development_coordinates.append(feature['geometry']['coordinates'])\n",
    "\n",
    "development_cell_ids = np.array(development_cell_ids)  # So we can do numpy stuff to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e564c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the Training Label Data for Development Stage (Training Dataset)\n",
    "\n",
    "# Read the file header (to get timetamp information)\n",
    "with open('Data/Snowcast Development/train_labels.csv') as f:\n",
    "    tline = f.readline().replace('\\n','')\n",
    "    header = tline.split(',')\n",
    "\n",
    "times = np.array(header[1:])\n",
    "num_times = len(times)\n",
    "num_lines = len(development_cell_ids)\n",
    "\n",
    "# Read in the data\n",
    "development_SWE = np.ones([num_lines, num_times]) * np.nan\n",
    "with open('Data/Snowcast Development/train_labels.csv') as f:\n",
    "    tline = f.readline()\n",
    "    tline = f.readline()\n",
    "    while not (tline == ''):\n",
    "        fields = tline.split(',')\n",
    "        id = fields[0]\n",
    "        loc = development_cell_ids == id\n",
    "        for d in range(len(fields)-1):\n",
    "            if not (fields[d+1] == '') and len(fields[d+1]) > 1:\n",
    "                development_SWE[loc,d] = float(fields[d+1])\n",
    "                \n",
    "        tline = f.readline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "692d8396",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the Training Label Data for Development Stage (2020-2021 Dataset)\n",
    "\n",
    "# Read the file header (to get timetamp information)\n",
    "with open('Data/Snowcast Evaluation/labels_2020_2021.csv') as f:\n",
    "    tline = f.readline().replace('\\n','')\n",
    "    header = tline.split(',')\n",
    "    \n",
    "times_2021 = np.array(header[1:])\n",
    "num_times_2021 = len(times_2021)\n",
    "num_lines = len(development_cell_ids)\n",
    "\n",
    "# Read in the data\n",
    "development_SWE_2021 = np.ones([num_lines, num_times_2021]) * np.nan\n",
    "with open('Data/Snowcast Evaluation/labels_2020_2021.csv') as f:\n",
    "    tline = f.readline()\n",
    "    tline = f.readline()\n",
    "    while not (tline == ''):\n",
    "        fields = tline.split(',')\n",
    "        id = fields[0]\n",
    "        loc = development_cell_ids == id\n",
    "        for d in range(len(fields)-1):\n",
    "            if not (fields[d+1] == '') and len(fields[d+1]) > 1:\n",
    "                development_SWE_2021[loc,d] = float(fields[d+1])\n",
    "                \n",
    "        tline = f.readline()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f9d089f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Arrange data from the development stage into the new grid cells\n",
    "\n",
    "# Concatenate the times and SWE data together\n",
    "times_all = np.concatenate((times, times_2021))\n",
    "development_SWE_all = np.concatenate((development_SWE, development_SWE_2021), axis=1)\n",
    "\n",
    "# Use the ids to match the grid cells together\n",
    "SWE_snowcast = np.ones([len(evaluation_cell_ids), len(times_all)]) * np.nan\n",
    "\n",
    "c = 0\n",
    "for evaluation_cell_id in evaluation_cell_ids:\n",
    "    loc = development_cell_ids == evaluation_cell_id\n",
    "    if np.any(loc):\n",
    "        SWE_snowcast[c,:] = development_SWE_all[loc,:]\n",
    "    c = c+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "642c4d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a database showing rasterized locations of all grid cells within 'ASO' domains for CO and CA\n",
    "## This step can take up to an hour, but results are saved to file so it only needs to be done once\n",
    "## Note that file is already saved\n",
    "\n",
    "CA_box_te = '-120.5 36 -118 38.5'\n",
    "CO_box_te = '-108.55 37 -105.75 39.7'\n",
    "\n",
    "if not os.path.exists('Data/ASO/ProcessedLocations.mat'):\n",
    "    print('Creating Locaton Maps')\n",
    "\n",
    "    # Write Modified geojson with list of file locations for each cell (so we can burn them on a raster (next step))\n",
    "    with open('Data/Snowcast Evaluation/grid_cells.geojson') as f:\n",
    "        gj = geojson.load(f)\n",
    "\n",
    "    features = gj['features']\n",
    "    development_cell_ids = []\n",
    "    development_coordinates = []\n",
    "    for c in range(len(features)):\n",
    "        features[c]['properties']['file_loc'] = c\n",
    "\n",
    "    gj['features'] = features\n",
    "    with open('tmp.geojson', 'w') as outfile:\n",
    "        geojson.dump(gj, outfile)\n",
    "\n",
    "    cmd = 'gdal_rasterize -a file_loc -a_srs \"EPSG:4326\" -te ' + CA_box_te + ' -tr 0.00042 0.00042 tmp.geojson tmp_CA_aso_locs.tif'  \n",
    "    print(cmd)\n",
    "    subprocess.run(cmd)\n",
    "    src = gdal.Open('tmp_CA_aso_locs.tif')\n",
    "    CA_box_locs = src.ReadAsArray()\n",
    "    src = None\n",
    "\n",
    "    cmd = 'gdal_rasterize -a file_loc -a_srs \"EPSG:4326\" -te ' + CO_box_te + ' -tr 0.00042 0.00042 tmp.geojson tmp_CO_aso_locs.tif'  \n",
    "    subprocess.run(cmd)\n",
    "    src = gdal.Open('tmp_CO_aso_locs.tif')\n",
    "    CO_box_locs = src.ReadAsArray()\n",
    "    src = None\n",
    "\n",
    "    print('Reading in locations of each grid cell on these grids (so we can reference them later)')\n",
    "    ca_locs = []\n",
    "    co_locs = []\n",
    "    for i in range(len(evaluation_cell_ids)):\n",
    "        ca_locs.append(np.where(CA_box_locs == i))\n",
    "        co_locs.append(np.where(CO_box_locs == i))\n",
    "       \n",
    "    # Write to file (so we only need to do this once)\n",
    "    mdict = {}\n",
    "    mdict['ca_locs'] = ca_locs\n",
    "    mdict['co_locs'] = co_locs\n",
    "    \n",
    "    os.remove('tmp.geojson')\n",
    "    os.remove('tmp_CA_aso_locs.tif')\n",
    "    os.remove('tmp_CO_aso_locs.tif')\n",
    "    \n",
    "    sio.savemat('Data/ASO/ProcessedLocations.mat', mdict)\n",
    "else:\n",
    "    mdict = sio.loadmat('Data/ASO/ProcessedLocations.mat',simplify_cells=True)\n",
    "    ca_locs = mdict['ca_locs']\n",
    "    co_locs = mdict['co_locs']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "219cd062",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting ASO Data for 2013-04-03\n",
      "Getting ASO Data for 2013-04-29\n",
      "Getting ASO Data for 2013-05-03\n",
      "Getting ASO Data for 2013-05-25\n",
      "Getting ASO Data for 2013-06-01\n",
      "Getting ASO Data for 2013-06-08\n",
      "Getting ASO Data for 2014-03-25\n",
      "Getting ASO Data for 2014-04-08\n",
      "Getting ASO Data for 2014-04-15\n",
      "Getting ASO Data for 2014-04-22\n",
      "Getting ASO Data for 2014-04-29\n",
      "Getting ASO Data for 2014-05-06\n",
      "Getting ASO Data for 2015-02-17\n",
      "Getting ASO Data for 2015-03-03\n",
      "Getting ASO Data for 2015-03-24\n",
      "Getting ASO Data for 2015-03-31\n",
      "Getting ASO Data for 2015-04-07\n",
      "Getting ASO Data for 2016-02-08\n",
      "Getting ASO Data for 2016-03-26\n",
      "Getting ASO Data for 2016-03-29\n",
      "Getting ASO Data for 2016-04-01\n",
      "Getting ASO Data for 2016-04-03\n",
      "Getting ASO Data for 2016-04-04\n",
      "Getting ASO Data for 2016-04-07\n",
      "Getting ASO Data for 2016-04-16\n",
      "Getting ASO Data for 2016-04-26\n",
      "Getting ASO Data for 2016-05-09\n",
      "Getting ASO Data for 2016-05-27\n",
      "Getting ASO Data for 2016-06-07\n",
      "Getting ASO Data for 2016-06-14\n",
      "Getting ASO Data for 2016-06-21\n",
      "Getting ASO Data for 2016-06-26\n",
      "Getting ASO Data for 2017-01-28\n",
      "Getting ASO Data for 2017-01-29\n",
      "Getting ASO Data for 2017-03-07\n",
      "Getting ASO Data for 2017-04-04\n",
      "Getting ASO Data for 2017-05-02\n",
      "Getting ASO Data for 2018-03-04\n",
      "Getting ASO Data for 2018-03-30\n",
      "Getting ASO Data for 2018-03-31\n",
      "Getting ASO Data for 2018-04-22\n",
      "Getting ASO Data for 2018-04-23\n",
      "Getting ASO Data for 2018-04-25\n",
      "Getting ASO Data for 2018-05-24\n",
      "Getting ASO Data for 2018-05-28\n",
      "Getting ASO Data for 2018-06-01\n",
      "Getting ASO Data for 2018-06-02\n",
      "Getting ASO Data for 2019-03-09\n",
      "Getting ASO Data for 2019-03-15\n",
      "Getting ASO Data for 2019-03-16\n",
      "Getting ASO Data for 2019-03-17\n",
      "Getting ASO Data for 2019-03-24\n",
      "Getting ASO Data for 2019-03-25\n",
      "Getting ASO Data for 2019-03-26\n",
      "Getting ASO Data for 2019-03-29\n",
      "Getting ASO Data for 2019-04-07\n",
      "Getting ASO Data for 2019-04-08\n",
      "Getting ASO Data for 2019-04-17\n",
      "Getting ASO Data for 2019-04-18\n",
      "Getting ASO Data for 2019-04-19\n",
      "Getting ASO Data for 2019-04-21\n",
      "Getting ASO Data for 2019-04-27\n",
      "Getting ASO Data for 2019-04-28\n",
      "Getting ASO Data for 2019-05-01\n",
      "Getting ASO Data for 2019-05-02\n",
      "Getting ASO Data for 2019-05-03\n",
      "Getting ASO Data for 2019-06-04\n",
      "Getting ASO Data for 2019-06-05\n",
      "Getting ASO Data for 2019-06-08\n",
      "Getting ASO Data for 2019-06-09\n",
      "Getting ASO Data for 2019-06-10\n",
      "Getting ASO Data for 2019-06-11\n",
      "Getting ASO Data for 2019-06-13\n",
      "Getting ASO Data for 2019-06-14\n",
      "Getting ASO Data for 2019-06-18\n",
      "Getting ASO Data for 2019-06-24\n",
      "Getting ASO Data for 2019-06-25\n",
      "Getting ASO Data for 2020-04-14\n",
      "Getting ASO Data for 2020-05-05\n",
      "Getting ASO Data for 2020-05-19\n",
      "Getting ASO Data for 2020-05-26\n",
      "Getting ASO Data for 2020-06-02\n",
      "Getting ASO Data for 2020-06-09\n",
      "Getting ASO Data for 2021-02-02\n",
      "Getting ASO Data for 2021-02-16\n",
      "Getting ASO Data for 2021-02-23\n",
      "Getting ASO Data for 2021-03-30\n",
      "Getting ASO Data for 2021-04-20\n",
      "Getting ASO Data for 2021-04-27\n",
      "Getting ASO Data for 2021-05-04\n",
      "Getting ASO Data for 2021-05-11\n",
      "Getting ASO Data for 2021-05-18\n",
      "Getting ASO Data for 2021-05-25\n"
     ]
    }
   ],
   "source": [
    "## Extract the ASO data using the lookup structure created above\n",
    "\n",
    "SWE_aso = np.ones(SWE_snowcast.shape) * np.nan\n",
    "\n",
    "# Get a listing of all ASO files (which are referenced using a list of virtual rasters)\n",
    "dir_list = os.listdir('Data/ASO/vrt')\n",
    "# Loop through all of the times\n",
    "for t in range(len(times_all)):\n",
    "    yyyy, mm, dd = times_all[t].split('-')\n",
    "    found = 0;\n",
    "    fname = yyyy + '_' + mm + '_' + dd + '.vrt'\n",
    "    for file in dir_list:\n",
    "        if file == fname:\n",
    "            found = 1\n",
    "            \n",
    "    # If a vrt file is found for that date, try to extract pixels for both the CA and CO domains\n",
    "\n",
    "    if found == 1:\n",
    "        print('Getting ASO Data for ' + times_all[t])\n",
    "        # Expand to cover the entire CA domain\n",
    "        cmd = 'gdalwarp -overwrite -t_srs \"EPSG:4326\" -te ' + CA_box_te + ' -tr 0.00042 0.00042 \"Data/ASO/vrt/' + fname + '\" tmp_CA.tif'\n",
    "        subprocess.run(cmd)\n",
    "        src = gdal.Open('tmp_CA.tif')\n",
    "        data = src.ReadAsArray()\n",
    "        src = None\n",
    "        os.remove('tmp_CA.tif')\n",
    "        # Use the lookup structure to query relavent cells, and if they are at least half covered \n",
    "        # by actual data, then get data for those cells\n",
    "        for i in range(len(evaluation_cell_ids)):\n",
    "            if len(ca_locs[i][0]) > 0:\n",
    "                data_sub = data[tuple(ca_locs[i])]\n",
    "                data_sub[data_sub < 0] = np.nan\n",
    "                if np.sum(~np.isnan(data_sub)) > data_sub.size/2:\n",
    "                    SWE_aso[i,t] = np.nanmean(data_sub) * 39.37    # Convert to inches   \n",
    "       \n",
    "        # Expand to cover the entire CO domain\n",
    "        cmd = 'gdalwarp -overwrite -t_srs \"EPSG:4326\" -te ' + CO_box_te + ' -tr 0.00042 0.00042 \"Data/ASO/vrt/' + fname + '\" tmp_CO.tif'\n",
    "        subprocess.run(cmd)\n",
    "        src = gdal.Open('tmp_CO.tif')\n",
    "        data = src.ReadAsArray()\n",
    "        src = None\n",
    "        os.remove('tmp_CO.tif')\n",
    "        # Use the lookup structure to query relavent cells, and if they are at least half covered \n",
    "        # by actual data, then get data for those cells\n",
    "        for i in range(len(evaluation_cell_ids)):\n",
    "            if len(co_locs[i][0]) > 0:\n",
    "                data_sub = data[tuple(co_locs[i])]\n",
    "                data_sub[data_sub < 0] = np.nan\n",
    "                if np.sum(~np.isnan(data_sub)) > data_sub.size/2:\n",
    "                    SWE_aso[i,t] = np.nanmean(data_sub) * 39.37    # Convert to inches \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a6e126b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting SNOTEL data from STM-Stouts Meadow.csv\n",
      "Getting SNOTEL data from GOL-Gold Lake.csv\n",
      "Getting SNOTEL data from 574-LEAVITT LAKE.csv\n",
      "Getting SNOTEL data from 931-BIG GOOSE.csv\n",
      "Getting SNOTEL data from 408-COLUMBINE.csv\n",
      "Getting SNOTEL data from 409-COLUMBINE PASS.csv\n",
      "Getting SNOTEL data from CBT-Crabtree Meadow.csv\n",
      "Getting SNOTEL data from 753-SHORT CREEK.csv\n",
      "Getting SNOTEL data from 843-VALLECITO.csv\n",
      "Getting SNOTEL data from 538-IDARADO.csv\n",
      "Getting SNOTEL data from 1137-VACARRO SPRING.csv\n",
      "Getting SNOTEL data from 517-HAYDEN FORK.csv\n",
      "Getting SNOTEL data from 599-LOST HORSE.csv\n",
      "Getting SNOTEL data from 800-SUMMER RIM.csv\n",
      "Getting SNOTEL data from 400-CLEAR CREEK #2.csv\n",
      "Getting SNOTEL data from 944-GUNSIGHT PASS.csv\n",
      "Getting SNOTEL data from 705-PROMONTORY.csv\n",
      "Getting SNOTEL data from 932-POORMAN CREEK.csv\n",
      "Getting SNOTEL data from 382-CAMAS CREEK DIVIDE.csv\n",
      "Getting SNOTEL data from 774-SOUTH MTN..csv\n",
      "Getting SNOTEL data from 323-BEAR MOUNTAIN.csv\n",
      "Getting SNOTEL data from 527-HOLE-IN-MOUNTAIN.csv\n",
      "Getting SNOTEL data from 971-PARRISH CREEK.csv\n",
      "Getting SNOTEL data from 813-TEPEE CREEK.csv\n",
      "Getting SNOTEL data from 511-HANNAGAN MEADOWS.csv\n",
      "Getting SNOTEL data from PLP-Pilot Peak.csv\n",
      "Getting SNOTEL data from 668-NORTH FRENCH CREEK.csv\n",
      "Getting SNOTEL data from STR-Ostrander Lake.csv\n",
      "Getting SNOTEL data from 1059-COCHETOPA PASS.csv\n",
      "Getting SNOTEL data from 1097-TIMBERLINE.csv\n",
      "Getting SNOTEL data from 624-MIDDLE CREEK.csv\n",
      "Getting SNOTEL data from 1102-HAYDEN PASS.csv\n",
      "Getting SNOTEL data from 1153-BUCKBOARD FLAT.csv\n",
      "Getting SNOTEL data from 374-BUG LAKE.csv\n",
      "Getting SNOTEL data from 394-CHAMITA.csv\n",
      "Getting SNOTEL data from 561-KOLOB.csv\n",
      "Getting SNOTEL data from 501-GRAVE SPRINGS.csv\n",
      "Getting SNOTEL data from 369-BRUMLEY.csv\n",
      "Getting SNOTEL data from PSR-Poison Ridge.csv\n",
      "Getting SNOTEL data from BSH-Bishop Pass.csv\n",
      "Getting SNOTEL data from 829-TRINCHERA.csv\n",
      "Getting SNOTEL data from 679-PARADISE.csv\n",
      "Getting SNOTEL data from 516-HAWKINS LAKE.csv\n",
      "Getting SNOTEL data from 636-MOONSHINE.csv\n",
      "Getting SNOTEL data from NLS-Noel Spring.csv\n",
      "Getting SNOTEL data from 898-MOUNT GARDNER.csv\n",
      "Getting SNOTEL data from 496-GRAHAM GUARD STA..csv\n",
      "Getting SNOTEL data from 443-DIAMOND PEAK.csv\n",
      "Getting SNOTEL data from 721-ROCK SPRINGS.csv\n",
      "Getting SNOTEL data from 367-BROOKLYN LAKE.csv\n",
      "Getting SNOTEL data from REL-Lower Relief Valley.csv\n",
      "Getting SNOTEL data from 357-BLUE MOUNTAIN SPRING.csv\n",
      "Getting SNOTEL data from 1110-RAINBOW CANYON.csv\n",
      "Getting SNOTEL data from 1112-LEE CANYON.csv\n",
      "Getting SNOTEL data from 1244-POLE CANYON.csv\n",
      "Getting SNOTEL data from 2141-KYLE CANYON.csv\n",
      "Getting SNOTEL data from 2170-PORTER CANYON.csv\n",
      "Getting SNOTEL data from 320-BEAR CANYON.csv\n",
      "Getting SNOTEL data from 363-BOX CANYON.csv\n",
      "Getting SNOTEL data from 384-CANYON.csv\n",
      "Getting SNOTEL data from 417-CORRAL CANYON.csv\n",
      "Getting SNOTEL data from 534-HOWELL CANYON.csv\n",
      "Getting SNOTEL data from 543-INDIAN CANYON.csv\n",
      "Getting SNOTEL data from 748-SHEEP CANYON.csv\n",
      "Getting SNOTEL data from 811-TAYLOR CANYON.csv\n",
      "Getting SNOTEL data from 814-THAYNES CANYON.csv\n",
      "Getting SNOTEL data from 907-AGUA CANYON.csv\n",
      "Getting SNOTEL data from 927-SNOWSLIDE CANYON.csv\n",
      "Getting SNOTEL data from 982-COLE CANYON.csv\n",
      "Getting SNOTEL data from 445-DISASTER PEAK.csv\n",
      "Getting SNOTEL data from 595-LOOKOUT MOUNTAIN.csv\n",
      "Getting SNOTEL data from 425-CRATER MEADOWS.csv\n",
      "Getting SNOTEL data from 315-BASIN CREEK.csv\n",
      "Getting SNOTEL data from 772-SOUTH BRUSH CREEK.csv\n",
      "Getting SNOTEL data from 711-RAINY PASS.csv\n",
      "Getting SNOTEL data from 760-SKALKAHO SUMMIT.csv\n",
      "Getting SNOTEL data from TUN-Tunnel Guard Station.csv\n",
      "Getting SNOTEL data from 1117-SPIRIT LK.csv\n",
      "Getting SNOTEL data from 1008-ONION PARK.csv\n",
      "Getting SNOTEL data from RBB-Robbs Saddle.csv\n",
      "Getting SNOTEL data from 454-DRAW CREEK.csv\n",
      "Getting SNOTEL data from 353-BLIND BULL SUM.csv\n",
      "Getting SNOTEL data from 853-WEBSTER FLAT.csv\n",
      "Getting SNOTEL data from 383-CAMP JACKSON.csv\n",
      "Getting SNOTEL data from 1078-SUN PASS.csv\n",
      "Getting SNOTEL data from 602-LOVELAND BASIN.csv\n",
      "Getting SNOTEL data from 725-S FORK SHIELDS.csv\n",
      "Getting SNOTEL data from 361-BOURNE.csv\n",
      "Getting SNOTEL data from MUM-Mumbo Basin.csv\n",
      "Getting SNOTEL data from 759-SILVIES.csv\n",
      "Getting SNOTEL data from 916-ALBRO LAKE.csv\n",
      "Getting SNOTEL data from 1130-CASTLE CREEK.csv\n",
      "Getting SNOTEL data from 506-GROS VENTRE SUMMIT.csv\n",
      "Getting SNOTEL data from 769-SOLDIER R.S..csv\n",
      "Getting SNOTEL data from 309-BALD MTN..csv\n",
      "Getting SNOTEL data from 471-EMIGRANT SUMMIT.csv\n",
      "Getting SNOTEL data from 352-BLEWETT PASS.csv\n",
      "Getting SNOTEL data from SNM-Snow Mountain.csv\n",
      "Getting SNOTEL data from 656-MULE CREEK.csv\n",
      "Getting SNOTEL data from SLK-South Lake.csv\n",
      "Getting SNOTEL data from 943-DUNGENESS.csv\n",
      "Getting SNOTEL data from SLI-Slide Canyon.csv\n",
      "Getting SNOTEL data from 1118-LOST CREEK RESV.csv\n",
      "Getting SNOTEL data from 479-FISH LK..csv\n",
      "Getting SNOTEL data from 697-POISON FLAT.csv\n",
      "Getting SNOTEL data from 860-WHITE ELEPHANT.csv\n",
      "Getting SNOTEL data from 446-DISMAL SWAMP.csv\n",
      "Getting SNOTEL data from 608-MADISON BUTTE.csv\n",
      "Getting SNOTEL data from 502-GREEN LAKE.csv\n",
      "Getting SNOTEL data from 442-DIAMOND LAKE.csv\n",
      "Getting SNOTEL data from 557-KIMBERLY MINE.csv\n",
      "Getting SNOTEL data from 828-TRIAL LAKE.csv\n",
      "Getting SNOTEL data from MB3-Middle Boulder 3.csv\n",
      "Getting SNOTEL data from 675-OVERLAND RES..csv\n",
      "Getting SNOTEL data from 893-TIZER BASIN.csv\n",
      "Getting SNOTEL data from 1047-LITTLE SNAKE RIVER.csv\n",
      "Getting SNOTEL data from 639-MORGAN CREEK.csv\n",
      "Getting SNOTEL data from KSP-Kaiser Point.csv\n",
      "Getting SNOTEL data from 940-LOST DOG.csv\n",
      "Getting SNOTEL data from 573-LAUREL DRAW.csv\n",
      "Getting SNOTEL data from 783-SLEEPING WOMAN.csv\n",
      "Getting SNOTEL data from 545-IRISH TAYLOR.csv\n",
      "Getting SNOTEL data from 688-PHANTOM VALLEY.csv\n",
      "Getting SNOTEL data from 1112-LEE CANYON.csv\n",
      "Getting SNOTEL data from 908-ALPINE MEADOWS.csv\n",
      "Getting SNOTEL data from 687-PEAVINE RIDGE.csv\n",
      "Getting SNOTEL data from 631-MINING FORK.csv\n",
      "Getting SNOTEL data from 928-HUCKLEBERRY CREEK.csv\n",
      "Getting SNOTEL data from HHM-Highland Meadow.csv\n",
      "Getting SNOTEL data from 969-HAPPY JACK.csv\n",
      "Getting SNOTEL data from 371-BUCK FLAT.csv\n",
      "Getting SNOTEL data from 775-SOUTH PASS.csv\n",
      "Getting SNOTEL data from 903-NEVADA RIDGE.csv\n",
      "Getting SNOTEL data from 1083-TRES RITOS.csv\n",
      "Getting SNOTEL data from 427-CRYSTAL LAKE.csv\n",
      "Getting SNOTEL data from 737-SCHOFIELD PASS.csv\n",
      "Getting SNOTEL data from 551-JOE WRIGHT.csv\n",
      "Getting SNOTEL data from 1212-BAR M.csv\n",
      "Getting SNOTEL data from 567-LAKEFORK BASIN.csv\n",
      "Getting SNOTEL data from 1081-RAGGED MOUNTAIN.csv\n",
      "Getting SNOTEL data from STL-State Lakes.csv\n",
      "Getting SNOTEL data from 584-LITTLE MEADOWS.csv\n",
      "Getting SNOTEL data from 789-STARR RIDGE.csv\n",
      "Getting SNOTEL data from 716-RENO HILL.csv\n",
      "Getting SNOTEL data from 632-MOLAS LAKE.csv\n",
      "Getting SNOTEL data from 821-TIPTON.csv\n",
      "Getting SNOTEL data from 983-CLAYTON SPRINGS.csv\n",
      "Getting SNOTEL data from 1221-GBRC HQ.csv\n",
      "Getting SNOTEL data from 1142-PIERCE R.S..csv\n",
      "Getting SNOTEL data from 609-MADISON PLATEAU.csv\n",
      "Getting SNOTEL data from 461-EAST WILLOW CREEK.csv\n",
      "Getting SNOTEL data from 634-MONTE CRISTO.csv\n",
      "Getting SNOTEL data from 594-LOOKOUT.csv\n",
      "Getting SNOTEL data from 595-LOOKOUT MOUNTAIN.csv\n",
      "Getting SNOTEL data from 596-LOOKOUT PEAK.csv\n",
      "Getting SNOTEL data from 1060-SHARKSTOOTH.csv\n",
      "Getting SNOTEL data from 867-WILDHORSE DIVIDE.csv\n",
      "Getting SNOTEL data from 504-GREENPOINT.csv\n",
      "Getting SNOTEL data from 1217-REES FLAT.csv\n",
      "Getting SNOTEL data from 692-PIGTAIL PEAK.csv\n",
      "Getting SNOTEL data from 395-CHEMULT ALTERNATE.csv\n",
      "Getting SNOTEL data from 537-HYNDMAN.csv\n",
      "Getting SNOTEL data from 788-STAMPEDE PASS.csv\n",
      "Getting SNOTEL data from 1163-EF BLACKS FORK GS.csv\n",
      "Getting SNOTEL data from 653-MT. HOWARD.csv\n",
      "Getting SNOTEL data from 1125-MORMON MTN SUMMIT.csv\n",
      "Getting SNOTEL data from 802-SUMMIT RANCH.csv\n",
      "Getting SNOTEL data from 707-QUARTZ PEAK.csv\n",
      "Getting SNOTEL data from 703-POWDER RIVER PASS.csv\n",
      "Getting SNOTEL data from 440-DERR..csv\n",
      "Getting SNOTEL data from 740-SECESH SUMMIT.csv\n",
      "Getting SNOTEL data from 534-HOWELL CANYON.csv\n",
      "Getting SNOTEL data from 1116-LAKEFORK #3.csv\n",
      "Getting SNOTEL data from CAP-Caples Lake.csv\n",
      "Getting SNOTEL data from 618-MC CLURE PASS.csv\n",
      "Getting SNOTEL data from 1194-SUMMIT LK.csv\n",
      "Getting SNOTEL data from 970-JONES PASS.csv\n",
      "Getting SNOTEL data from 1069-LYNN LAKE.csv\n",
      "Getting SNOTEL data from 1080-BROWN TOP.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting SNOTEL data from 776-SPENCER MEADOW.csv\n",
      "Getting SNOTEL data from 695-PINE CREEK PASS.csv\n",
      "Getting SNOTEL data from 836-TWIN LAKES.csv\n",
      "Getting SNOTEL data from 435-DANIELS-STRAWBERRY.csv\n",
      "Getting SNOTEL data from 359-BOSTETTER R.S..csv\n",
      "Getting SNOTEL data from 469-EMERY CREEK.csv\n",
      "Getting SNOTEL data from KIB-Lower Kibbie Ridge.csv\n",
      "Getting SNOTEL data from 378-BURRO MOUNTAIN.csv\n",
      "Getting SNOTEL data from 518-HEAVENLY VALLEY.csv\n",
      "Getting SNOTEL data from 979-VAN WYCK.csv\n",
      "Getting SNOTEL data from 1151-GEORGE CREEK.csv\n",
      "Getting SNOTEL data from 1056-LIGHTNING RIDGE.csv\n",
      "Getting SNOTEL data from 515-HARTS PASS.csv\n",
      "Getting SNOTEL data from 307-BADGER PASS.csv\n",
      "Getting SNOTEL data from 343-BIGELOW CAMP.csv\n",
      "Getting SNOTEL data from 405-COLD SPRINGS.csv\n",
      "Getting SNOTEL data from 406-COLD SPRINGS CAMP.csv\n",
      "Getting SNOTEL data from FOR-Four Trees.csv\n",
      "Getting SNOTEL data from 351-BLAZED ALDER.csv\n",
      "Getting SNOTEL data from 362-BOWMAN SPRINGS.csv\n",
      "Getting SNOTEL data from 934-TOLBY.csv\n",
      "Getting SNOTEL data from 870-WILLOW PARK.csv\n",
      "Getting SNOTEL data from 578-LICK CREEK.csv\n",
      "Getting SNOTEL data from 910-ELBOW LAKE.csv\n",
      "Getting SNOTEL data from 1188-WAGER GULCH.csv\n",
      "Getting SNOTEL data from 875-WOLVERINE.csv\n",
      "Getting SNOTEL data from 547-IVANHOE.csv\n",
      "Getting SNOTEL data from 318-BEAGLE SPRINGS.csv\n",
      "Getting SNOTEL data from 660-NEW CRESCENT LAKE.csv\n",
      "Getting SNOTEL data from 1106-ELK PEAK.csv\n",
      "Getting SNOTEL data from 933-RICE PARK.csv\n",
      "Getting SNOTEL data from 766-SNOWBIRD.csv\n",
      "Getting SNOTEL data from 914-MEDANO PASS.csv\n",
      "Getting SNOTEL data from 1006-LEWIS PEAK.csv\n",
      "Getting SNOTEL data from 701-PORPHYRY CREEK.csv\n",
      "Getting SNOTEL data from 601-LOST-WOOD DIVIDE.csv\n",
      "Getting SNOTEL data from 722-ROCKER PEAK.csv\n",
      "Getting SNOTEL data from 635-MONUMENT PEAK.csv\n",
      "Getting SNOTEL data from 1041-BEAVER CK VILLAGE.csv\n",
      "Getting SNOTEL data from 1224-MT BALDY.csv\n",
      "Getting SNOTEL data from 310-BALDY.csv\n",
      "Getting SNOTEL data from 985-SOURDOUGH GULCH.csv\n",
      "Getting SNOTEL data from 1042-WILD BASIN.csv\n",
      "Getting SNOTEL data from 866-WILDCAT.csv\n",
      "Getting SNOTEL data from 1057-GLEN COVE.csv\n",
      "Getting SNOTEL data from 938-BUCKSKIN JOE.csv\n",
      "Getting SNOTEL data from 586-LIZARD HEAD PASS.csv\n",
      "Getting SNOTEL data from 719-ROARING RIVER.csv\n",
      "Getting SNOTEL data from 546-ISLAND PARK.csv\n"
     ]
    }
   ],
   "source": [
    "## Extract the SNOTEL/CDEC Data\n",
    "\n",
    "SWE_snotel = np.ones(SWE_snowcast.shape) * np.nan\n",
    "\n",
    "## Define date vector for SNOTEL data\n",
    "\n",
    "# Function to generate date vector between two dates\n",
    "def date_range(start, end):\n",
    "    delta = end - start  # as timedelta\n",
    "    days = [start + timedelta(days=i) for i in range(delta.days + 1)]\n",
    "    return days\n",
    "\n",
    "# All snotel data is organized into tables that start on 10/1/2010 and end on 9/30/2019 \n",
    "start_date = datetime(2010, 10, 1)\n",
    "end_date = datetime(2021, 9, 30)\n",
    "snotel_ts = date_range(start_date, end_date)\n",
    "\n",
    "# Convert date vecotr into a vector of formatted dates (to match the csv files)\n",
    "snotel_dates = []\n",
    "for date in snotel_ts:\n",
    "    snotel_dates.append(date.strftime('%Y-%m-%d'))\n",
    "snotel_dates = np.array(snotel_dates)\n",
    "    \n",
    "## Get locations and names of all available SNOTEL data\n",
    "\n",
    "# Get information about SNOTELs from a shapefile\n",
    "all_snotel_lats = []\n",
    "all_snotel_lons = []\n",
    "all_snotel_names = []\n",
    "src = ogr.Open('Data/SNOTEL/Locations.shp')\n",
    "layer = src.GetLayer(0)\n",
    "for feature in layer:\n",
    "    all_snotel_lats.append(feature.GetField('Latitude'))\n",
    "    all_snotel_lons.append(feature.GetField('Longitude'))\n",
    "    all_snotel_names.append(feature.GetField('Name'))\n",
    "src = None\n",
    "all_snotel_lats = np.array(all_snotel_lats)\n",
    "all_snotel_lons = np.array(all_snotel_lons)\n",
    "\n",
    "# List of cells that are probable snotels (> 50 measurements)\n",
    "snotel_locs = np.sum(~np.isnan(SWE_snowcast),axis=1)>50\n",
    "dir_list = os.listdir('Data/SNOTEL/Data')\n",
    "# Loop through all grid cells\n",
    "for i in range(len(evaluation_cell_ids)):\n",
    "    \n",
    "    # But only consider those that are snotel grids\n",
    "    if snotel_locs[i] == True:\n",
    "       \n",
    "        # Get the centroid of each cells, and look for closest snotel\n",
    "        X_center = (evaluation_coordinates[i][0][0][0] + evaluation_coordinates[i][0][2][0]) / 2\n",
    "        Y_center = (evaluation_coordinates[i][0][0][1] + evaluation_coordinates[i][0][1][1]) / 2\n",
    "        dist = np.sqrt((all_snotel_lats-Y_center)**2 + (all_snotel_lons-X_center)**2)\n",
    "        \n",
    "        # If a very close one is found, then read the data\n",
    "        if np.min(dist) < 0.01:\n",
    "            snotel_name = all_snotel_names[np.where(dist == np.min(dist))[0][0]]\n",
    "            for fname in dir_list:\n",
    "                if snotel_name in fname:\n",
    "                    print('Getting SNOTEL data from ' + fname)\n",
    "                    swe = np.genfromtxt('Data/SNOTEL/Data/' + fname, delimiter=',',skip_header=5)[:,3]\n",
    "                    \n",
    "                    # Extract data for the relavent times\n",
    "                    for d in range(len(times_all)):\n",
    "                        tloc = snotel_dates == times_all[d]\n",
    "                        SWE_snotel[i,d] = swe[tloc]/25.4\n",
    "\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "822a6966",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put All of the data together\n",
    "\n",
    "SWE_final = np.ones(SWE_snowcast.shape) * np.nan\n",
    "\n",
    "## For non-SNOTEL grid cells, primarily rely on external ASO data\n",
    "\n",
    "non_snotel_locs = np.sum(~np.isnan(SWE_snowcast),axis=1) < 50\n",
    "SWE_final[non_snotel_locs,:] = SWE_aso[non_snotel_locs,:]\n",
    "\n",
    "# This date has bad data though, so disregard\n",
    "SWE_final[non_snotel_locs,253] = np.nan\n",
    "\n",
    "# Where there isn't any other ASO data, but the Snowcast files have non-zero SWE, use the Snowcast data\n",
    "non_snotel_locs = np.tile(np.reshape(non_snotel_locs, (-1, len(non_snotel_locs))).T,[1, 324])\n",
    "locs = non_snotel_locs * np.isnan(SWE_final) * (SWE_snowcast > 0)\n",
    "SWE_final[locs] = SWE_snowcast[locs]\n",
    "\n",
    "## For SNOTEL grid cells, primarily rely on external SNOTEL data\n",
    "\n",
    "snotel_locs = np.where(np.sum(~np.isnan(SWE_snowcast),axis=1) > 50)\n",
    "SWE_final[snotel_locs,:] = SWE_snotel[snotel_locs,:]\n",
    "\n",
    "# From manual inspection, it was determined that the Snowcast data was better for the following locations\n",
    "# (either more complete record, does not show any artifacts, or where external SNOTEL data is missing)\n",
    "SWE_final[snotel_locs[0][4],:] = SWE_snowcast[snotel_locs[0][4],:]\n",
    "SWE_final[snotel_locs[0][43],:] = SWE_snowcast[snotel_locs[0][43],:]\n",
    "SWE_final[snotel_locs[0][52],:] = SWE_snowcast[snotel_locs[0][52],:]\n",
    "SWE_final[snotel_locs[0][59],:] = SWE_snowcast[snotel_locs[0][59],:]\n",
    "SWE_final[snotel_locs[0][76],:] = SWE_snowcast[snotel_locs[0][76],:]\n",
    "SWE_final[snotel_locs[0][80],:] = SWE_snowcast[snotel_locs[0][80],:]\n",
    "SWE_final[snotel_locs[0][116],:] = SWE_snowcast[snotel_locs[0][116],:]\n",
    "SWE_final[snotel_locs[0][126],:] = SWE_snowcast[snotel_locs[0][126],:]\n",
    "SWE_final[snotel_locs[0][129],:] = SWE_snowcast[snotel_locs[0][129],:]\n",
    "SWE_final[snotel_locs[0][137],:] = SWE_snowcast[snotel_locs[0][137],:]\n",
    "SWE_final[snotel_locs[0][142],:] = SWE_snowcast[snotel_locs[0][142],:]\n",
    "SWE_final[snotel_locs[0][161],:] = SWE_snowcast[snotel_locs[0][161],:]\n",
    "SWE_final[snotel_locs[0][165],:] = SWE_snowcast[snotel_locs[0][165],:]\n",
    "SWE_final[snotel_locs[0][184],:] = SWE_snowcast[snotel_locs[0][184],:]\n",
    "SWE_final[snotel_locs[0][186],:] = SWE_snowcast[snotel_locs[0][186],:]\n",
    "SWE_final[snotel_locs[0][207],:] = SWE_snowcast[snotel_locs[0][207],:]\n",
    "SWE_final[snotel_locs[0][209],:] = SWE_snowcast[snotel_locs[0][209],:]\n",
    "\n",
    "SWE_final[SWE_final<0] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be1af5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write Output File\n",
    "\n",
    "f = open('Training Tables/train_label_data.csv', 'w')\n",
    "\n",
    "# Write the first line\n",
    "f.write('cell_id')\n",
    "for time in times:\n",
    "    f.write(',' + time)\n",
    "f.write('\\n')\n",
    "\n",
    "# For subsequent lines, write the cell id and then the data for each date\n",
    "i = 0\n",
    "for evaluation_cell_id in evaluation_cell_ids:\n",
    "    f.write(evaluation_cell_id)\n",
    "    for d in range(len(times_all)):\n",
    "        f.write(',{:.2f}'.format(SWE_final[i, d]))\n",
    "    f.write('\\n')\n",
    "    i = i+1\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57d2598",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
